{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc429ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# noinspection PyUnresolvedReferences\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Subtract, Add, Reshape\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f814ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import recall_score, precision_score # New\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.preprocessing\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Add, Multiply, Subtract\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "# regularizers\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dropout, SpatialDropout1D\n",
    "from scipy import signal\n",
    "import openpyxl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import svd\n",
    "import gc\n",
    "from keijzer import *\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.layers import Dense, Input, Flatten, Add, concatenate, Dropout, Activation, Multiply, Embedding, Layer, Reshape\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D  \n",
    "from keras.layers import Conv1D, AveragePooling1D, MaxPooling1D\n",
    "#from keras.ops import convert_to_tensor, convert_to_numpy\n",
    "#from keras.utils import plot_model\n",
    "from keras import activations\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "class LossHistory(tensorflow.keras.callbacks.Callback):  # history = LossHistory()\n",
    "    def init(self):  # history.init()\n",
    "        self.losses = []\n",
    "        # self.accs = []\n",
    "        self.val_losses = []\n",
    "        # self.val_accs = []\n",
    "        self.rmses = []\n",
    "        self.mses = []\n",
    "        self.maes = []\n",
    "        self.mapes = []\n",
    "        self.val_rmses = []\n",
    "        self.val_mses = []\n",
    "        self.val_maes = []\n",
    "        self.val_mapes = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        # self.accs.append(logs.get('acc'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        # self.val_accs.append(logs.get('val_accuracy\n",
    "        self.rmses.append(logs.get('root_mean_squared_error'))\n",
    "        self.mses.append(logs.get('mean_squared_error'))\n",
    "        self.maes.append(logs.get('mean_absolute_error'))\n",
    "        self.mapes.append(logs.get('mean_absolute_percentage_error'))\n",
    "        self.val_rmses.append(logs.get('val_root_mean_squared_error'))\n",
    "        self.val_mses.append(logs.get('val_mean_squared_error'))\n",
    "        self.val_maes.append(logs.get('val_mean_absolute_error'))\n",
    "        self.val_mapes.append(logs.get('val_mean_absolute_percentage_error'))\n",
    "\n",
    "\n",
    "\n",
    "def root_squared_mean_error(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true-K.mean(y_true)))*100\n",
    "\n",
    "def mse_mae(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))+K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747ed099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import scipy as sc\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # 여러개 사용시 '0,1,2' 식으로 하나의 문자열에 입력\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') # 호스트 러나임에 표시되는 GPU 장치 목록 반환\n",
    "\n",
    "if gpus: # 반환된 GPU 장치 목록이 있다면\n",
    "    try: # 해당 장치에 대한 메모리 증가 활성화 여부 설정\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e: # try문 실패시에 에러문구 출력\n",
    "        print(e)\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c960add6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Wind Change</th>\n",
       "      <th>Wind Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.620197</td>\n",
       "      <td>0.644724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.544580</td>\n",
       "      <td>0.658617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.570751</td>\n",
       "      <td>0.683924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.599594</td>\n",
       "      <td>0.721813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.495246</td>\n",
       "      <td>0.714187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35058</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.774643</td>\n",
       "      <td>0.484984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.735512</td>\n",
       "      <td>0.582157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.601593</td>\n",
       "      <td>0.620917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.536583</td>\n",
       "      <td>0.631322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.454969</td>\n",
       "      <td>0.606127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35063 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  Wind Change  Wind Output\n",
       "0      2020     0.620197     0.644724\n",
       "1      2020     0.544580     0.658617\n",
       "2      2020     0.570751     0.683924\n",
       "3      2020     0.599594     0.721813\n",
       "4      2020     0.495246     0.714187\n",
       "...     ...          ...          ...\n",
       "35058  2023     0.774643     0.484984\n",
       "35059  2023     0.735512     0.582157\n",
       "35060  2023     0.601593     0.620917\n",
       "35061  2023     0.536583     0.631322\n",
       "35062  2023     0.454969     0.606127\n",
       "\n",
       "[35063 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2020.csv'\n",
    "data_source1 = pd.read_csv(data_path)\n",
    "dat_source1 = pd.DataFrame(data_source1)\n",
    "#dat_source1 = dat_source1.iloc[:-3,:]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2021.csv'\n",
    "data_source2 = pd.read_csv(data_path)\n",
    "dat_source2 = pd.DataFrame(data_source2)\n",
    "dat_source2 = dat_source2.iloc[:8761,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2022.csv'\n",
    "data_source3 = pd.read_csv(data_path)\n",
    "dat_source3 = pd.DataFrame(data_source3)\n",
    "dat_source3 = dat_source3.iloc[:8760,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2023.csv'\n",
    "data_source4 = pd.read_csv(data_path)\n",
    "dat_source4 = pd.DataFrame(data_source4)\n",
    "dat_source4 = dat_source4.iloc[:8760,:9]\n",
    "\n",
    "dat_arr1 = np.array(dat_source1)\n",
    "dat_arr2 = np.array(dat_source2)\n",
    "dat_arr3 = np.array(dat_source3)\n",
    "dat_arr4 = np.array(dat_source4)\n",
    "\n",
    "dat_arr = np.concatenate([dat_arr1, dat_arr2, dat_arr3, dat_arr4],axis=0)\n",
    "dat_arr.shape # 8784, 8761, 8760, 8760\n",
    "\n",
    "dat_source = pd.DataFrame(dat_arr, columns=['Time-Date stamp','Date','ERCOT Load','Total Wind Output','Total Wind Installed','Wind Output, % of Load','Wind Output, % of Installed','1-hr MW change','1-hr % change'])\n",
    "dat = pd.DataFrame()\n",
    "dat['MW % change'] = np.array(dat_source.iloc[1:,3], dtype=float)-np.array(dat_source.iloc[0:-1,3], dtype=float)\n",
    "dat['MW % change'] = np.divide(np.array(dat['MW % change']),np.array(dat_source.iloc[1:,4]))*100\n",
    "dat['Total Wind Output'] = np.divide(np.array(dat_source.iloc[1:,3]), np.array(dat_source.iloc[1:,4]))\n",
    "\n",
    "ramp_ratio = 20\n",
    "load_ratio = 10\n",
    "year_arr = np.concatenate([np.ones(8784)*2020, np.ones(8760)*2021, np.ones(8760)*2022, np.ones(8760)*2023])\n",
    "year_df = pd.DataFrame(year_arr, columns=['Year'], dtype=int)\n",
    "wind_df = pd.concat([year_df, dat], axis=1)\n",
    "wind_df = wind_df.reset_index(drop=True)\n",
    "\n",
    "std_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_load = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_ramp = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "wind_df[['Total Wind Output']] = std_scaler.fit_transform(wind_df[['Total Wind Output']])\n",
    "wind_df[['MW % change']] = std_scaler_ramp.fit_transform(wind_df[['MW % change']])\n",
    "wind_df = wind_df.iloc[:-1,:]\n",
    "ramp_df = pd.DataFrame(wind_df)\n",
    "ramp_df.columns=['Year', 'Wind Change', 'Wind Output']\n",
    "ramp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4360729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, timesteps, output_timesteps, leadtime):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - timesteps -output_timesteps - leadtime - 1):\n",
    "        a = dataset[i:(i + timesteps), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[(i + timesteps + leadtime):(i+timesteps+output_timesteps+leadtime), :])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def svdadd(X, start, end, b_size):\n",
    "    dec_x = X[start:end, :, :]\n",
    "    U, S, Vh = svd(dec_x.reshape(b_size, -1), full_matrices=True)\n",
    "\n",
    "    high_sig = np.matmul(np.matmul(U[:,:high_ind], np.diag(S[0:high_ind])), Vh[:high_ind,:])\n",
    "    low_sig = X[:dec_num, :, :].reshape(b_size,-1)-high_sig\n",
    "\n",
    "    rec_x = np.zeros((b_size, b_size, high_ind))\n",
    "    \n",
    "    for i in range(high_ind):\n",
    "        rec_x[:,:,i] = np.matmul((U[:,i]*S[i]).reshape(-1,1), Vh[i,:].reshape(1,-1))    \n",
    "    return rec_x, low_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f57d5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = np.array(wind_df.iloc[:,1]).reshape(-1,1)\n",
    "timesteps = 24*7\n",
    "output_timesteps = 24\n",
    "leadtime = 6\n",
    "num_features = 1\n",
    "X, Y = create_dataset(norm_df, timesteps, output_timesteps, leadtime)\n",
    "b_size = timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5662125",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_num = timesteps\n",
    "high_ind = 35\n",
    "\n",
    "addX = np.zeros((X.shape[0], dec_num, high_ind+1))\n",
    "\n",
    "for k in range(X.shape[0]//dec_num):\n",
    "    rec_x, low_sig = svdadd(X, k*dec_num, (k+1)*dec_num, b_size)\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, :-1] = rec_x\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, -1] = low_sig\n",
    "    \n",
    "rec_x, low_sig = svdadd(X, X.shape[0]-dec_num, X.shape[0], b_size)\n",
    "addX[X.shape[0]-dec_num:X.shape[0], :, :-1] = rec_x\n",
    "addX[X.shape[0]-dec_num:X.shape[0],:, -1] = low_sig\n",
    "\n",
    "addX_det = addX[:, :, :-1]\n",
    "addX_det = np.sum(addX_det, axis=2)\n",
    "X = np.concatenate([X, addX], axis=2)\n",
    "num_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c40fbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31377, 168, 37), (31377, 24), (3487, 168, 37), (3487, 24))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trvaX, teX, trvaY, teY = train_test_split(X, Y, test_size=0.1, shuffle=False)\n",
    "\n",
    "idx = np.arange(trvaX.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "strvaX = trvaX[idx]\n",
    "strvaY = trvaY[idx]\n",
    "strvaY = strvaY.reshape(-1, output_timesteps)\n",
    "\n",
    "trvaY = trvaY.reshape(-1, output_timesteps)\n",
    "teY = teY.reshape(-1, output_timesteps)\n",
    "\n",
    "strX, svaX, strY, svaY = train_test_split(strvaX, strvaY, test_size=0.3, shuffle=False)\n",
    "trX, vaX, trY, vaY = train_test_split(trvaX, trvaY, test_size=0.3, shuffle=False)\n",
    "\n",
    "trvaX.shape, trvaY.shape, teX.shape, teY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c9c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAEMD(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true-K.mean(y_true)))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b813f7",
   "metadata": {},
   "source": [
    "## N-Beats Model - With SVD With Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ece6139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBeatsNet:\n",
    "    GENERIC_BLOCK = 'generic'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "\n",
    "    _BACKCAST = 'backcast'\n",
    "    _FORECAST = 'forecast'\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim=1,\n",
    "                 output_dim=1,\n",
    "                 exo_dim=0,\n",
    "                 backcast_length=10,\n",
    "                 forecast_length=1,\n",
    "                 stack_types=(TREND_BLOCK, SEASONALITY_BLOCK),\n",
    "                 nb_blocks_per_stack=3,\n",
    "                 thetas_dim=(4, 8),\n",
    "                 share_weights_in_stack=False,\n",
    "                 hidden_layer_units=256,\n",
    "                 nb_harmonics=None):\n",
    "\n",
    "        self.stack_types = stack_types\n",
    "        self.nb_blocks_per_stack = nb_blocks_per_stack\n",
    "        self.thetas_dim = thetas_dim\n",
    "        self.units = hidden_layer_units\n",
    "        self.share_weights_in_stack = share_weights_in_stack\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.exo_dim = exo_dim\n",
    "        self.input_shape = (self.backcast_length, self.input_dim)\n",
    "        self.exo_shape = (self.backcast_length, self.exo_dim)\n",
    "        self.output_shape = (self.forecast_length, self.output_dim)\n",
    "        self.weights = {}\n",
    "        self.nb_harmonics = nb_harmonics\n",
    "        self._gen_intermediate_outputs = False\n",
    "        self._intermediary_outputs = []\n",
    "        assert len(self.stack_types) == len(self.thetas_dim)\n",
    "\n",
    "        x = Input(shape=self.input_shape, name='input_variable')\n",
    "        x_ = {}\n",
    "        for k in range(self.input_dim):\n",
    "            x_[k] = Lambda(lambda z: z[..., k])(x)\n",
    "        e_ = {}\n",
    "        if self.has_exog():\n",
    "            e = Input(shape=self.exo_shape, name='exos_variables')\n",
    "            for k in range(self.exo_dim):\n",
    "                e_[k] = Lambda(lambda z: z[..., k])(e)\n",
    "        else:\n",
    "            e = None\n",
    "        y_ = {}\n",
    "\n",
    "        for stack_id in range(len(self.stack_types)):\n",
    "            stack_type = self.stack_types[stack_id]\n",
    "            nb_poly = self.thetas_dim[stack_id]\n",
    "            for block_id in range(self.nb_blocks_per_stack):\n",
    "                backcast, forecast = self.create_block(x_, e_, stack_id, block_id, stack_type, nb_poly)\n",
    "                for k in range(self.input_dim):\n",
    "                    x_[k] = Subtract()([x_[k], backcast[k]])\n",
    "                    layer_name = f'stack_{stack_id}-{stack_type.title()}Block_{block_id}'\n",
    "                    if self.input_dim >= 1:\n",
    "                        layer_name += f'_Dim_{k}'\n",
    "                    # rename.\n",
    "                    forecast[k] = Lambda(function=lambda _x: _x, name=layer_name)(forecast[k])\n",
    "                    if stack_id == 0 and block_id == 0:\n",
    "                        y_[k] = forecast[k]\n",
    "                    else:\n",
    "                        y_[k] = Add()([y_[k], forecast[k]])\n",
    "\n",
    "        for k in range(self.input_dim):\n",
    "            y_[k] = Reshape(target_shape=(self.forecast_length, 1))(y_[k])\n",
    "            x_[k] = Reshape(target_shape=(self.backcast_length, 1))(x_[k])\n",
    "        if self.input_dim > 1:\n",
    "            y_ = Concatenate()([y_[ll] for ll in range(self.input_dim)])\n",
    "            x_ = Concatenate()([x_[ll] for ll in range(self.input_dim)])\n",
    "        else:\n",
    "            y_ = y_[0]\n",
    "            x_ = x_[0]\n",
    "\n",
    "        if self.input_dim != self.output_dim:\n",
    "            y_ = Dense(self.output_dim, activation='linear', name='reg_y')(y_)\n",
    "            x_ = Dense(self.output_dim, activation='linear', name='reg_x')(x_)\n",
    "\n",
    "        inputs_x = [x, e] if self.has_exog() else x\n",
    "        n_beats_forecast = Model(inputs_x, y_, name=self._FORECAST)\n",
    "        n_beats_backcast = Model(inputs_x, x_, name=self._BACKCAST)\n",
    "\n",
    "        self.models = {model.name: model for model in [n_beats_backcast, n_beats_forecast]}\n",
    "        self.cast_type = self._FORECAST\n",
    "\n",
    "    def get_generic_and_interpretable_outputs(self):\n",
    "        g_pred = sum([a['value'][0] for a in self._intermediary_outputs if 'generic' in a['layer'].lower()])\n",
    "        i_pred = sum([a['value'][0] for a in self._intermediary_outputs if 'generic' not in a['layer'].lower()])\n",
    "        outputs = {o['layer']: o['value'][0] for o in self._intermediary_outputs}\n",
    "        return g_pred, i_pred, outputs\n",
    "\n",
    "    def has_exog(self):\n",
    "        # exo/exog is short for 'exogenous variable', i.e. any input\n",
    "        # features other than the target time-series itself.\n",
    "        return self.exo_dim > 0\n",
    "\n",
    "    @staticmethod\n",
    "    def name():\n",
    "        return 'NBeatsKeras'\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath, custom_objects=None, compile=True):\n",
    "        from tensorflow.keras.models import load_model\n",
    "        return load_model(filepath, custom_objects, compile)\n",
    "\n",
    "    def _r(self, layer_with_weights, stack_id):\n",
    "        # mechanism to restore weights when block share the same weights.\n",
    "        # only useful when share_weights_in_stack=True.\n",
    "        if self.share_weights_in_stack:\n",
    "            layer_name = layer_with_weights.name.split('/')[-1]\n",
    "            try:\n",
    "                reused_weights = self.weights[stack_id][layer_name]\n",
    "                return reused_weights\n",
    "            except KeyError:\n",
    "                pass\n",
    "            if stack_id not in self.weights:\n",
    "                self.weights[stack_id] = {}\n",
    "            self.weights[stack_id][layer_name] = layer_with_weights\n",
    "        return layer_with_weights\n",
    "\n",
    "    def disable_intermediate_outputs(self):\n",
    "        self._gen_intermediate_outputs = False\n",
    "\n",
    "    def enable_intermediate_outputs(self):\n",
    "        self._gen_intermediate_outputs = True\n",
    "\n",
    "    def create_block(self, x, e, stack_id, block_id, stack_type, nb_poly):\n",
    "        # register weights (useful when share_weights_in_stack=True)\n",
    "        def reg(layer):\n",
    "            return self._r(layer, stack_id)\n",
    "\n",
    "        # update name (useful when share_weights_in_stack=True)\n",
    "        def n(layer_name):\n",
    "            return '/'.join([str(stack_id), str(block_id), stack_type, layer_name])\n",
    "\n",
    "        backcast_ = {}\n",
    "        forecast_ = {}\n",
    "        d1 = reg(Dense(self.units, activation='relu', name=n('d1')))\n",
    "        d2 = reg(Dense(self.units, activation='relu', name=n('d2')))\n",
    "        d3 = reg(Dense(self.units, activation='relu', name=n('d3')))\n",
    "        d4 = reg(Dense(self.units, activation='relu', name=n('d4')))\n",
    "        if stack_type == 'generic':\n",
    "            theta_b = reg(Dense(nb_poly, activation='linear', use_bias=False, name=n('theta_b')))\n",
    "            theta_f = reg(Dense(nb_poly, activation='linear', use_bias=False, name=n('theta_f')))\n",
    "            backcast = reg(Dense(self.backcast_length, activation='linear', name=n('backcast')))\n",
    "            forecast = reg(Dense(self.forecast_length, activation='linear', name=n('forecast')))\n",
    "        elif stack_type == 'trend':\n",
    "            theta_f = theta_b = reg(Dense(nb_poly, activation='linear', use_bias=False, name=n('theta_f_b')))\n",
    "            backcast = Lambda(trend_model, arguments={'is_forecast': False, 'backcast_length': self.backcast_length,\n",
    "                                                      'forecast_length': self.forecast_length})\n",
    "            forecast = Lambda(trend_model, arguments={'is_forecast': True, 'backcast_length': self.backcast_length,\n",
    "                                                      'forecast_length': self.forecast_length})\n",
    "        else:  # 'seasonality'\n",
    "            if self.nb_harmonics:\n",
    "                theta_b = reg(Dense(self.nb_harmonics, activation='linear', use_bias=False, name=n('theta_b')))\n",
    "            else:\n",
    "                theta_b = reg(Dense(self.forecast_length, activation='linear', use_bias=False, name=n('theta_b')))\n",
    "            theta_f = reg(Dense(self.forecast_length, activation='linear', use_bias=False, name=n('theta_f')))\n",
    "            backcast = Lambda(seasonality_model,\n",
    "                              arguments={'is_forecast': False, 'backcast_length': self.backcast_length,\n",
    "                                         'forecast_length': self.forecast_length})\n",
    "            forecast = Lambda(seasonality_model,\n",
    "                              arguments={'is_forecast': True, 'backcast_length': self.backcast_length,\n",
    "                                         'forecast_length': self.forecast_length})\n",
    "        for k in range(self.input_dim):\n",
    "            if self.has_exog():\n",
    "                d0 = Concatenate()([x[k]] + [e[ll] for ll in range(self.exo_dim)])\n",
    "            else:\n",
    "                d0 = x[k]\n",
    "            d1_ = d1(d0)\n",
    "            d2_ = d2(d1_)\n",
    "            d3_ = d3(d2_)\n",
    "            d4_ = d4(d3_)\n",
    "            theta_f_ = theta_f(d4_)\n",
    "            theta_b_ = theta_b(d4_)\n",
    "            backcast_[k] = backcast(theta_b_)\n",
    "            forecast_[k] = forecast(theta_f_)\n",
    "\n",
    "        return backcast_, forecast_\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # https://github.com/faif/python-patterns\n",
    "        # model.predict() instead of model.n_beats.predict()\n",
    "        # same for fit(), train_on_batch()...\n",
    "        attr = getattr(self.models[self._FORECAST], name)\n",
    "\n",
    "        if not callable(attr):\n",
    "            return attr\n",
    "\n",
    "        def wrapper(*args, **kwargs):\n",
    "            cast_type = self._FORECAST\n",
    "            if attr.__name__ == 'predict' and 'return_backcast' in kwargs and kwargs['return_backcast']:\n",
    "                del kwargs['return_backcast']\n",
    "                cast_type = self._BACKCAST\n",
    "\n",
    "            if attr.__name__ == 'predict' and self._gen_intermediate_outputs:\n",
    "                import keract\n",
    "                outputs = keract.get_activations(model=self, x=args)\n",
    "                self._intermediary_outputs = [\n",
    "                    {'layer': a, 'value': b} for a, b in outputs.items() if str(a).startswith('stack_')\n",
    "                ]\n",
    "            return getattr(self.models[cast_type], attr.__name__)(*args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "\n",
    "def linear_space(backcast_length, forecast_length, is_forecast=True):\n",
    "    # ls = K.arange(-float(backcast_length), float(forecast_length), 1) / forecast_length\n",
    "    # return ls[backcast_length:] if is_forecast else K.abs(K.reverse(ls[:backcast_length], axes=0))\n",
    "    horizon = forecast_length if is_forecast else backcast_length\n",
    "    return K.arange(0, horizon) / horizon\n",
    "\n",
    "\n",
    "def seasonality_model(thetas, backcast_length, forecast_length, is_forecast):\n",
    "    p = thetas.get_shape().as_list()[-1]\n",
    "    p1, p2 = (p // 2, p // 2) if p % 2 == 0 else (p // 2, p // 2 + 1)\n",
    "    t = linear_space(backcast_length, forecast_length, is_forecast=is_forecast)\n",
    "    s1 = K.stack([K.cos(2 * np.pi * i * t) for i in range(p1)])\n",
    "    s2 = K.stack([K.sin(2 * np.pi * i * t) for i in range(p2)])\n",
    "    if p == 1:\n",
    "        s = s2\n",
    "    else:\n",
    "        s = K.concatenate([s1, s2], axis=0)\n",
    "    s = K.cast(s, np.float32)\n",
    "    return K.dot(thetas, s)\n",
    "\n",
    "\n",
    "def trend_model(thetas, backcast_length, forecast_length, is_forecast):\n",
    "    p = thetas.shape[-1]\n",
    "    t = linear_space(backcast_length, forecast_length, is_forecast=is_forecast)\n",
    "    t = K.transpose(K.stack([t ** i for i in range(p)]))\n",
    "    t = K.cast(t, np.float32)\n",
    "    return K.dot(thetas, K.transpose(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e56e214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model = NBeatsNet(input_dim=num_features, backcast_length=timesteps, forecast_length=output_timesteps,\n",
    "                stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n",
    "                nb_blocks_per_stack=4, thetas_dim=(24, 24), share_weights_in_stack=True,\n",
    "                hidden_layer_units=168)\n",
    "    #print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9cd4131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8199"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "885262b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.compile(loss=MAEMD, optimizer='adam', metrics=['mse','mae', MAEMD])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "    batch_size = 100\n",
    "    epochs = 1000\n",
    "    history = LossHistory()\n",
    "    history.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05d1a822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "131/131 [==============================] - 152s 889ms/step - loss: 0.1782 - mse: 0.0257 - mae: 0.1173 - MAEMD: 0.1781 - val_loss: 0.1414 - val_mse: 0.0182 - val_mae: 0.1066 - val_MAEMD: 0.1401\n",
      "Epoch 2/1000\n",
      "131/131 [==============================] - 107s 817ms/step - loss: 0.1350 - mse: 0.0176 - mae: 0.1054 - MAEMD: 0.1350 - val_loss: 0.1344 - val_mse: 0.0180 - val_mae: 0.1069 - val_MAEMD: 0.1332\n",
      "Epoch 3/1000\n",
      "131/131 [==============================] - 107s 816ms/step - loss: 0.1260 - mse: 0.0173 - mae: 0.1046 - MAEMD: 0.1260 - val_loss: 0.1342 - val_mse: 0.0176 - val_mae: 0.1058 - val_MAEMD: 0.1331\n",
      "Epoch 4/1000\n",
      "131/131 [==============================] - 107s 817ms/step - loss: 0.1200 - mse: 0.0169 - mae: 0.1037 - MAEMD: 0.1200 - val_loss: 0.1323 - val_mse: 0.0172 - val_mae: 0.1049 - val_MAEMD: 0.1313\n",
      "Epoch 5/1000\n",
      "131/131 [==============================] - 107s 814ms/step - loss: 0.1151 - mse: 0.0167 - mae: 0.1027 - MAEMD: 0.1151 - val_loss: 0.1309 - val_mse: 0.0169 - val_mae: 0.1038 - val_MAEMD: 0.1299\n",
      "Epoch 6/1000\n",
      "131/131 [==============================] - 107s 815ms/step - loss: 0.1111 - mse: 0.0165 - mae: 0.1020 - MAEMD: 0.1111 - val_loss: 0.1302 - val_mse: 0.0172 - val_mae: 0.1049 - val_MAEMD: 0.1292\n",
      "Epoch 7/1000\n",
      "131/131 [==============================] - 107s 816ms/step - loss: 0.1072 - mse: 0.0162 - mae: 0.1011 - MAEMD: 0.1072 - val_loss: 0.1338 - val_mse: 0.0177 - val_mae: 0.1064 - val_MAEMD: 0.1329\n",
      "Epoch 8/1000\n",
      "131/131 [==============================] - 107s 814ms/step - loss: 0.1050 - mse: 0.0161 - mae: 0.1009 - MAEMD: 0.1050 - val_loss: 0.1347 - val_mse: 0.0184 - val_mae: 0.1085 - val_MAEMD: 0.1338\n",
      "Epoch 9/1000\n",
      "131/131 [==============================] - 107s 817ms/step - loss: 0.1043 - mse: 0.0161 - mae: 0.1010 - MAEMD: 0.1042 - val_loss: 0.1317 - val_mse: 0.0187 - val_mae: 0.1092 - val_MAEMD: 0.1308\n",
      "Epoch 10/1000\n",
      "131/131 [==============================] - 107s 818ms/step - loss: 0.1010 - mse: 0.0157 - mae: 0.0994 - MAEMD: 0.1010 - val_loss: 0.1323 - val_mse: 0.0195 - val_mae: 0.1117 - val_MAEMD: 0.1313\n",
      "Epoch 11/1000\n",
      "131/131 [==============================] - 106s 808ms/step - loss: 0.0982 - mse: 0.0155 - mae: 0.0990 - MAEMD: 0.0982 - val_loss: 0.1366 - val_mse: 0.0216 - val_mae: 0.1172 - val_MAEMD: 0.1358\n",
      "Epoch 12/1000\n",
      "131/131 [==============================] - 100s 763ms/step - loss: 0.0955 - mse: 0.0153 - mae: 0.0983 - MAEMD: 0.0956 - val_loss: 0.1332 - val_mse: 0.0191 - val_mae: 0.1099 - val_MAEMD: 0.1323\n",
      "Epoch 13/1000\n",
      "131/131 [==============================] - 106s 812ms/step - loss: 0.0934 - mse: 0.0151 - mae: 0.0974 - MAEMD: 0.0934 - val_loss: 0.1343 - val_mse: 0.0182 - val_mae: 0.1074 - val_MAEMD: 0.1333\n",
      "Epoch 14/1000\n",
      "131/131 [==============================] - 107s 814ms/step - loss: 0.0919 - mse: 0.0150 - mae: 0.0971 - MAEMD: 0.0919 - val_loss: 0.1324 - val_mse: 0.0184 - val_mae: 0.1084 - val_MAEMD: 0.1314\n",
      "Epoch 15/1000\n",
      "131/131 [==============================] - 107s 814ms/step - loss: 0.0910 - mse: 0.0149 - mae: 0.0969 - MAEMD: 0.0910 - val_loss: 0.1337 - val_mse: 0.0193 - val_mae: 0.1107 - val_MAEMD: 0.1326\n",
      "Epoch 16/1000\n",
      "131/131 [==============================] - 107s 814ms/step - loss: 0.0897 - mse: 0.0149 - mae: 0.0966 - MAEMD: 0.0897 - val_loss: 0.1330 - val_mse: 0.0177 - val_mae: 0.1057 - val_MAEMD: 0.1320\n",
      "Wall time: 29min 5s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    b_size = 168\n",
    "    hist = model.fit(strX, strY, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(svaX, svaY), callbacks=[history, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c61b883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4590"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "522e9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = model.predict(teX, batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "500add32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npMAEMS(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true))*100\n",
    "def npMAEMD(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true-np.mean(y_true)))*100\n",
    "def npMSE(y_true, y_pred):\n",
    "    return np.mean(np.square(-y_true+y_pred))\n",
    "def npMAE(y_true, y_pred):\n",
    "    return np.mean(abs(-y_true+y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "902909e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.01750532470609717  MAE ==  0.10528547894813921  MAEMS ==  2.9169472387988713\n"
     ]
    }
   ],
   "source": [
    "tePredict = testPredict.reshape(-1)\n",
    "testY = teY.reshape(-1)\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict), ' MAE == ', npMAE(testY, tePredict), ' MAEMS == ', npMAEMS(testY, tePredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334acce",
   "metadata": {},
   "source": [
    "## Without Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4d96507",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model2 = NBeatsNet(input_dim=num_features, backcast_length=timesteps, forecast_length=output_timesteps,\n",
    "                stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n",
    "                nb_blocks_per_stack=4, thetas_dim=(24, 24), share_weights_in_stack=True,\n",
    "                hidden_layer_units=168)\n",
    "    #print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30618005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12188"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff6ab3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model2.compile(loss=MAEMD, optimizer='adam', metrics=['mse','mae', MAEMD])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "    batch_size = 100\n",
    "    epochs = 1000\n",
    "    history2 = LossHistory()\n",
    "    history2.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9776bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "131/131 [==============================] - 152s 881ms/step - loss: 0.2033 - mse: 0.0267 - mae: 0.1220 - MAEMD: 0.2031 - val_loss: 0.1218 - val_mse: 0.0146 - val_mae: 0.0949 - val_MAEMD: 0.1225\n",
      "Epoch 2/1000\n",
      "131/131 [==============================] - 106s 811ms/step - loss: 0.1544 - mse: 0.0195 - mae: 0.1102 - MAEMD: 0.1543 - val_loss: 0.1141 - val_mse: 0.0145 - val_mae: 0.0953 - val_MAEMD: 0.1148\n",
      "Epoch 3/1000\n",
      "131/131 [==============================] - 106s 812ms/step - loss: 0.1433 - mse: 0.0190 - mae: 0.1089 - MAEMD: 0.1432 - val_loss: 0.1128 - val_mse: 0.0144 - val_mae: 0.0948 - val_MAEMD: 0.1134\n",
      "Epoch 4/1000\n",
      "131/131 [==============================] - 106s 811ms/step - loss: 0.1365 - mse: 0.0186 - mae: 0.1077 - MAEMD: 0.1364 - val_loss: 0.1124 - val_mse: 0.0141 - val_mae: 0.0935 - val_MAEMD: 0.1129\n",
      "Epoch 5/1000\n",
      "131/131 [==============================] - 106s 811ms/step - loss: 0.1313 - mse: 0.0181 - mae: 0.1064 - MAEMD: 0.1312 - val_loss: 0.1151 - val_mse: 0.0147 - val_mae: 0.0957 - val_MAEMD: 0.1156\n",
      "Epoch 6/1000\n",
      "131/131 [==============================] - 76s 580ms/step - loss: 0.1273 - mse: 0.0180 - mae: 0.1059 - MAEMD: 0.1271 - val_loss: 0.1150 - val_mse: 0.0155 - val_mae: 0.0981 - val_MAEMD: 0.1155\n",
      "Epoch 7/1000\n",
      "131/131 [==============================] - 67s 515ms/step - loss: 0.1243 - mse: 0.0179 - mae: 0.1056 - MAEMD: 0.1242 - val_loss: 0.1150 - val_mse: 0.0164 - val_mae: 0.1007 - val_MAEMD: 0.1157\n",
      "Epoch 8/1000\n",
      "131/131 [==============================] - 68s 519ms/step - loss: 0.1227 - mse: 0.0181 - mae: 0.1063 - MAEMD: 0.1226 - val_loss: 0.1163 - val_mse: 0.0161 - val_mae: 0.0995 - val_MAEMD: 0.1170\n",
      "Epoch 9/1000\n",
      "131/131 [==============================] - 68s 516ms/step - loss: 0.1207 - mse: 0.0177 - mae: 0.1054 - MAEMD: 0.1206 - val_loss: 0.1129 - val_mse: 0.0155 - val_mae: 0.0982 - val_MAEMD: 0.1135\n",
      "Epoch 10/1000\n",
      "131/131 [==============================] - 67s 515ms/step - loss: 0.1165 - mse: 0.0175 - mae: 0.1047 - MAEMD: 0.1164 - val_loss: 0.1142 - val_mse: 0.0154 - val_mae: 0.0984 - val_MAEMD: 0.1146\n",
      "Epoch 11/1000\n",
      "131/131 [==============================] - 68s 516ms/step - loss: 0.1147 - mse: 0.0174 - mae: 0.1044 - MAEMD: 0.1146 - val_loss: 0.1157 - val_mse: 0.0158 - val_mae: 0.1000 - val_MAEMD: 0.1165\n",
      "Epoch 12/1000\n",
      "131/131 [==============================] - 68s 516ms/step - loss: 0.1150 - mse: 0.0175 - mae: 0.1046 - MAEMD: 0.1149 - val_loss: 0.1131 - val_mse: 0.0155 - val_mae: 0.0981 - val_MAEMD: 0.1138\n",
      "Epoch 13/1000\n",
      "131/131 [==============================] - 68s 517ms/step - loss: 0.1114 - mse: 0.0167 - mae: 0.1023 - MAEMD: 0.1113 - val_loss: 0.1123 - val_mse: 0.0160 - val_mae: 0.0998 - val_MAEMD: 0.1130\n",
      "Epoch 14/1000\n",
      "131/131 [==============================] - 67s 512ms/step - loss: 0.1046 - mse: 0.0161 - mae: 0.1001 - MAEMD: 0.1045 - val_loss: 0.1129 - val_mse: 0.0166 - val_mae: 0.1015 - val_MAEMD: 0.1135\n",
      "Epoch 15/1000\n",
      "131/131 [==============================] - 68s 517ms/step - loss: 0.1017 - mse: 0.0159 - mae: 0.0995 - MAEMD: 0.1016 - val_loss: 0.1115 - val_mse: 0.0159 - val_mae: 0.0999 - val_MAEMD: 0.1122\n",
      "Epoch 16/1000\n",
      "131/131 [==============================] - 68s 518ms/step - loss: 0.1003 - mse: 0.0158 - mae: 0.0992 - MAEMD: 0.1003 - val_loss: 0.1142 - val_mse: 0.0161 - val_mae: 0.1002 - val_MAEMD: 0.1151\n",
      "Epoch 17/1000\n",
      "131/131 [==============================] - 68s 516ms/step - loss: 0.1001 - mse: 0.0160 - mae: 0.0998 - MAEMD: 0.1001 - val_loss: 0.1138 - val_mse: 0.0163 - val_mae: 0.1011 - val_MAEMD: 0.1147\n",
      "Epoch 18/1000\n",
      "131/131 [==============================] - 67s 514ms/step - loss: 0.0998 - mse: 0.0157 - mae: 0.0988 - MAEMD: 0.0997 - val_loss: 0.1156 - val_mse: 0.0177 - val_mae: 0.1055 - val_MAEMD: 0.1167\n",
      "Epoch 19/1000\n",
      "131/131 [==============================] - 67s 515ms/step - loss: 0.0972 - mse: 0.0154 - mae: 0.0979 - MAEMD: 0.0971 - val_loss: 0.1141 - val_mse: 0.0168 - val_mae: 0.1024 - val_MAEMD: 0.1149\n",
      "Epoch 20/1000\n",
      "131/131 [==============================] - 67s 515ms/step - loss: 0.0965 - mse: 0.0153 - mae: 0.0977 - MAEMD: 0.0964 - val_loss: 0.1136 - val_mse: 0.0166 - val_mae: 0.1022 - val_MAEMD: 0.1142\n",
      "Epoch 21/1000\n",
      "131/131 [==============================] - 68s 516ms/step - loss: 0.0928 - mse: 0.0150 - mae: 0.0964 - MAEMD: 0.0928 - val_loss: 0.1155 - val_mse: 0.0168 - val_mae: 0.1030 - val_MAEMD: 0.1159\n",
      "Epoch 22/1000\n",
      "131/131 [==============================] - 68s 516ms/step - loss: 0.0926 - mse: 0.0151 - mae: 0.0969 - MAEMD: 0.0926 - val_loss: 0.1166 - val_mse: 0.0175 - val_mae: 0.1050 - val_MAEMD: 0.1170\n",
      "Epoch 23/1000\n",
      "131/131 [==============================] - 68s 516ms/step - loss: 0.0928 - mse: 0.0150 - mae: 0.0966 - MAEMD: 0.0927 - val_loss: 0.1181 - val_mse: 0.0189 - val_mae: 0.1088 - val_MAEMD: 0.1187\n",
      "Epoch 24/1000\n",
      "131/131 [==============================] - 66s 507ms/step - loss: 0.0924 - mse: 0.0150 - mae: 0.0966 - MAEMD: 0.0923 - val_loss: 0.1159 - val_mse: 0.0179 - val_mae: 0.1058 - val_MAEMD: 0.1167\n",
      "Epoch 25/1000\n",
      "131/131 [==============================] - 66s 503ms/step - loss: 0.0926 - mse: 0.0150 - mae: 0.0966 - MAEMD: 0.0925 - val_loss: 0.1149 - val_mse: 0.0169 - val_mae: 0.1028 - val_MAEMD: 0.1158\n",
      "Wall time: 32min 13s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    b_size = 168\n",
    "    hist2 = model2.fit(trX, trY, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(vaX, vaY), callbacks=[history2, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58b2f633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4594"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daac87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict2 = model2.predict(teX, batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6351442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.01750532470609717  MAE ==  0.10528547894813921  MAEMS ==  0.11455015854744471\n",
      "Error Test Score > MSE ==  0.016946783668618896  MAE ==  0.10342474110942583  MAEMS ==  0.11571708456195044\n"
     ]
    }
   ],
   "source": [
    "tePredict2 = testPredict2.reshape(-1)\n",
    "testY = teY.reshape(-1)\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict), ' MAE == ', npMAE(testY, tePredict), ' MAEMS == ', npMAEMD(testY, tePredict))\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict2), ' MAE == ', npMAE(testY, tePredict2), ' MAEMS == ', npMAEMD(testY, tePredict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1e34b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = testPredict.reshape(-1,24)\n",
    "testPredict2 = testPredict2.reshape(-1,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1def20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy.stats import norm\n",
    "\n",
    "def diebold_mariano_test(forecast1, forecast2, actual, opt): \n",
    "    \n",
    "    if opt==0: # MSE\n",
    "        e1 = actual-forecast1\n",
    "        e2 = actual-forecast2\n",
    "        d = e1**2 - e2**2\n",
    "    elif opt==1: # MAE\n",
    "        e1 = abs(actual-forecast1)\n",
    "        e2 = abs(actual-forecast2)\n",
    "        d = e1 - e2\n",
    "    else:\n",
    "        e1 = np.multiply(abs(actual - forecast1), np.square(actual-np.mean(actual)))\n",
    "        e2 = np.multiply(abs(actual - forecast2), np.square(actual-np.mean(actual)))\n",
    "        d = e1-e2\n",
    "    \n",
    "    # Mean of the loss differentials\n",
    "    mean_d = np.mean(d)\n",
    "    \n",
    "    # Standard deviation of the loss differentials\n",
    "    std_d = np.std(d, ddof=1)\n",
    "    \n",
    "    # Calculate the test statistic\n",
    "    test_stat = (mean_d / std_d) * np.sqrt(len(d))\n",
    "    \n",
    "    # Calculate the p-value using a two-tailed test\n",
    "    p_value = 2 * (1 - norm.cdf(abs(test_stat)))\n",
    "    \n",
    "    return test_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79106e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1.1899665010412142, 0.2340595587728349),\n",
       " (1.2011418321413896, 0.2296961884542097),\n",
       " (-0.338549580293817, 0.7349490707518083))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diebold_mariano_test(testPredict, testPredict2, teY, 0), diebold_mariano_test(testPredict, testPredict2, teY, 1), diebold_mariano_test(testPredict, testPredict2, teY, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
