{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c88c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import time\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19c483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import recall_score, precision_score # New\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.preprocessing\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Add, Multiply, Subtract\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "# regularizers\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dropout, SpatialDropout1D\n",
    "from scipy import signal\n",
    "import openpyxl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import svd\n",
    "import gc\n",
    "from keijzer import *\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.layers import Dense, Input, Flatten, Add, concatenate, Dropout, Activation, Multiply, Embedding, Layer, Reshape\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D  \n",
    "from keras.layers import Conv1D, AveragePooling1D, MaxPooling1D\n",
    "#from keras.ops import convert_to_tensor, convert_to_numpy\n",
    "#from keras.utils import plot_model\n",
    "from keras import activations\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "class LossHistory(tensorflow.keras.callbacks.Callback):  # history = LossHistory()\n",
    "    def init(self):  # history.init()\n",
    "        self.losses = []\n",
    "        # self.accs = []\n",
    "        self.val_losses = []\n",
    "        # self.val_accs = []\n",
    "        self.rmses = []\n",
    "        self.mses = []\n",
    "        self.maes = []\n",
    "        self.mapes = []\n",
    "        self.val_rmses = []\n",
    "        self.val_mses = []\n",
    "        self.val_maes = []\n",
    "        self.val_mapes = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        # self.accs.append(logs.get('acc'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        # self.val_accs.append(logs.get('val_accuracy\n",
    "        self.rmses.append(logs.get('root_mean_squared_error'))\n",
    "        self.mses.append(logs.get('mean_squared_error'))\n",
    "        self.maes.append(logs.get('mean_absolute_error'))\n",
    "        self.mapes.append(logs.get('mean_absolute_percentage_error'))\n",
    "        self.val_rmses.append(logs.get('val_root_mean_squared_error'))\n",
    "        self.val_mses.append(logs.get('val_mean_squared_error'))\n",
    "        self.val_maes.append(logs.get('val_mean_absolute_error'))\n",
    "        self.val_mapes.append(logs.get('val_mean_absolute_percentage_error'))\n",
    "\n",
    "\n",
    "\n",
    "def root_squared_mean_error(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true-K.mean(y_true)))*100\n",
    "\n",
    "def mse_mae(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))+K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8a74a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import scipy as sc\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # 여러개 사용시 '0,1,2' 식으로 하나의 문자열에 입력\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') # 호스트 러나임에 표시되는 GPU 장치 목록 반환\n",
    "\n",
    "if gpus: # 반환된 GPU 장치 목록이 있다면\n",
    "    try: # 해당 장치에 대한 메모리 증가 활성화 여부 설정\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e: # try문 실패시에 에러문구 출력\n",
    "        print(e)\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e757566d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Wind Change</th>\n",
       "      <th>Wind Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.620197</td>\n",
       "      <td>0.644724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.544580</td>\n",
       "      <td>0.658617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.570751</td>\n",
       "      <td>0.683924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.599594</td>\n",
       "      <td>0.721813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.495246</td>\n",
       "      <td>0.714187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35058</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.774643</td>\n",
       "      <td>0.484984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.735512</td>\n",
       "      <td>0.582157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.601593</td>\n",
       "      <td>0.620917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.536583</td>\n",
       "      <td>0.631322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.454969</td>\n",
       "      <td>0.606127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35063 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  Wind Change  Wind Output\n",
       "0      2020     0.620197     0.644724\n",
       "1      2020     0.544580     0.658617\n",
       "2      2020     0.570751     0.683924\n",
       "3      2020     0.599594     0.721813\n",
       "4      2020     0.495246     0.714187\n",
       "...     ...          ...          ...\n",
       "35058  2023     0.774643     0.484984\n",
       "35059  2023     0.735512     0.582157\n",
       "35060  2023     0.601593     0.620917\n",
       "35061  2023     0.536583     0.631322\n",
       "35062  2023     0.454969     0.606127\n",
       "\n",
       "[35063 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2020.csv'\n",
    "data_source1 = pd.read_csv(data_path)\n",
    "dat_source1 = pd.DataFrame(data_source1)\n",
    "#dat_source1 = dat_source1.iloc[:-3,:]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2021.csv'\n",
    "data_source2 = pd.read_csv(data_path)\n",
    "dat_source2 = pd.DataFrame(data_source2)\n",
    "dat_source2 = dat_source2.iloc[:8761,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2022.csv'\n",
    "data_source3 = pd.read_csv(data_path)\n",
    "dat_source3 = pd.DataFrame(data_source3)\n",
    "dat_source3 = dat_source3.iloc[:8760,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2023.csv'\n",
    "data_source4 = pd.read_csv(data_path)\n",
    "dat_source4 = pd.DataFrame(data_source4)\n",
    "dat_source4 = dat_source4.iloc[:8760,:9]\n",
    "\n",
    "dat_arr1 = np.array(dat_source1)\n",
    "dat_arr2 = np.array(dat_source2)\n",
    "dat_arr3 = np.array(dat_source3)\n",
    "dat_arr4 = np.array(dat_source4)\n",
    "\n",
    "dat_arr = np.concatenate([dat_arr1, dat_arr2, dat_arr3, dat_arr4],axis=0)\n",
    "dat_arr.shape # 8784, 8761, 8760, 8760\n",
    "\n",
    "dat_source = pd.DataFrame(dat_arr, columns=['Time-Date stamp','Date','ERCOT Load','Total Wind Output','Total Wind Installed','Wind Output, % of Load','Wind Output, % of Installed','1-hr MW change','1-hr % change'])\n",
    "dat = pd.DataFrame()\n",
    "dat['MW % change'] = np.array(dat_source.iloc[1:,3], dtype=float)-np.array(dat_source.iloc[0:-1,3], dtype=float)\n",
    "dat['MW % change'] = np.divide(np.array(dat['MW % change']),np.array(dat_source.iloc[1:,4]))*100\n",
    "dat['Total Wind Output'] = np.divide(np.array(dat_source.iloc[1:,3]), np.array(dat_source.iloc[1:,4]))\n",
    "\n",
    "ramp_ratio = 20\n",
    "load_ratio = 10\n",
    "year_arr = np.concatenate([np.ones(8784)*2020, np.ones(8760)*2021, np.ones(8760)*2022, np.ones(8760)*2023])\n",
    "year_df = pd.DataFrame(year_arr, columns=['Year'], dtype=int)\n",
    "wind_df = pd.concat([year_df, dat], axis=1)\n",
    "wind_df = wind_df.reset_index(drop=True)\n",
    "\n",
    "std_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_load = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_ramp = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "wind_df[['Total Wind Output']] = std_scaler.fit_transform(wind_df[['Total Wind Output']])\n",
    "wind_df[['MW % change']] = std_scaler_ramp.fit_transform(wind_df[['MW % change']])\n",
    "wind_df = wind_df.iloc[:-1,:]\n",
    "ramp_df = pd.DataFrame(wind_df)\n",
    "ramp_df.columns=['Year', 'Wind Change', 'Wind Output']\n",
    "ramp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9605f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, timesteps, output_timesteps, leadtime):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - timesteps -output_timesteps - leadtime - 1):\n",
    "        a = dataset[i:(i + timesteps), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[(i + timesteps + leadtime):(i+timesteps+output_timesteps+leadtime), :])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def svdadd(X, start, end, b_size):\n",
    "    dec_x = X[start:end, :, :]\n",
    "    U, S, Vh = svd(dec_x.reshape(b_size, -1), full_matrices=True)\n",
    "\n",
    "    high_sig = np.matmul(np.matmul(U[:,:high_ind], np.diag(S[0:high_ind])), Vh[:high_ind,:])\n",
    "    low_sig = X[:dec_num, :, :].reshape(b_size,-1)-high_sig\n",
    "\n",
    "    rec_x = np.zeros((b_size, b_size, high_ind))\n",
    "    \n",
    "    for i in range(high_ind):\n",
    "        rec_x[:,:,i] = np.matmul((U[:,i]*S[i]).reshape(-1,1), Vh[i,:].reshape(1,-1))    \n",
    "    return rec_x, low_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec71cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = np.array(wind_df.iloc[:,1]).reshape(-1,1)\n",
    "timesteps = 24*7\n",
    "output_timesteps = 24\n",
    "leadtime = 6\n",
    "num_features = 1\n",
    "X, Y = create_dataset(norm_df, timesteps, output_timesteps, leadtime)\n",
    "b_size = timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbbc7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_num = timesteps\n",
    "high_ind = 35\n",
    "\n",
    "addX = np.zeros((X.shape[0], dec_num, high_ind+1))\n",
    "\n",
    "for k in range(X.shape[0]//dec_num):\n",
    "    rec_x, low_sig = svdadd(X, k*dec_num, (k+1)*dec_num, b_size)\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, :-1] = rec_x\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, -1] = low_sig\n",
    "    \n",
    "rec_x, low_sig = svdadd(X, X.shape[0]-dec_num, X.shape[0], b_size)\n",
    "addX[X.shape[0]-dec_num:X.shape[0], :, :-1] = rec_x\n",
    "addX[X.shape[0]-dec_num:X.shape[0],:, -1] = low_sig\n",
    "\n",
    "addX_det = addX[:, :, :-1]\n",
    "addX_det = np.sum(addX_det, axis=2)\n",
    "X = np.concatenate([X, addX], axis=2)\n",
    "num_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b518cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31377, 168, 37), (31377, 24), (3487, 168, 37), (3487, 24))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trvaX, teX, trvaY, teY = train_test_split(X, Y, test_size=0.1, shuffle=False)\n",
    "\n",
    "idx = np.arange(trvaX.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "strvaX = trvaX[idx]\n",
    "strvaY = trvaY[idx]\n",
    "strvaY = strvaY.reshape(-1, output_timesteps)\n",
    "\n",
    "trvaY = trvaY.reshape(-1, output_timesteps)\n",
    "teY = teY.reshape(-1, output_timesteps)\n",
    "\n",
    "strX, svaX, strY, svaY = train_test_split(strvaX, strvaY, test_size=0.3, shuffle=False)\n",
    "trX, vaX, trY, vaY = train_test_split(trvaX, trvaY, test_size=0.3, shuffle=False)\n",
    "\n",
    "trvaX.shape, trvaY.shape, teX.shape, teY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0329b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAEMD(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true-K.mean(y_true)))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e32467",
   "metadata": {},
   "source": [
    "## Transformer Model - With SVD With Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5549596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = trvaX.shape[2]\n",
    "output_timesteps = trvaY.shape[1]\n",
    "transformer_block_num = 10\n",
    "mlp_units = [24*7*5]\n",
    "mlp_dropout = 0.4\n",
    "dropout = 0.25\n",
    "\n",
    "head_size=24\n",
    "num_heads=4\n",
    "ff_dim=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baa0a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    inputs = keras.Input(shape=(timesteps, num_features))\n",
    "    x = inputs\n",
    "    \n",
    "    for _ in range(transformer_block_num):\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "\n",
    "        res = x + inputs\n",
    "\n",
    "        # Feed Forward Part\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "\n",
    "        x = x + res\n",
    "        \n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "    # output layer\n",
    "    outputs = layers.Dense(output_timesteps)(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "977baf81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1943"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fae6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.compile(loss=MAEMD, optimizer='adam', metrics=['mse','mae', MAEMD])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    batch_size = 100\n",
    "    epochs = 1000\n",
    "    history = LossHistory()\n",
    "    history.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff0f124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "131/131 [==============================] - 30s 175ms/step - loss: 0.2131 - mse: 0.0271 - mae: 0.1210 - MAEMD: 0.2130 - val_loss: 0.1932 - val_mse: 0.0123 - val_mae: 0.0847 - val_MAEMD: 0.1938\n",
      "Epoch 2/1000\n",
      "131/131 [==============================] - 21s 164ms/step - loss: 0.1931 - mse: 0.0151 - mae: 0.0956 - MAEMD: 0.1931 - val_loss: 0.1853 - val_mse: 0.0118 - val_mae: 0.0833 - val_MAEMD: 0.1860\n",
      "Epoch 3/1000\n",
      "131/131 [==============================] - 21s 164ms/step - loss: 0.1678 - mse: 0.0164 - mae: 0.1007 - MAEMD: 0.1678 - val_loss: 0.1534 - val_mse: 0.0192 - val_mae: 0.1106 - val_MAEMD: 0.1542\n",
      "Epoch 4/1000\n",
      "131/131 [==============================] - 22s 165ms/step - loss: 0.1459 - mse: 0.0174 - mae: 0.1043 - MAEMD: 0.1459 - val_loss: 0.1454 - val_mse: 0.0106 - val_mae: 0.0793 - val_MAEMD: 0.1460\n",
      "Epoch 5/1000\n",
      "131/131 [==============================] - 22s 165ms/step - loss: 0.1371 - mse: 0.0174 - mae: 0.1039 - MAEMD: 0.1371 - val_loss: 0.1651 - val_mse: 0.0105 - val_mae: 0.0787 - val_MAEMD: 0.1656\n",
      "Epoch 6/1000\n",
      "131/131 [==============================] - 22s 165ms/step - loss: 0.1329 - mse: 0.0170 - mae: 0.1027 - MAEMD: 0.1329 - val_loss: 0.1653 - val_mse: 0.0110 - val_mae: 0.0806 - val_MAEMD: 0.1659\n",
      "Epoch 7/1000\n",
      "131/131 [==============================] - 22s 165ms/step - loss: 0.1319 - mse: 0.0168 - mae: 0.1021 - MAEMD: 0.1319 - val_loss: 0.1683 - val_mse: 0.0122 - val_mae: 0.0863 - val_MAEMD: 0.1688\n",
      "Epoch 8/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1300 - mse: 0.0166 - mae: 0.1013 - MAEMD: 0.1299 - val_loss: 0.1642 - val_mse: 0.0097 - val_mae: 0.0751 - val_MAEMD: 0.1649\n",
      "Epoch 9/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1294 - mse: 0.0164 - mae: 0.1007 - MAEMD: 0.1294 - val_loss: 0.1645 - val_mse: 0.0095 - val_mae: 0.0741 - val_MAEMD: 0.1651\n",
      "Epoch 10/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1280 - mse: 0.0161 - mae: 0.0999 - MAEMD: 0.1280 - val_loss: 0.1654 - val_mse: 0.0096 - val_mae: 0.0747 - val_MAEMD: 0.1661\n",
      "Epoch 11/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1271 - mse: 0.0159 - mae: 0.0994 - MAEMD: 0.1271 - val_loss: 0.1649 - val_mse: 0.0095 - val_mae: 0.0745 - val_MAEMD: 0.1656\n",
      "Epoch 12/1000\n",
      "131/131 [==============================] - 22s 165ms/step - loss: 0.1257 - mse: 0.0157 - mae: 0.0985 - MAEMD: 0.1256 - val_loss: 0.1706 - val_mse: 0.0111 - val_mae: 0.0813 - val_MAEMD: 0.1714\n",
      "Epoch 13/1000\n",
      "131/131 [==============================] - 22s 165ms/step - loss: 0.1251 - mse: 0.0156 - mae: 0.0981 - MAEMD: 0.1250 - val_loss: 0.1675 - val_mse: 0.0093 - val_mae: 0.0731 - val_MAEMD: 0.1682\n",
      "Epoch 14/1000\n",
      "131/131 [==============================] - 22s 165ms/step - loss: 0.1240 - mse: 0.0154 - mae: 0.0976 - MAEMD: 0.1240 - val_loss: 0.1667 - val_mse: 0.0095 - val_mae: 0.0742 - val_MAEMD: 0.1672\n",
      "Wall time: 5min 11s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    b_size = 168\n",
    "    hist = model.fit(strX, strY, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(svaX, svaY), callbacks=[history, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "117aaf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4590"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a10709c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = model.predict(teX, batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac5bd29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npMAEMS(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true))*100\n",
    "def npMAEMD(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true-np.mean(y_true)))*100\n",
    "def npMSE(y_true, y_pred):\n",
    "    return np.mean(np.square(-y_true+y_pred))\n",
    "def npMAE(y_true, y_pred):\n",
    "    return np.mean(abs(-y_true+y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c4dde9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.009893181153494341  MAE ==  0.07725442515845178  MAEMD ==  0.11526116201100013\n"
     ]
    }
   ],
   "source": [
    "tePredict = testPredict.reshape(-1)\n",
    "testY = teY.reshape(-1)\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict), ' MAE == ', npMAE(testY, tePredict), ' MAEMD == ', npMAEMD(testY, tePredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d728c9e",
   "metadata": {},
   "source": [
    "## Without Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67488728",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    inputs = keras.Input(shape=(timesteps, num_features))\n",
    "    x = inputs\n",
    "    \n",
    "    for _ in range(transformer_block_num):\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "\n",
    "        res = x + inputs\n",
    "\n",
    "        # Feed Forward Part\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "\n",
    "        x = x + res\n",
    "        \n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "    # output layer\n",
    "    outputs = layers.Dense(output_timesteps)(x)\n",
    "\n",
    "    model2 = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7dc8114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3613"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3cd55cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model2.compile(loss=MAEMD, optimizer='adam', metrics=['mse','mae', MAEMD])\n",
    "    early_stopping2 =EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    batch_size = 100\n",
    "    epochs = 1000\n",
    "    history2 = LossHistory()\n",
    "    history2.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49bc86d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "131/131 [==============================] - 29s 172ms/step - loss: 0.2255 - mse: 0.0252 - mae: 0.1203 - MAEMD: 0.2254 - val_loss: 0.1550 - val_mse: 0.0105 - val_mae: 0.0799 - val_MAEMD: 0.1563\n",
      "Epoch 2/1000\n",
      "131/131 [==============================] - 22s 165ms/step - loss: 0.2114 - mse: 0.0172 - mae: 0.1029 - MAEMD: 0.2113 - val_loss: 0.1513 - val_mse: 0.0108 - val_mae: 0.0821 - val_MAEMD: 0.1526\n",
      "Epoch 3/1000\n",
      "131/131 [==============================] - 22s 165ms/step - loss: 0.1965 - mse: 0.0179 - mae: 0.1054 - MAEMD: 0.1963 - val_loss: 0.1157 - val_mse: 0.0132 - val_mae: 0.0912 - val_MAEMD: 0.1174\n",
      "Epoch 4/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1696 - mse: 0.0185 - mae: 0.1067 - MAEMD: 0.1695 - val_loss: 0.1298 - val_mse: 0.0099 - val_mae: 0.0772 - val_MAEMD: 0.1304\n",
      "Epoch 5/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1579 - mse: 0.0181 - mae: 0.1052 - MAEMD: 0.1578 - val_loss: 0.1301 - val_mse: 0.0086 - val_mae: 0.0710 - val_MAEMD: 0.1309\n",
      "Epoch 6/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1490 - mse: 0.0177 - mae: 0.1035 - MAEMD: 0.1488 - val_loss: 0.1213 - val_mse: 0.0097 - val_mae: 0.0771 - val_MAEMD: 0.1224\n",
      "Epoch 7/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1453 - mse: 0.0172 - mae: 0.1021 - MAEMD: 0.1451 - val_loss: 0.1538 - val_mse: 0.0288 - val_mae: 0.1493 - val_MAEMD: 0.1560\n",
      "Epoch 8/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1432 - mse: 0.0171 - mae: 0.1020 - MAEMD: 0.1431 - val_loss: 0.1237 - val_mse: 0.0087 - val_mae: 0.0720 - val_MAEMD: 0.1245\n",
      "Epoch 9/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1412 - mse: 0.0168 - mae: 0.1011 - MAEMD: 0.1411 - val_loss: 0.1227 - val_mse: 0.0091 - val_mae: 0.0740 - val_MAEMD: 0.1236\n",
      "Epoch 10/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1407 - mse: 0.0166 - mae: 0.1005 - MAEMD: 0.1406 - val_loss: 0.1261 - val_mse: 0.0106 - val_mae: 0.0807 - val_MAEMD: 0.1266\n",
      "Epoch 11/1000\n",
      "131/131 [==============================] - 22s 166ms/step - loss: 0.1394 - mse: 0.0165 - mae: 0.1002 - MAEMD: 0.1393 - val_loss: 0.1258 - val_mse: 0.0101 - val_mae: 0.0783 - val_MAEMD: 0.1264\n",
      "Epoch 12/1000\n",
      "131/131 [==============================] - 22s 167ms/step - loss: 0.1389 - mse: 0.0166 - mae: 0.1005 - MAEMD: 0.1388 - val_loss: 0.1372 - val_mse: 0.0200 - val_mae: 0.1204 - val_MAEMD: 0.1390\n",
      "Epoch 13/1000\n",
      "131/131 [==============================] - 22s 167ms/step - loss: 0.1374 - mse: 0.0161 - mae: 0.0990 - MAEMD: 0.1373 - val_loss: 0.1250 - val_mse: 0.0077 - val_mae: 0.0668 - val_MAEMD: 0.1258\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    hist2 = model2.fit(trX, trY, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(vaX, vaY), callbacks=[history2, early_stopping2])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce02ceea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456183"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fb57632",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict2 = model2.predict(teX, batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5db78193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.009893181153494341  MAE ==  0.07725442515845178  MAEMD ==  0.11526116201100013\n",
      "Error Test Score > MSE ==  0.01512583450839119  MAE ==  0.09766634655708042  MAEMD ==  0.11938022307021476\n"
     ]
    }
   ],
   "source": [
    "tePredict2 = testPredict2.reshape(-1)\n",
    "testY = teY.reshape(-1)\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict), ' MAE == ', npMAE(testY, tePredict), ' MAEMD == ', npMAEMD(testY, tePredict))\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict2), ' MAE == ', npMAE(testY, tePredict2), ' MAEMD == ', npMAEMD(testY, tePredict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33b036c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy.stats import norm\n",
    "\n",
    "def diebold_mariano_test(forecast1, forecast2, actual, opt): \n",
    "    \n",
    "    if opt==0: # MSE\n",
    "        e1 = actual-forecast1\n",
    "        e2 = actual-forecast2\n",
    "        d = e1**2 - e2**2\n",
    "    elif opt==1: # MAE\n",
    "        e1 = abs(actual-forecast1)\n",
    "        e2 = abs(actual-forecast2)\n",
    "        d = e1 - e2\n",
    "    else:\n",
    "        e1 = np.multiply(abs(actual - forecast1), np.square(actual-np.mean(actual)))\n",
    "        e2 = np.multiply(abs(actual - forecast2), np.square(actual-np.mean(actual)))\n",
    "        d = e1-e2\n",
    "    \n",
    "    # Mean of the loss differentials\n",
    "    mean_d = np.mean(d)\n",
    "    \n",
    "    # Standard deviation of the loss differentials\n",
    "    std_d = np.std(d, ddof=1)\n",
    "    \n",
    "    # Calculate the test statistic\n",
    "    test_stat = (mean_d / std_d) * np.sqrt(len(d))\n",
    "    \n",
    "    # Calculate the p-value using a two-tailed test\n",
    "    p_value = 2 * (1 - norm.cdf(abs(test_stat)))\n",
    "    \n",
    "    return test_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa44e9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-22.35642084442273, 0.0),\n",
       " (-22.514518015190035, 0.0),\n",
       " (-2.2353887641427233, 0.025391837111480076))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diebold_mariano_test(testPredict, testPredict2, teY, 0), diebold_mariano_test(testPredict, testPredict2, teY, 1), diebold_mariano_test(testPredict, testPredict2, teY, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc28a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
