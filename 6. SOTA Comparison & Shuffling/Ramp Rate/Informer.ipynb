{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69053916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import time\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d6a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import recall_score, precision_score # New\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.preprocessing\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Add, Multiply, Subtract\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "# regularizers\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dropout, SpatialDropout1D\n",
    "from scipy import signal\n",
    "import openpyxl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import svd\n",
    "import gc\n",
    "from keijzer import *\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.layers import Dense, Input, Flatten, Add, concatenate, Dropout, Activation, Multiply, Embedding, Layer, Reshape\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D  \n",
    "from keras.layers import Conv1D, AveragePooling1D, MaxPooling1D\n",
    "#from keras.ops import convert_to_tensor, convert_to_numpy\n",
    "#from keras.utils import plot_model\n",
    "from keras import activations\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "class LossHistory(tensorflow.keras.callbacks.Callback):  # history = LossHistory()\n",
    "    def init(self):  # history.init()\n",
    "        self.losses = []\n",
    "        # self.accs = []\n",
    "        self.val_losses = []\n",
    "        # self.val_accs = []\n",
    "        self.rmses = []\n",
    "        self.mses = []\n",
    "        self.maes = []\n",
    "        self.mapes = []\n",
    "        self.val_rmses = []\n",
    "        self.val_mses = []\n",
    "        self.val_maes = []\n",
    "        self.val_mapes = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        # self.accs.append(logs.get('acc'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        # self.val_accs.append(logs.get('val_accuracy\n",
    "        self.rmses.append(logs.get('root_mean_squared_error'))\n",
    "        self.mses.append(logs.get('mean_squared_error'))\n",
    "        self.maes.append(logs.get('mean_absolute_error'))\n",
    "        self.mapes.append(logs.get('mean_absolute_percentage_error'))\n",
    "        self.val_rmses.append(logs.get('val_root_mean_squared_error'))\n",
    "        self.val_mses.append(logs.get('val_mean_squared_error'))\n",
    "        self.val_maes.append(logs.get('val_mean_absolute_error'))\n",
    "        self.val_mapes.append(logs.get('val_mean_absolute_percentage_error'))\n",
    "\n",
    "\n",
    "\n",
    "def root_squared_mean_error(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true-K.mean(y_true)))*100\n",
    "\n",
    "def mse_mae(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))+K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac9e413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import scipy as sc\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # 여러개 사용시 '0,1,2' 식으로 하나의 문자열에 입력\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') # 호스트 러나임에 표시되는 GPU 장치 목록 반환\n",
    "\n",
    "if gpus: # 반환된 GPU 장치 목록이 있다면\n",
    "    try: # 해당 장치에 대한 메모리 증가 활성화 여부 설정\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e: # try문 실패시에 에러문구 출력\n",
    "        print(e)\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69620963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Wind Change</th>\n",
       "      <th>Wind Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.620197</td>\n",
       "      <td>0.644724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.544580</td>\n",
       "      <td>0.658617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.570751</td>\n",
       "      <td>0.683924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.599594</td>\n",
       "      <td>0.721813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.495246</td>\n",
       "      <td>0.714187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35058</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.774643</td>\n",
       "      <td>0.484984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.735512</td>\n",
       "      <td>0.582157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.601593</td>\n",
       "      <td>0.620917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.536583</td>\n",
       "      <td>0.631322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.454969</td>\n",
       "      <td>0.606127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35063 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  Wind Change  Wind Output\n",
       "0      2020     0.620197     0.644724\n",
       "1      2020     0.544580     0.658617\n",
       "2      2020     0.570751     0.683924\n",
       "3      2020     0.599594     0.721813\n",
       "4      2020     0.495246     0.714187\n",
       "...     ...          ...          ...\n",
       "35058  2023     0.774643     0.484984\n",
       "35059  2023     0.735512     0.582157\n",
       "35060  2023     0.601593     0.620917\n",
       "35061  2023     0.536583     0.631322\n",
       "35062  2023     0.454969     0.606127\n",
       "\n",
       "[35063 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2020.csv'\n",
    "data_source1 = pd.read_csv(data_path)\n",
    "dat_source1 = pd.DataFrame(data_source1)\n",
    "#dat_source1 = dat_source1.iloc[:-3,:]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2021.csv'\n",
    "data_source2 = pd.read_csv(data_path)\n",
    "dat_source2 = pd.DataFrame(data_source2)\n",
    "dat_source2 = dat_source2.iloc[:8761,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2022.csv'\n",
    "data_source3 = pd.read_csv(data_path)\n",
    "dat_source3 = pd.DataFrame(data_source3)\n",
    "dat_source3 = dat_source3.iloc[:8760,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2023.csv'\n",
    "data_source4 = pd.read_csv(data_path)\n",
    "dat_source4 = pd.DataFrame(data_source4)\n",
    "dat_source4 = dat_source4.iloc[:8760,:9]\n",
    "\n",
    "dat_arr1 = np.array(dat_source1)\n",
    "dat_arr2 = np.array(dat_source2)\n",
    "dat_arr3 = np.array(dat_source3)\n",
    "dat_arr4 = np.array(dat_source4)\n",
    "\n",
    "dat_arr = np.concatenate([dat_arr1, dat_arr2, dat_arr3, dat_arr4],axis=0)\n",
    "dat_arr.shape # 8784, 8761, 8760, 8760\n",
    "\n",
    "dat_source = pd.DataFrame(dat_arr, columns=['Time-Date stamp','Date','ERCOT Load','Total Wind Output','Total Wind Installed','Wind Output, % of Load','Wind Output, % of Installed','1-hr MW change','1-hr % change'])\n",
    "dat = pd.DataFrame()\n",
    "dat['MW % change'] = np.array(dat_source.iloc[1:,3], dtype=float)-np.array(dat_source.iloc[0:-1,3], dtype=float)\n",
    "dat['MW % change'] = np.divide(np.array(dat['MW % change']),np.array(dat_source.iloc[1:,4]))*100\n",
    "dat['Total Wind Output'] = np.divide(np.array(dat_source.iloc[1:,3]), np.array(dat_source.iloc[1:,4]))\n",
    "\n",
    "ramp_ratio = 20\n",
    "load_ratio = 10\n",
    "year_arr = np.concatenate([np.ones(8784)*2020, np.ones(8760)*2021, np.ones(8760)*2022, np.ones(8760)*2023])\n",
    "year_df = pd.DataFrame(year_arr, columns=['Year'], dtype=int)\n",
    "wind_df = pd.concat([year_df, dat], axis=1)\n",
    "wind_df = wind_df.reset_index(drop=True)\n",
    "\n",
    "std_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_load = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_ramp = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "wind_df[['Total Wind Output']] = std_scaler.fit_transform(wind_df[['Total Wind Output']])\n",
    "wind_df[['MW % change']] = std_scaler_ramp.fit_transform(wind_df[['MW % change']])\n",
    "wind_df = wind_df.iloc[:-1,:]\n",
    "ramp_df = pd.DataFrame(wind_df)\n",
    "ramp_df.columns=['Year', 'Wind Change', 'Wind Output']\n",
    "ramp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828cdf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, timesteps, output_timesteps, leadtime):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - timesteps -output_timesteps - leadtime - 1):\n",
    "        a = dataset[i:(i + timesteps), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[(i + timesteps + leadtime):(i+timesteps+output_timesteps+leadtime), :])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def svdadd(X, start, end, b_size):\n",
    "    dec_x = X[start:end, :, :]\n",
    "    U, S, Vh = svd(dec_x.reshape(b_size, -1), full_matrices=True)\n",
    "\n",
    "    high_sig = np.matmul(np.matmul(U[:,:high_ind], np.diag(S[0:high_ind])), Vh[:high_ind,:])\n",
    "    low_sig = X[:dec_num, :, :].reshape(b_size,-1)-high_sig\n",
    "\n",
    "    rec_x = np.zeros((b_size, b_size, high_ind))\n",
    "    \n",
    "    for i in range(high_ind):\n",
    "        rec_x[:,:,i] = np.matmul((U[:,i]*S[i]).reshape(-1,1), Vh[i,:].reshape(1,-1))    \n",
    "    return rec_x, low_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657cb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = np.array(wind_df.iloc[:,1]).reshape(-1,1)\n",
    "timesteps = 24*7\n",
    "output_timesteps = 24\n",
    "leadtime = 6\n",
    "num_features = 1\n",
    "X, Y = create_dataset(norm_df, timesteps, output_timesteps, leadtime)\n",
    "b_size = timesteps\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe677607",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_num = timesteps\n",
    "high_ind = 35\n",
    "\n",
    "addX = np.zeros((X.shape[0], dec_num, high_ind+1))\n",
    "\n",
    "for k in range(X.shape[0]//dec_num):\n",
    "    rec_x, low_sig = svdadd(X, k*dec_num, (k+1)*dec_num, b_size)\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, :-1] = rec_x\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, -1] = low_sig\n",
    "    \n",
    "rec_x, low_sig = svdadd(X, X.shape[0]-dec_num, X.shape[0], b_size)\n",
    "addX[X.shape[0]-dec_num:X.shape[0], :, :-1] = rec_x\n",
    "addX[X.shape[0]-dec_num:X.shape[0],:, -1] = low_sig\n",
    "\n",
    "addX_det = addX[:, :, :-1]\n",
    "addX_det = np.sum(addX_det, axis=2)\n",
    "X = np.concatenate([X, addX], axis=2)\n",
    "num_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d660834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31377, 168, 37), (31377, 24), (3487, 168, 37), (3487, 24))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trvaX, teX, trvaY, teY = train_test_split(X, Y, test_size=0.1, shuffle=False)\n",
    "\n",
    "idx = np.arange(trvaX.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "strvaX = trvaX[idx]\n",
    "strvaY = trvaY[idx]\n",
    "strvaY = strvaY.reshape(-1, output_timesteps)\n",
    "\n",
    "trvaY = trvaY.reshape(-1, output_timesteps)\n",
    "teY = teY.reshape(-1, output_timesteps)\n",
    "\n",
    "strX, svaX, strY, svaY = train_test_split(strvaX, strvaY, test_size=0.3, shuffle=False)\n",
    "trX, vaX, trY, vaY = train_test_split(trvaX, trvaY, test_size=0.3, shuffle=False)\n",
    "\n",
    "trvaX.shape, trvaY.shape, teX.shape, teY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f5bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAEMS(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true))*100\n",
    "\n",
    "def MAEMD(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true-K.mean(y_true)))*100\n",
    "\n",
    "def MAE(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true)))\n",
    "\n",
    "def MSE(y_true, y_pred):\n",
    "    return K.mean((K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ef0f0",
   "metadata": {},
   "source": [
    "## Informer Model - With SVD With Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e4b46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.depth = d_model // n_heads\n",
    "\n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        q = self.wq(inputs)\n",
    "        k = self.wk(inputs)\n",
    "        v = self.wv(inputs)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention_logits = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_attention_logits /= tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reduce_mean(output, axis=2)\n",
    "        #print(output.shape)\n",
    "        output = tf.reshape(output, (batch_size, -1, int(self.d_model / n_heads)))\n",
    "        #print(output.shape)\n",
    "        return output\n",
    "\n",
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = SelfAttention(d_model, n_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model // n_heads) \n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.attention(inputs)\n",
    "        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        #print(ffn_output.shape)\n",
    "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n",
    "\n",
    "class Informer(Model):\n",
    "    def __init__(self, input_shape, num_layers, d_model, n_heads, d_ff, output_length):\n",
    "        super(Informer, self).__init__()\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)]\n",
    "        self.flatten = layers.Flatten()  # Flatten before the final Dense layer\n",
    "        self.final_layer = layers.Dense(output_length)  # Ensure output_length matches 24\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.flatten(x)  # Flatten the output\n",
    "        return self.final_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ea0aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"informer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_layer (EncoderLayer) multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_1 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_2 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_3 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_4 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_5 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_6 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_7 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_8 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_9 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_10 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_11 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_12 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_13 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_14 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_15 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_16 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_17 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_18 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_19 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            multiple                  149208    \n",
      "=================================================================\n",
      "Total params: 502,348\n",
      "Trainable params: 502,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (168, num_features)  # None for num_feature\n",
    "num_layers = 20\n",
    "d_model = 37*4\n",
    "n_heads = 4\n",
    "d_ff = n_heads*2\n",
    "output_shape = 24\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model = Informer(input_shape=input_shape, num_layers=num_layers, d_model=d_model, \n",
    "                      n_heads=n_heads, d_ff=d_ff, output_length=output_shape)\n",
    "\n",
    "    batch_size = 168\n",
    "    example_input = tf.random.normal((batch_size, *input_shape))\n",
    "\n",
    "    output = model(example_input)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2164e01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6ae7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.compile(loss=MAEMD, optimizer='adam', metrics=['mse','mae', MAEMD])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    batch_size = 168\n",
    "    epochs = 1000\n",
    "    history = LossHistory()\n",
    "    history.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a10e402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "131/131 [==============================] - 59s 321ms/step - loss: 0.4975 - mse: 0.4264 - mae: 0.4543 - MAEMD: 0.4972 - val_loss: 0.3286 - val_mse: 0.1246 - val_mae: 0.2900 - val_MAEMD: 0.3280\n",
      "Epoch 2/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.3109 - mse: 0.1071 - mae: 0.2595 - MAEMD: 0.3108 - val_loss: 0.2330 - val_mse: 0.0451 - val_mae: 0.1723 - val_MAEMD: 0.2316\n",
      "Epoch 3/1000\n",
      "131/131 [==============================] - 38s 293ms/step - loss: 0.2538 - mse: 0.0584 - mae: 0.1923 - MAEMD: 0.2537 - val_loss: 0.2242 - val_mse: 0.0381 - val_mae: 0.1551 - val_MAEMD: 0.2237\n",
      "Epoch 4/1000\n",
      "131/131 [==============================] - 39s 294ms/step - loss: 0.2233 - mse: 0.0352 - mae: 0.1488 - MAEMD: 0.2233 - val_loss: 0.2091 - val_mse: 0.0256 - val_mae: 0.1289 - val_MAEMD: 0.2086\n",
      "Epoch 5/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.2120 - mse: 0.0270 - mae: 0.1298 - MAEMD: 0.2120 - val_loss: 0.2067 - val_mse: 0.0249 - val_mae: 0.1279 - val_MAEMD: 0.2061\n",
      "Epoch 6/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.2042 - mse: 0.0240 - mae: 0.1225 - MAEMD: 0.2042 - val_loss: 0.1785 - val_mse: 0.0315 - val_mae: 0.1414 - val_MAEMD: 0.1777\n",
      "Epoch 7/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.1610 - mse: 0.0235 - mae: 0.1218 - MAEMD: 0.1609 - val_loss: 0.1579 - val_mse: 0.0290 - val_mae: 0.1367 - val_MAEMD: 0.1572\n",
      "Epoch 8/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.1468 - mse: 0.0215 - mae: 0.1162 - MAEMD: 0.1468 - val_loss: 0.1432 - val_mse: 0.0216 - val_mae: 0.1171 - val_MAEMD: 0.1427\n",
      "Epoch 9/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.1396 - mse: 0.0197 - mae: 0.1111 - MAEMD: 0.1396 - val_loss: 0.1335 - val_mse: 0.0195 - val_mae: 0.1114 - val_MAEMD: 0.1329\n",
      "Epoch 10/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.1341 - mse: 0.0182 - mae: 0.1067 - MAEMD: 0.1340 - val_loss: 0.1272 - val_mse: 0.0170 - val_mae: 0.1035 - val_MAEMD: 0.1266\n",
      "Epoch 11/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.1288 - mse: 0.0168 - mae: 0.1024 - MAEMD: 0.1288 - val_loss: 0.1265 - val_mse: 0.0183 - val_mae: 0.1071 - val_MAEMD: 0.1259\n",
      "Epoch 12/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.1259 - mse: 0.0162 - mae: 0.1003 - MAEMD: 0.1259 - val_loss: 0.1254 - val_mse: 0.0184 - val_mae: 0.1078 - val_MAEMD: 0.1251\n",
      "Epoch 13/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.1229 - mse: 0.0157 - mae: 0.0987 - MAEMD: 0.1228 - val_loss: 0.1216 - val_mse: 0.0168 - val_mae: 0.1024 - val_MAEMD: 0.1212\n",
      "Epoch 14/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.1204 - mse: 0.0153 - mae: 0.0973 - MAEMD: 0.1204 - val_loss: 0.1200 - val_mse: 0.0169 - val_mae: 0.1032 - val_MAEMD: 0.1197\n",
      "Epoch 15/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.1179 - mse: 0.0150 - mae: 0.0966 - MAEMD: 0.1179 - val_loss: 0.1192 - val_mse: 0.0168 - val_mae: 0.1025 - val_MAEMD: 0.1188\n",
      "Epoch 16/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.1168 - mse: 0.0149 - mae: 0.0962 - MAEMD: 0.1168 - val_loss: 0.1184 - val_mse: 0.0165 - val_mae: 0.1022 - val_MAEMD: 0.1179\n",
      "Epoch 17/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.1150 - mse: 0.0147 - mae: 0.0956 - MAEMD: 0.1150 - val_loss: 0.1173 - val_mse: 0.0165 - val_mae: 0.1012 - val_MAEMD: 0.1169\n",
      "Epoch 18/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.1136 - mse: 0.0146 - mae: 0.0953 - MAEMD: 0.1136 - val_loss: 0.1154 - val_mse: 0.0156 - val_mae: 0.0986 - val_MAEMD: 0.1149\n",
      "Epoch 19/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.1122 - mse: 0.0145 - mae: 0.0948 - MAEMD: 0.1122 - val_loss: 0.1148 - val_mse: 0.0156 - val_mae: 0.0984 - val_MAEMD: 0.1144\n",
      "Epoch 20/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.1104 - mse: 0.0145 - mae: 0.0947 - MAEMD: 0.1103 - val_loss: 0.1151 - val_mse: 0.0168 - val_mae: 0.1025 - val_MAEMD: 0.1147\n",
      "Epoch 21/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.1093 - mse: 0.0145 - mae: 0.0948 - MAEMD: 0.1093 - val_loss: 0.1128 - val_mse: 0.0151 - val_mae: 0.0969 - val_MAEMD: 0.1125\n",
      "Epoch 22/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.1076 - mse: 0.0145 - mae: 0.0947 - MAEMD: 0.1076 - val_loss: 0.1110 - val_mse: 0.0147 - val_mae: 0.0955 - val_MAEMD: 0.1106\n",
      "Epoch 23/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.1057 - mse: 0.0143 - mae: 0.0943 - MAEMD: 0.1057 - val_loss: 0.1101 - val_mse: 0.0149 - val_mae: 0.0963 - val_MAEMD: 0.1096\n",
      "Epoch 24/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.1039 - mse: 0.0143 - mae: 0.0942 - MAEMD: 0.1039 - val_loss: 0.1101 - val_mse: 0.0152 - val_mae: 0.0975 - val_MAEMD: 0.1096\n",
      "Epoch 25/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.1020 - mse: 0.0143 - mae: 0.0941 - MAEMD: 0.1019 - val_loss: 0.1086 - val_mse: 0.0151 - val_mae: 0.0970 - val_MAEMD: 0.1081\n",
      "Epoch 26/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.1000 - mse: 0.0142 - mae: 0.0938 - MAEMD: 0.1000 - val_loss: 0.1069 - val_mse: 0.0143 - val_mae: 0.0940 - val_MAEMD: 0.1065\n",
      "Epoch 27/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0981 - mse: 0.0141 - mae: 0.0932 - MAEMD: 0.0981 - val_loss: 0.1058 - val_mse: 0.0148 - val_mae: 0.0965 - val_MAEMD: 0.1053\n",
      "Epoch 28/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0962 - mse: 0.0139 - mae: 0.0929 - MAEMD: 0.0962 - val_loss: 0.1052 - val_mse: 0.0138 - val_mae: 0.0929 - val_MAEMD: 0.1048\n",
      "Epoch 29/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0942 - mse: 0.0137 - mae: 0.0922 - MAEMD: 0.0942 - val_loss: 0.1028 - val_mse: 0.0139 - val_mae: 0.0929 - val_MAEMD: 0.1024\n",
      "Epoch 30/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0926 - mse: 0.0136 - mae: 0.0918 - MAEMD: 0.0925 - val_loss: 0.1016 - val_mse: 0.0134 - val_mae: 0.0911 - val_MAEMD: 0.1011\n",
      "Epoch 31/1000\n",
      "131/131 [==============================] - 39s 295ms/step - loss: 0.0908 - mse: 0.0135 - mae: 0.0914 - MAEMD: 0.0907 - val_loss: 0.1005 - val_mse: 0.0131 - val_mae: 0.0904 - val_MAEMD: 0.1000\n",
      "Epoch 32/1000\n",
      "131/131 [==============================] - 39s 295ms/step - loss: 0.0887 - mse: 0.0134 - mae: 0.0909 - MAEMD: 0.0886 - val_loss: 0.0982 - val_mse: 0.0145 - val_mae: 0.0954 - val_MAEMD: 0.0978\n",
      "Epoch 33/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0871 - mse: 0.0131 - mae: 0.0901 - MAEMD: 0.0871 - val_loss: 0.0988 - val_mse: 0.0133 - val_mae: 0.0910 - val_MAEMD: 0.0985\n",
      "Epoch 34/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0858 - mse: 0.0131 - mae: 0.0898 - MAEMD: 0.0858 - val_loss: 0.0965 - val_mse: 0.0140 - val_mae: 0.0935 - val_MAEMD: 0.0961\n",
      "Epoch 35/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0841 - mse: 0.0128 - mae: 0.0889 - MAEMD: 0.0841 - val_loss: 0.0943 - val_mse: 0.0126 - val_mae: 0.0881 - val_MAEMD: 0.0940\n",
      "Epoch 36/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0823 - mse: 0.0126 - mae: 0.0883 - MAEMD: 0.0823 - val_loss: 0.0942 - val_mse: 0.0124 - val_mae: 0.0880 - val_MAEMD: 0.0938\n",
      "Epoch 37/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0810 - mse: 0.0124 - mae: 0.0874 - MAEMD: 0.0810 - val_loss: 0.0917 - val_mse: 0.0127 - val_mae: 0.0888 - val_MAEMD: 0.0914\n",
      "Epoch 38/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0794 - mse: 0.0121 - mae: 0.0865 - MAEMD: 0.0794 - val_loss: 0.0908 - val_mse: 0.0131 - val_mae: 0.0904 - val_MAEMD: 0.0904\n",
      "Epoch 39/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0787 - mse: 0.0122 - mae: 0.0865 - MAEMD: 0.0787 - val_loss: 0.0902 - val_mse: 0.0135 - val_mae: 0.0918 - val_MAEMD: 0.0899\n",
      "Epoch 40/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0772 - mse: 0.0120 - mae: 0.0857 - MAEMD: 0.0772 - val_loss: 0.0877 - val_mse: 0.0125 - val_mae: 0.0880 - val_MAEMD: 0.0876\n",
      "Epoch 41/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0751 - mse: 0.0117 - mae: 0.0845 - MAEMD: 0.0751 - val_loss: 0.0882 - val_mse: 0.0137 - val_mae: 0.0928 - val_MAEMD: 0.0879\n",
      "Epoch 42/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0743 - mse: 0.0116 - mae: 0.0844 - MAEMD: 0.0743 - val_loss: 0.0848 - val_mse: 0.0125 - val_mae: 0.0882 - val_MAEMD: 0.0847\n",
      "Epoch 43/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0731 - mse: 0.0113 - mae: 0.0834 - MAEMD: 0.0730 - val_loss: 0.0866 - val_mse: 0.0139 - val_mae: 0.0935 - val_MAEMD: 0.0864\n",
      "Epoch 44/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0718 - mse: 0.0112 - mae: 0.0827 - MAEMD: 0.0718 - val_loss: 0.0842 - val_mse: 0.0133 - val_mae: 0.0913 - val_MAEMD: 0.0842\n",
      "Epoch 45/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0713 - mse: 0.0110 - mae: 0.0823 - MAEMD: 0.0712 - val_loss: 0.0856 - val_mse: 0.0140 - val_mae: 0.0940 - val_MAEMD: 0.0854\n",
      "Epoch 46/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0699 - mse: 0.0109 - mae: 0.0815 - MAEMD: 0.0699 - val_loss: 0.0815 - val_mse: 0.0121 - val_mae: 0.0868 - val_MAEMD: 0.0815\n",
      "Epoch 47/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0684 - mse: 0.0106 - mae: 0.0806 - MAEMD: 0.0684 - val_loss: 0.0819 - val_mse: 0.0126 - val_mae: 0.0887 - val_MAEMD: 0.0819\n",
      "Epoch 48/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0669 - mse: 0.0105 - mae: 0.0799 - MAEMD: 0.0669 - val_loss: 0.0799 - val_mse: 0.0111 - val_mae: 0.0827 - val_MAEMD: 0.0798\n",
      "Epoch 49/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0658 - mse: 0.0102 - mae: 0.0790 - MAEMD: 0.0658 - val_loss: 0.0787 - val_mse: 0.0108 - val_mae: 0.0815 - val_MAEMD: 0.0786\n",
      "Epoch 50/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0653 - mse: 0.0102 - mae: 0.0788 - MAEMD: 0.0653 - val_loss: 0.0792 - val_mse: 0.0114 - val_mae: 0.0841 - val_MAEMD: 0.0792\n",
      "Epoch 51/1000\n",
      "131/131 [==============================] - 39s 295ms/step - loss: 0.0641 - mse: 0.0100 - mae: 0.0778 - MAEMD: 0.0641 - val_loss: 0.0742 - val_mse: 0.0109 - val_mae: 0.0821 - val_MAEMD: 0.0741\n",
      "Epoch 52/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0630 - mse: 0.0098 - mae: 0.0771 - MAEMD: 0.0629 - val_loss: 0.0747 - val_mse: 0.0108 - val_mae: 0.0820 - val_MAEMD: 0.0747\n",
      "Epoch 53/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0623 - mse: 0.0097 - mae: 0.0767 - MAEMD: 0.0623 - val_loss: 0.0739 - val_mse: 0.0099 - val_mae: 0.0784 - val_MAEMD: 0.0738\n",
      "Epoch 54/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0615 - mse: 0.0096 - mae: 0.0761 - MAEMD: 0.0615 - val_loss: 0.0720 - val_mse: 0.0104 - val_mae: 0.0799 - val_MAEMD: 0.0720\n",
      "Epoch 55/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0602 - mse: 0.0093 - mae: 0.0750 - MAEMD: 0.0602 - val_loss: 0.0722 - val_mse: 0.0099 - val_mae: 0.0779 - val_MAEMD: 0.0722\n",
      "Epoch 56/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0597 - mse: 0.0093 - mae: 0.0749 - MAEMD: 0.0597 - val_loss: 0.0714 - val_mse: 0.0099 - val_mae: 0.0779 - val_MAEMD: 0.0712\n",
      "Epoch 57/1000\n",
      "131/131 [==============================] - 39s 295ms/step - loss: 0.0587 - mse: 0.0092 - mae: 0.0744 - MAEMD: 0.0587 - val_loss: 0.0701 - val_mse: 0.0097 - val_mae: 0.0767 - val_MAEMD: 0.0700\n",
      "Epoch 58/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0580 - mse: 0.0090 - mae: 0.0737 - MAEMD: 0.0580 - val_loss: 0.0708 - val_mse: 0.0097 - val_mae: 0.0769 - val_MAEMD: 0.0706\n",
      "Epoch 59/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0572 - mse: 0.0089 - mae: 0.0731 - MAEMD: 0.0572 - val_loss: 0.0688 - val_mse: 0.0098 - val_mae: 0.0774 - val_MAEMD: 0.0686\n",
      "Epoch 60/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0565 - mse: 0.0087 - mae: 0.0725 - MAEMD: 0.0565 - val_loss: 0.0692 - val_mse: 0.0094 - val_mae: 0.0758 - val_MAEMD: 0.0689\n",
      "Epoch 61/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0558 - mse: 0.0086 - mae: 0.0720 - MAEMD: 0.0558 - val_loss: 0.0675 - val_mse: 0.0094 - val_mae: 0.0759 - val_MAEMD: 0.0675\n",
      "Epoch 62/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0555 - mse: 0.0085 - mae: 0.0716 - MAEMD: 0.0555 - val_loss: 0.0674 - val_mse: 0.0097 - val_mae: 0.0770 - val_MAEMD: 0.0673\n",
      "Epoch 63/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0555 - mse: 0.0085 - mae: 0.0714 - MAEMD: 0.0555 - val_loss: 0.0642 - val_mse: 0.0090 - val_mae: 0.0739 - val_MAEMD: 0.0641\n",
      "Epoch 64/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0549 - mse: 0.0084 - mae: 0.0709 - MAEMD: 0.0549 - val_loss: 0.0644 - val_mse: 0.0088 - val_mae: 0.0732 - val_MAEMD: 0.0642\n",
      "Epoch 65/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0540 - mse: 0.0082 - mae: 0.0704 - MAEMD: 0.0540 - val_loss: 0.0634 - val_mse: 0.0085 - val_mae: 0.0722 - val_MAEMD: 0.0632\n",
      "Epoch 66/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0532 - mse: 0.0081 - mae: 0.0695 - MAEMD: 0.0532 - val_loss: 0.0644 - val_mse: 0.0082 - val_mae: 0.0710 - val_MAEMD: 0.0642\n",
      "Epoch 67/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0527 - mse: 0.0080 - mae: 0.0690 - MAEMD: 0.0526 - val_loss: 0.0619 - val_mse: 0.0084 - val_mae: 0.0716 - val_MAEMD: 0.0617\n",
      "Epoch 68/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0514 - mse: 0.0078 - mae: 0.0683 - MAEMD: 0.0514 - val_loss: 0.0613 - val_mse: 0.0083 - val_mae: 0.0712 - val_MAEMD: 0.0611\n",
      "Epoch 69/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0509 - mse: 0.0077 - mae: 0.0678 - MAEMD: 0.0508 - val_loss: 0.0622 - val_mse: 0.0087 - val_mae: 0.0729 - val_MAEMD: 0.0620\n",
      "Epoch 70/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0499 - mse: 0.0076 - mae: 0.0672 - MAEMD: 0.0499 - val_loss: 0.0598 - val_mse: 0.0083 - val_mae: 0.0712 - val_MAEMD: 0.0597\n",
      "Epoch 71/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0498 - mse: 0.0076 - mae: 0.0670 - MAEMD: 0.0498 - val_loss: 0.0588 - val_mse: 0.0085 - val_mae: 0.0720 - val_MAEMD: 0.0586\n",
      "Epoch 72/1000\n",
      "131/131 [==============================] - 39s 300ms/step - loss: 0.0491 - mse: 0.0075 - mae: 0.0665 - MAEMD: 0.0490 - val_loss: 0.0598 - val_mse: 0.0089 - val_mae: 0.0734 - val_MAEMD: 0.0597\n",
      "Epoch 73/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0492 - mse: 0.0074 - mae: 0.0664 - MAEMD: 0.0492 - val_loss: 0.0579 - val_mse: 0.0079 - val_mae: 0.0690 - val_MAEMD: 0.0577\n",
      "Epoch 74/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0491 - mse: 0.0074 - mae: 0.0660 - MAEMD: 0.0491 - val_loss: 0.0585 - val_mse: 0.0083 - val_mae: 0.0709 - val_MAEMD: 0.0584\n",
      "Epoch 75/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0486 - mse: 0.0073 - mae: 0.0658 - MAEMD: 0.0486 - val_loss: 0.0574 - val_mse: 0.0075 - val_mae: 0.0673 - val_MAEMD: 0.0572\n",
      "Epoch 76/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0489 - mse: 0.0073 - mae: 0.0658 - MAEMD: 0.0489 - val_loss: 0.0562 - val_mse: 0.0077 - val_mae: 0.0680 - val_MAEMD: 0.0560\n",
      "Epoch 77/1000\n",
      "131/131 [==============================] - 42s 320ms/step - loss: 0.0490 - mse: 0.0072 - mae: 0.0656 - MAEMD: 0.0490 - val_loss: 0.0571 - val_mse: 0.0079 - val_mae: 0.0692 - val_MAEMD: 0.0569\n",
      "Epoch 78/1000\n",
      "131/131 [==============================] - 44s 334ms/step - loss: 0.0477 - mse: 0.0071 - mae: 0.0648 - MAEMD: 0.0477 - val_loss: 0.0592 - val_mse: 0.0086 - val_mae: 0.0720 - val_MAEMD: 0.0591\n",
      "Epoch 79/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0468 - mse: 0.0070 - mae: 0.0641 - MAEMD: 0.0468 - val_loss: 0.0565 - val_mse: 0.0080 - val_mae: 0.0694 - val_MAEMD: 0.0562\n",
      "Epoch 80/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0464 - mse: 0.0069 - mae: 0.0638 - MAEMD: 0.0464 - val_loss: 0.0556 - val_mse: 0.0082 - val_mae: 0.0704 - val_MAEMD: 0.0554\n",
      "Epoch 81/1000\n",
      "131/131 [==============================] - 47s 361ms/step - loss: 0.0459 - mse: 0.0068 - mae: 0.0634 - MAEMD: 0.0459 - val_loss: 0.0541 - val_mse: 0.0074 - val_mae: 0.0667 - val_MAEMD: 0.0538\n",
      "Epoch 82/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0460 - mse: 0.0068 - mae: 0.0631 - MAEMD: 0.0460 - val_loss: 0.0536 - val_mse: 0.0073 - val_mae: 0.0660 - val_MAEMD: 0.0534\n",
      "Epoch 83/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0454 - mse: 0.0067 - mae: 0.0627 - MAEMD: 0.0454 - val_loss: 0.0546 - val_mse: 0.0072 - val_mae: 0.0656 - val_MAEMD: 0.0544\n",
      "Epoch 84/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0452 - mse: 0.0066 - mae: 0.0625 - MAEMD: 0.0452 - val_loss: 0.0578 - val_mse: 0.0077 - val_mae: 0.0684 - val_MAEMD: 0.0576\n",
      "Epoch 85/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0442 - mse: 0.0065 - mae: 0.0618 - MAEMD: 0.0442 - val_loss: 0.0520 - val_mse: 0.0073 - val_mae: 0.0660 - val_MAEMD: 0.0518\n",
      "Epoch 86/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0436 - mse: 0.0064 - mae: 0.0613 - MAEMD: 0.0436 - val_loss: 0.0521 - val_mse: 0.0073 - val_mae: 0.0661 - val_MAEMD: 0.0520\n",
      "Epoch 87/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0435 - mse: 0.0064 - mae: 0.0613 - MAEMD: 0.0435 - val_loss: 0.0515 - val_mse: 0.0069 - val_mae: 0.0640 - val_MAEMD: 0.0512\n",
      "Epoch 88/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0426 - mse: 0.0063 - mae: 0.0605 - MAEMD: 0.0426 - val_loss: 0.0510 - val_mse: 0.0070 - val_mae: 0.0647 - val_MAEMD: 0.0508\n",
      "Epoch 89/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0424 - mse: 0.0063 - mae: 0.0604 - MAEMD: 0.0424 - val_loss: 0.0508 - val_mse: 0.0066 - val_mae: 0.0627 - val_MAEMD: 0.0506\n",
      "Epoch 90/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0425 - mse: 0.0062 - mae: 0.0603 - MAEMD: 0.0425 - val_loss: 0.0515 - val_mse: 0.0069 - val_mae: 0.0639 - val_MAEMD: 0.0514\n",
      "Epoch 91/1000\n",
      "131/131 [==============================] - 39s 300ms/step - loss: 0.0427 - mse: 0.0062 - mae: 0.0602 - MAEMD: 0.0426 - val_loss: 0.0506 - val_mse: 0.0067 - val_mae: 0.0633 - val_MAEMD: 0.0505\n",
      "Epoch 92/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0419 - mse: 0.0061 - mae: 0.0596 - MAEMD: 0.0419 - val_loss: 0.0493 - val_mse: 0.0065 - val_mae: 0.0620 - val_MAEMD: 0.0492\n",
      "Epoch 93/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0420 - mse: 0.0061 - mae: 0.0598 - MAEMD: 0.0420 - val_loss: 0.0495 - val_mse: 0.0065 - val_mae: 0.0620 - val_MAEMD: 0.0493\n",
      "Epoch 94/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0420 - mse: 0.0061 - mae: 0.0596 - MAEMD: 0.0420 - val_loss: 0.0496 - val_mse: 0.0062 - val_mae: 0.0605 - val_MAEMD: 0.0495\n",
      "Epoch 95/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0425 - mse: 0.0061 - mae: 0.0597 - MAEMD: 0.0425 - val_loss: 0.0548 - val_mse: 0.0074 - val_mae: 0.0667 - val_MAEMD: 0.0547\n",
      "Epoch 96/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0429 - mse: 0.0061 - mae: 0.0597 - MAEMD: 0.0429 - val_loss: 0.0533 - val_mse: 0.0075 - val_mae: 0.0672 - val_MAEMD: 0.0532\n",
      "Epoch 97/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0419 - mse: 0.0060 - mae: 0.0592 - MAEMD: 0.0419 - val_loss: 0.0484 - val_mse: 0.0067 - val_mae: 0.0628 - val_MAEMD: 0.0482\n",
      "Epoch 98/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0416 - mse: 0.0059 - mae: 0.0587 - MAEMD: 0.0416 - val_loss: 0.0481 - val_mse: 0.0064 - val_mae: 0.0615 - val_MAEMD: 0.0478\n",
      "Epoch 99/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0408 - mse: 0.0058 - mae: 0.0582 - MAEMD: 0.0408 - val_loss: 0.0476 - val_mse: 0.0065 - val_mae: 0.0617 - val_MAEMD: 0.0474\n",
      "Epoch 100/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0403 - mse: 0.0058 - mae: 0.0577 - MAEMD: 0.0403 - val_loss: 0.0481 - val_mse: 0.0068 - val_mae: 0.0634 - val_MAEMD: 0.0479\n",
      "Epoch 101/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0396 - mse: 0.0057 - mae: 0.0573 - MAEMD: 0.0396 - val_loss: 0.0462 - val_mse: 0.0065 - val_mae: 0.0616 - val_MAEMD: 0.0461\n",
      "Epoch 102/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0393 - mse: 0.0056 - mae: 0.0569 - MAEMD: 0.0392 - val_loss: 0.0459 - val_mse: 0.0064 - val_mae: 0.0611 - val_MAEMD: 0.0456\n",
      "Epoch 103/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0389 - mse: 0.0056 - mae: 0.0566 - MAEMD: 0.0389 - val_loss: 0.0450 - val_mse: 0.0059 - val_mae: 0.0587 - val_MAEMD: 0.0448\n",
      "Epoch 104/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0387 - mse: 0.0055 - mae: 0.0565 - MAEMD: 0.0387 - val_loss: 0.0449 - val_mse: 0.0058 - val_mae: 0.0581 - val_MAEMD: 0.0447\n",
      "Epoch 105/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0380 - mse: 0.0055 - mae: 0.0560 - MAEMD: 0.0380 - val_loss: 0.0457 - val_mse: 0.0060 - val_mae: 0.0592 - val_MAEMD: 0.0456\n",
      "Epoch 106/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0380 - mse: 0.0055 - mae: 0.0560 - MAEMD: 0.0380 - val_loss: 0.0479 - val_mse: 0.0061 - val_mae: 0.0601 - val_MAEMD: 0.0477\n",
      "Epoch 107/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0378 - mse: 0.0054 - mae: 0.0558 - MAEMD: 0.0378 - val_loss: 0.0469 - val_mse: 0.0059 - val_mae: 0.0590 - val_MAEMD: 0.0469\n",
      "Epoch 108/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0374 - mse: 0.0054 - mae: 0.0555 - MAEMD: 0.0374 - val_loss: 0.0490 - val_mse: 0.0061 - val_mae: 0.0604 - val_MAEMD: 0.0488\n",
      "Epoch 109/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0380 - mse: 0.0054 - mae: 0.0557 - MAEMD: 0.0380 - val_loss: 0.0504 - val_mse: 0.0063 - val_mae: 0.0613 - val_MAEMD: 0.0503\n",
      "Epoch 110/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0379 - mse: 0.0054 - mae: 0.0556 - MAEMD: 0.0379 - val_loss: 0.0469 - val_mse: 0.0058 - val_mae: 0.0587 - val_MAEMD: 0.0467\n",
      "Epoch 111/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0377 - mse: 0.0053 - mae: 0.0554 - MAEMD: 0.0377 - val_loss: 0.0458 - val_mse: 0.0057 - val_mae: 0.0580 - val_MAEMD: 0.0456\n",
      "Epoch 112/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0376 - mse: 0.0053 - mae: 0.0552 - MAEMD: 0.0376 - val_loss: 0.0441 - val_mse: 0.0055 - val_mae: 0.0564 - val_MAEMD: 0.0440\n",
      "Epoch 113/1000\n",
      "131/131 [==============================] - 39s 300ms/step - loss: 0.0372 - mse: 0.0053 - mae: 0.0550 - MAEMD: 0.0372 - val_loss: 0.0447 - val_mse: 0.0055 - val_mae: 0.0567 - val_MAEMD: 0.0446\n",
      "Epoch 114/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0380 - mse: 0.0053 - mae: 0.0554 - MAEMD: 0.0380 - val_loss: 0.0458 - val_mse: 0.0055 - val_mae: 0.0570 - val_MAEMD: 0.0456\n",
      "Epoch 115/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0378 - mse: 0.0053 - mae: 0.0551 - MAEMD: 0.0378 - val_loss: 0.0434 - val_mse: 0.0055 - val_mae: 0.0565 - val_MAEMD: 0.0432\n",
      "Epoch 116/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0377 - mse: 0.0053 - mae: 0.0551 - MAEMD: 0.0377 - val_loss: 0.0428 - val_mse: 0.0057 - val_mae: 0.0574 - val_MAEMD: 0.0425\n",
      "Epoch 117/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0373 - mse: 0.0052 - mae: 0.0547 - MAEMD: 0.0373 - val_loss: 0.0442 - val_mse: 0.0056 - val_mae: 0.0573 - val_MAEMD: 0.0441\n",
      "Epoch 118/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0371 - mse: 0.0052 - mae: 0.0545 - MAEMD: 0.0370 - val_loss: 0.0422 - val_mse: 0.0056 - val_mae: 0.0568 - val_MAEMD: 0.0421\n",
      "Epoch 119/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0370 - mse: 0.0051 - mae: 0.0542 - MAEMD: 0.0370 - val_loss: 0.0423 - val_mse: 0.0055 - val_mae: 0.0565 - val_MAEMD: 0.0422\n",
      "Epoch 120/1000\n",
      "131/131 [==============================] - 39s 302ms/step - loss: 0.0371 - mse: 0.0052 - mae: 0.0545 - MAEMD: 0.0371 - val_loss: 0.0442 - val_mse: 0.0058 - val_mae: 0.0581 - val_MAEMD: 0.0441\n",
      "Epoch 121/1000\n",
      "131/131 [==============================] - 55s 418ms/step - loss: 0.0370 - mse: 0.0051 - mae: 0.0543 - MAEMD: 0.0370 - val_loss: 0.0432 - val_mse: 0.0056 - val_mae: 0.0572 - val_MAEMD: 0.0429\n",
      "Epoch 122/1000\n",
      "131/131 [==============================] - 43s 312ms/step - loss: 0.0373 - mse: 0.0052 - mae: 0.0544 - MAEMD: 0.0373 - val_loss: 0.0424 - val_mse: 0.0056 - val_mae: 0.0573 - val_MAEMD: 0.0423\n",
      "Epoch 123/1000\n",
      "131/131 [==============================] - 39s 300ms/step - loss: 0.0380 - mse: 0.0052 - mae: 0.0547 - MAEMD: 0.0380 - val_loss: 0.0418 - val_mse: 0.0056 - val_mae: 0.0572 - val_MAEMD: 0.0417\n",
      "Epoch 124/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0380 - mse: 0.0051 - mae: 0.0545 - MAEMD: 0.0380 - val_loss: 0.0413 - val_mse: 0.0054 - val_mae: 0.0557 - val_MAEMD: 0.0412\n",
      "Epoch 125/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0370 - mse: 0.0050 - mae: 0.0537 - MAEMD: 0.0370 - val_loss: 0.0401 - val_mse: 0.0053 - val_mae: 0.0550 - val_MAEMD: 0.0400\n",
      "Epoch 126/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0364 - mse: 0.0050 - mae: 0.0533 - MAEMD: 0.0364 - val_loss: 0.0405 - val_mse: 0.0050 - val_mae: 0.0539 - val_MAEMD: 0.0404\n",
      "Epoch 127/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0355 - mse: 0.0049 - mae: 0.0526 - MAEMD: 0.0355 - val_loss: 0.0395 - val_mse: 0.0051 - val_mae: 0.0540 - val_MAEMD: 0.0394\n",
      "Epoch 128/1000\n",
      "131/131 [==============================] - 39s 295ms/step - loss: 0.0347 - mse: 0.0048 - mae: 0.0522 - MAEMD: 0.0347 - val_loss: 0.0408 - val_mse: 0.0048 - val_mae: 0.0524 - val_MAEMD: 0.0407\n",
      "Epoch 129/1000\n",
      "131/131 [==============================] - 39s 295ms/step - loss: 0.0348 - mse: 0.0048 - mae: 0.0522 - MAEMD: 0.0348 - val_loss: 0.0393 - val_mse: 0.0049 - val_mae: 0.0530 - val_MAEMD: 0.0391\n",
      "Epoch 130/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0346 - mse: 0.0048 - mae: 0.0520 - MAEMD: 0.0346 - val_loss: 0.0406 - val_mse: 0.0050 - val_mae: 0.0538 - val_MAEMD: 0.0404\n",
      "Epoch 131/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0341 - mse: 0.0047 - mae: 0.0517 - MAEMD: 0.0340 - val_loss: 0.0405 - val_mse: 0.0049 - val_mae: 0.0532 - val_MAEMD: 0.0405\n",
      "Epoch 132/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0344 - mse: 0.0047 - mae: 0.0517 - MAEMD: 0.0344 - val_loss: 0.0405 - val_mse: 0.0049 - val_mae: 0.0529 - val_MAEMD: 0.0404\n",
      "Epoch 133/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0338 - mse: 0.0047 - mae: 0.0514 - MAEMD: 0.0338 - val_loss: 0.0397 - val_mse: 0.0049 - val_mae: 0.0532 - val_MAEMD: 0.0396\n",
      "Epoch 134/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0340 - mse: 0.0047 - mae: 0.0515 - MAEMD: 0.0340 - val_loss: 0.0400 - val_mse: 0.0050 - val_mae: 0.0540 - val_MAEMD: 0.0398\n",
      "Epoch 135/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0337 - mse: 0.0047 - mae: 0.0513 - MAEMD: 0.0337 - val_loss: 0.0397 - val_mse: 0.0048 - val_mae: 0.0524 - val_MAEMD: 0.0396\n",
      "Epoch 136/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0333 - mse: 0.0046 - mae: 0.0510 - MAEMD: 0.0333 - val_loss: 0.0385 - val_mse: 0.0049 - val_mae: 0.0531 - val_MAEMD: 0.0383\n",
      "Epoch 137/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0337 - mse: 0.0046 - mae: 0.0511 - MAEMD: 0.0337 - val_loss: 0.0378 - val_mse: 0.0049 - val_mae: 0.0528 - val_MAEMD: 0.0377\n",
      "Epoch 138/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0334 - mse: 0.0046 - mae: 0.0509 - MAEMD: 0.0334 - val_loss: 0.0435 - val_mse: 0.0056 - val_mae: 0.0572 - val_MAEMD: 0.0434\n",
      "Epoch 139/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0339 - mse: 0.0046 - mae: 0.0511 - MAEMD: 0.0340 - val_loss: 0.0491 - val_mse: 0.0059 - val_mae: 0.0597 - val_MAEMD: 0.0489\n",
      "Epoch 140/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0342 - mse: 0.0047 - mae: 0.0513 - MAEMD: 0.0342 - val_loss: 0.0443 - val_mse: 0.0054 - val_mae: 0.0566 - val_MAEMD: 0.0441\n",
      "Epoch 141/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0338 - mse: 0.0046 - mae: 0.0510 - MAEMD: 0.0338 - val_loss: 0.0543 - val_mse: 0.0065 - val_mae: 0.0633 - val_MAEMD: 0.0541\n",
      "Epoch 142/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0333 - mse: 0.0045 - mae: 0.0506 - MAEMD: 0.0333 - val_loss: 0.0466 - val_mse: 0.0056 - val_mae: 0.0576 - val_MAEMD: 0.0464\n",
      "Epoch 143/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0337 - mse: 0.0046 - mae: 0.0508 - MAEMD: 0.0337 - val_loss: 0.0454 - val_mse: 0.0054 - val_mae: 0.0566 - val_MAEMD: 0.0452\n",
      "Epoch 144/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0339 - mse: 0.0046 - mae: 0.0509 - MAEMD: 0.0338 - val_loss: 0.0394 - val_mse: 0.0049 - val_mae: 0.0531 - val_MAEMD: 0.0393\n",
      "Epoch 145/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0339 - mse: 0.0046 - mae: 0.0507 - MAEMD: 0.0339 - val_loss: 0.0368 - val_mse: 0.0047 - val_mae: 0.0518 - val_MAEMD: 0.0365\n",
      "Epoch 146/1000\n",
      "131/131 [==============================] - 39s 296ms/step - loss: 0.0341 - mse: 0.0046 - mae: 0.0509 - MAEMD: 0.0341 - val_loss: 0.0374 - val_mse: 0.0047 - val_mae: 0.0519 - val_MAEMD: 0.0372\n",
      "Epoch 147/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0337 - mse: 0.0045 - mae: 0.0505 - MAEMD: 0.0337 - val_loss: 0.0374 - val_mse: 0.0046 - val_mae: 0.0511 - val_MAEMD: 0.0372\n",
      "Epoch 148/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0339 - mse: 0.0045 - mae: 0.0505 - MAEMD: 0.0339 - val_loss: 0.0375 - val_mse: 0.0048 - val_mae: 0.0523 - val_MAEMD: 0.0374\n",
      "Epoch 149/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0336 - mse: 0.0045 - mae: 0.0502 - MAEMD: 0.0336 - val_loss: 0.0369 - val_mse: 0.0046 - val_mae: 0.0514 - val_MAEMD: 0.0367\n",
      "Epoch 150/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0328 - mse: 0.0044 - mae: 0.0497 - MAEMD: 0.0328 - val_loss: 0.0398 - val_mse: 0.0049 - val_mae: 0.0532 - val_MAEMD: 0.0396\n",
      "Epoch 151/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0323 - mse: 0.0044 - mae: 0.0494 - MAEMD: 0.0323 - val_loss: 0.0387 - val_mse: 0.0050 - val_mae: 0.0534 - val_MAEMD: 0.0385\n",
      "Epoch 152/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0321 - mse: 0.0043 - mae: 0.0491 - MAEMD: 0.0321 - val_loss: 0.0369 - val_mse: 0.0046 - val_mae: 0.0512 - val_MAEMD: 0.0367\n",
      "Epoch 153/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0316 - mse: 0.0043 - mae: 0.0488 - MAEMD: 0.0316 - val_loss: 0.0359 - val_mse: 0.0045 - val_mae: 0.0506 - val_MAEMD: 0.0358\n",
      "Epoch 154/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0316 - mse: 0.0043 - mae: 0.0488 - MAEMD: 0.0315 - val_loss: 0.0351 - val_mse: 0.0046 - val_mae: 0.0506 - val_MAEMD: 0.0350\n",
      "Epoch 155/1000\n",
      "131/131 [==============================] - 46s 351ms/step - loss: 0.0313 - mse: 0.0042 - mae: 0.0485 - MAEMD: 0.0313 - val_loss: 0.0347 - val_mse: 0.0045 - val_mae: 0.0503 - val_MAEMD: 0.0346\n",
      "Epoch 156/1000\n",
      "131/131 [==============================] - 109s 836ms/step - loss: 0.0313 - mse: 0.0042 - mae: 0.0484 - MAEMD: 0.0313 - val_loss: 0.0357 - val_mse: 0.0043 - val_mae: 0.0492 - val_MAEMD: 0.0356\n",
      "Epoch 157/1000\n",
      "131/131 [==============================] - 114s 862ms/step - loss: 0.0310 - mse: 0.0042 - mae: 0.0483 - MAEMD: 0.0310 - val_loss: 0.0362 - val_mse: 0.0045 - val_mae: 0.0507 - val_MAEMD: 0.0361\n",
      "Epoch 158/1000\n",
      "131/131 [==============================] - 112s 860ms/step - loss: 0.0308 - mse: 0.0042 - mae: 0.0482 - MAEMD: 0.0308 - val_loss: 0.0364 - val_mse: 0.0045 - val_mae: 0.0507 - val_MAEMD: 0.0362\n",
      "Epoch 159/1000\n",
      "131/131 [==============================] - 69s 530ms/step - loss: 0.0311 - mse: 0.0042 - mae: 0.0483 - MAEMD: 0.0311 - val_loss: 0.0374 - val_mse: 0.0045 - val_mae: 0.0504 - val_MAEMD: 0.0373\n",
      "Epoch 160/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0311 - mse: 0.0042 - mae: 0.0482 - MAEMD: 0.0311 - val_loss: 0.0363 - val_mse: 0.0045 - val_mae: 0.0504 - val_MAEMD: 0.0362\n",
      "Epoch 161/1000\n",
      "131/131 [==============================] - 39s 299ms/step - loss: 0.0310 - mse: 0.0042 - mae: 0.0481 - MAEMD: 0.0310 - val_loss: 0.0360 - val_mse: 0.0045 - val_mae: 0.0506 - val_MAEMD: 0.0359\n",
      "Epoch 162/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0310 - mse: 0.0042 - mae: 0.0481 - MAEMD: 0.0310 - val_loss: 0.0367 - val_mse: 0.0048 - val_mae: 0.0521 - val_MAEMD: 0.0365\n",
      "Epoch 163/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0310 - mse: 0.0042 - mae: 0.0480 - MAEMD: 0.0310 - val_loss: 0.0364 - val_mse: 0.0048 - val_mae: 0.0520 - val_MAEMD: 0.0363\n",
      "Epoch 164/1000\n",
      "131/131 [==============================] - 39s 298ms/step - loss: 0.0309 - mse: 0.0041 - mae: 0.0480 - MAEMD: 0.0309 - val_loss: 0.0371 - val_mse: 0.0046 - val_mae: 0.0515 - val_MAEMD: 0.0370\n",
      "Epoch 165/1000\n",
      "131/131 [==============================] - 39s 297ms/step - loss: 0.0309 - mse: 0.0041 - mae: 0.0479 - MAEMD: 0.0309 - val_loss: 0.0352 - val_mse: 0.0046 - val_mae: 0.0507 - val_MAEMD: 0.0350\n",
      "Wall time: 1h 52min 19s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    b_size = 168\n",
    "    hist = model.fit(strX, strY, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(svaX, svaY), callbacks=[history, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75dc13ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4594"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f48c718",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = model.predict(teX, batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e32149c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npMAEMS(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true))*100\n",
    "def npMAEMD(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true-np.mean(y_true)))*100\n",
    "def npMSE(y_true, y_pred):\n",
    "    return np.mean(np.square(-y_true+y_pred))\n",
    "def npMAE(y_true, y_pred):\n",
    "    return np.mean(abs(-y_true+y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f14cf27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.010810348994591984  MAE ==  0.08211944045758982  MAEMD ==  0.10642939353682926\n"
     ]
    }
   ],
   "source": [
    "tePredict = testPredict.reshape(-1)\n",
    "testY = teY.reshape(-1)\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict), ' MAE == ', npMAE(testY, tePredict), ' MAEMD == ', npMAEMD(testY, tePredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b5397",
   "metadata": {},
   "source": [
    "## Without Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "10303f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"informer_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_layer_126 (EncoderLa multiple                  18257     \n",
      "_________________________________________________________________\n",
      "encoder_layer_127 (EncoderLa multiple                  18257     \n",
      "_________________________________________________________________\n",
      "encoder_layer_128 (EncoderLa multiple                  18257     \n",
      "_________________________________________________________________\n",
      "encoder_layer_129 (EncoderLa multiple                  18257     \n",
      "_________________________________________________________________\n",
      "encoder_layer_130 (EncoderLa multiple                  18257     \n",
      "_________________________________________________________________\n",
      "encoder_layer_131 (EncoderLa multiple                  18257     \n",
      "_________________________________________________________________\n",
      "encoder_layer_132 (EncoderLa multiple                  18257     \n",
      "_________________________________________________________________\n",
      "encoder_layer_133 (EncoderLa multiple                  18257     \n",
      "_________________________________________________________________\n",
      "encoder_layer_134 (EncoderLa multiple                  18257     \n",
      "_________________________________________________________________\n",
      "encoder_layer_135 (EncoderLa multiple                  18257     \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_692 (Dense)            multiple                  149208    \n",
      "=================================================================\n",
      "Total params: 331,778\n",
      "Trainable params: 331,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (168, num_features)  # None for num_feature\n",
    "num_layers = 10\n",
    "d_model = 37*4\n",
    "n_heads = 4\n",
    "d_ff = n_heads*4\n",
    "output_shape = 24\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model2 = Informer(input_shape=input_shape, num_layers=num_layers, d_model=d_model, \n",
    "                      n_heads=n_heads, d_ff=d_ff, output_length=output_shape)\n",
    "\n",
    "    batch_size = 168\n",
    "    example_input = tf.random.normal((batch_size, *input_shape))\n",
    "\n",
    "    output = model2(example_input)\n",
    "    model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5df883c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3445"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e1ab2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model2.compile(loss=MAEMD, optimizer='adam', metrics=['mse','mae', MAEMD])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    batch_size = 168\n",
    "    epochs = 1000\n",
    "    history2 = LossHistory()\n",
    "    history2.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b151a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "131/131 [==============================] - 30s 163ms/step - loss: 0.5699 - mse: 0.5432 - mae: 0.4812 - MAEMD: 0.5693 - val_loss: 0.2248 - val_mse: 0.0604 - val_mae: 0.2059 - val_MAEMD: 0.2264\n",
      "Epoch 2/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.3076 - mse: 0.0931 - mae: 0.2399 - MAEMD: 0.3074 - val_loss: 0.1941 - val_mse: 0.0527 - val_mae: 0.1922 - val_MAEMD: 0.1962\n",
      "Epoch 3/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 0.2447 - mse: 0.0619 - mae: 0.1969 - MAEMD: 0.2445 - val_loss: 0.1560 - val_mse: 0.0345 - val_mae: 0.1528 - val_MAEMD: 0.1586\n",
      "Epoch 4/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 0.2111 - mse: 0.0447 - mae: 0.1668 - MAEMD: 0.2109 - val_loss: 0.1351 - val_mse: 0.0235 - val_mae: 0.1224 - val_MAEMD: 0.1371\n",
      "Epoch 5/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 0.1900 - mse: 0.0343 - mae: 0.1467 - MAEMD: 0.1898 - val_loss: 0.1173 - val_mse: 0.0192 - val_mae: 0.1099 - val_MAEMD: 0.1194\n",
      "Epoch 6/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1726 - mse: 0.0273 - mae: 0.1310 - MAEMD: 0.1725 - val_loss: 0.1118 - val_mse: 0.0154 - val_mae: 0.0978 - val_MAEMD: 0.1139\n",
      "Epoch 7/1000\n",
      "131/131 [==============================] - 20s 154ms/step - loss: 0.1597 - mse: 0.0227 - mae: 0.1189 - MAEMD: 0.1596 - val_loss: 0.1117 - val_mse: 0.0152 - val_mae: 0.0975 - val_MAEMD: 0.1136\n",
      "Epoch 8/1000\n",
      "131/131 [==============================] - 24s 184ms/step - loss: 0.1538 - mse: 0.0209 - mae: 0.1140 - MAEMD: 0.1536 - val_loss: 0.1052 - val_mse: 0.0127 - val_mae: 0.0886 - val_MAEMD: 0.1070\n",
      "Epoch 9/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 0.1503 - mse: 0.0201 - mae: 0.1117 - MAEMD: 0.1502 - val_loss: 0.1091 - val_mse: 0.0133 - val_mae: 0.0900 - val_MAEMD: 0.1105\n",
      "Epoch 10/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1475 - mse: 0.0194 - mae: 0.1097 - MAEMD: 0.1474 - val_loss: 0.1054 - val_mse: 0.0117 - val_mae: 0.0851 - val_MAEMD: 0.1068\n",
      "Epoch 11/1000\n",
      "131/131 [==============================] - 35s 265ms/step - loss: 0.1434 - mse: 0.0184 - mae: 0.1066 - MAEMD: 0.1433 - val_loss: 0.1045 - val_mse: 0.0129 - val_mae: 0.0894 - val_MAEMD: 0.1058\n",
      "Epoch 12/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1425 - mse: 0.0182 - mae: 0.1060 - MAEMD: 0.1423 - val_loss: 0.1030 - val_mse: 0.0124 - val_mae: 0.0874 - val_MAEMD: 0.1044\n",
      "Epoch 13/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1419 - mse: 0.0182 - mae: 0.1061 - MAEMD: 0.1418 - val_loss: 0.1061 - val_mse: 0.0134 - val_mae: 0.0914 - val_MAEMD: 0.1074\n",
      "Epoch 14/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1398 - mse: 0.0178 - mae: 0.1049 - MAEMD: 0.1397 - val_loss: 0.1068 - val_mse: 0.0141 - val_mae: 0.0942 - val_MAEMD: 0.1079\n",
      "Epoch 15/1000\n",
      "131/131 [==============================] - 20s 152ms/step - loss: 0.1383 - mse: 0.0173 - mae: 0.1029 - MAEMD: 0.1381 - val_loss: 0.1070 - val_mse: 0.0139 - val_mae: 0.0933 - val_MAEMD: 0.1081\n",
      "Epoch 16/1000\n",
      "131/131 [==============================] - 20s 153ms/step - loss: 0.1356 - mse: 0.0168 - mae: 0.1013 - MAEMD: 0.1355 - val_loss: 0.1015 - val_mse: 0.0126 - val_mae: 0.0886 - val_MAEMD: 0.1027\n",
      "Epoch 17/1000\n",
      "131/131 [==============================] - 20s 152ms/step - loss: 0.1346 - mse: 0.0167 - mae: 0.1011 - MAEMD: 0.1344 - val_loss: 0.1011 - val_mse: 0.0124 - val_mae: 0.0877 - val_MAEMD: 0.1022\n",
      "Epoch 18/1000\n",
      "131/131 [==============================] - 20s 153ms/step - loss: 0.1341 - mse: 0.0166 - mae: 0.1009 - MAEMD: 0.1339 - val_loss: 0.0988 - val_mse: 0.0115 - val_mae: 0.0839 - val_MAEMD: 0.0998\n",
      "Epoch 19/1000\n",
      "131/131 [==============================] - 20s 153ms/step - loss: 0.1324 - mse: 0.0164 - mae: 0.1004 - MAEMD: 0.1322 - val_loss: 0.0985 - val_mse: 0.0114 - val_mae: 0.0835 - val_MAEMD: 0.0996\n",
      "Epoch 20/1000\n",
      "131/131 [==============================] - 20s 152ms/step - loss: 0.1316 - mse: 0.0163 - mae: 0.1001 - MAEMD: 0.1314 - val_loss: 0.0984 - val_mse: 0.0113 - val_mae: 0.0832 - val_MAEMD: 0.0995\n",
      "Epoch 21/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1317 - mse: 0.0165 - mae: 0.1009 - MAEMD: 0.1316 - val_loss: 0.0980 - val_mse: 0.0115 - val_mae: 0.0840 - val_MAEMD: 0.0992\n",
      "Epoch 22/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 0.1303 - mse: 0.0165 - mae: 0.1008 - MAEMD: 0.1302 - val_loss: 0.0977 - val_mse: 0.0119 - val_mae: 0.0853 - val_MAEMD: 0.0988\n",
      "Epoch 23/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1295 - mse: 0.0163 - mae: 0.1003 - MAEMD: 0.1294 - val_loss: 0.0988 - val_mse: 0.0127 - val_mae: 0.0881 - val_MAEMD: 0.0999\n",
      "Epoch 24/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1284 - mse: 0.0164 - mae: 0.1006 - MAEMD: 0.1283 - val_loss: 0.0986 - val_mse: 0.0125 - val_mae: 0.0875 - val_MAEMD: 0.0999\n",
      "Epoch 25/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1274 - mse: 0.0163 - mae: 0.1004 - MAEMD: 0.1273 - val_loss: 0.0991 - val_mse: 0.0126 - val_mae: 0.0876 - val_MAEMD: 0.1003\n",
      "Epoch 26/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1254 - mse: 0.0161 - mae: 0.0997 - MAEMD: 0.1253 - val_loss: 0.0989 - val_mse: 0.0126 - val_mae: 0.0878 - val_MAEMD: 0.1001\n",
      "Epoch 27/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1260 - mse: 0.0163 - mae: 0.1004 - MAEMD: 0.1259 - val_loss: 0.0999 - val_mse: 0.0131 - val_mae: 0.0897 - val_MAEMD: 0.1011\n",
      "Epoch 28/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1250 - mse: 0.0163 - mae: 0.1003 - MAEMD: 0.1249 - val_loss: 0.0983 - val_mse: 0.0121 - val_mae: 0.0859 - val_MAEMD: 0.0994\n",
      "Epoch 29/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1233 - mse: 0.0160 - mae: 0.0994 - MAEMD: 0.1232 - val_loss: 0.0988 - val_mse: 0.0123 - val_mae: 0.0866 - val_MAEMD: 0.1000\n",
      "Epoch 30/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 0.1219 - mse: 0.0163 - mae: 0.1003 - MAEMD: 0.1218 - val_loss: 0.0980 - val_mse: 0.0113 - val_mae: 0.0830 - val_MAEMD: 0.0993\n",
      "Epoch 31/1000\n",
      "131/131 [==============================] - 20s 152ms/step - loss: 0.1212 - mse: 0.0160 - mae: 0.0995 - MAEMD: 0.1211 - val_loss: 0.0987 - val_mse: 0.0122 - val_mae: 0.0863 - val_MAEMD: 0.0999\n",
      "Epoch 32/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 0.1202 - mse: 0.0160 - mae: 0.0994 - MAEMD: 0.1201 - val_loss: 0.0990 - val_mse: 0.0126 - val_mae: 0.0878 - val_MAEMD: 0.1000\n",
      "Wall time: 11min 2s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    hist2 = model2.fit(trX, trY, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(vaX, vaY), callbacks=[history2, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f16a6c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704538"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6f86adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict2 = model2.predict(teX, batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7802f641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.010810348994591984  MAE ==  0.08211944045758982  MAEMD ==  0.10642939353682926\n",
      "Error Test Score > MSE ==  0.011241171126035657  MAE ==  0.08322160423327984  MAEMD ==  0.0949018921183549\n"
     ]
    }
   ],
   "source": [
    "tePredict2 = testPredict2.reshape(-1)\n",
    "testY = teY.reshape(-1)\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict), ' MAE == ', npMAE(testY, tePredict), ' MAEMD == ', npMAEMD(testY, tePredict))\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict2), ' MAE == ', npMAE(testY, tePredict2), ' MAEMD == ', npMAEMD(testY, tePredict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "86dc16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy.stats import norm\n",
    "\n",
    "def diebold_mariano_test(forecast1, forecast2, actual, opt): \n",
    "    \n",
    "    if opt==0: # MSE\n",
    "        e1 = actual-forecast1\n",
    "        e2 = actual-forecast2\n",
    "        d = e1**2 - e2**2\n",
    "    elif opt==1: # MAE\n",
    "        e1 = abs(actual-forecast1)\n",
    "        e2 = abs(actual-forecast2)\n",
    "        d = e1 - e2\n",
    "    else:\n",
    "        e1 = np.multiply(abs(actual - forecast1), (actual-np.mean(actual))**2)\n",
    "        e2 = np.multiply(abs(actual - forecast2), (actual-np.mean(actual))**2)\n",
    "        d = e1-e2\n",
    "    \n",
    "    # Mean of the loss differentials\n",
    "    mean_d = np.mean(d)\n",
    "    \n",
    "    # Standard deviation of the loss differentials\n",
    "    std_d = np.std(d, ddof=1)\n",
    "    \n",
    "    # Calculate the test statistic\n",
    "    test_stat = (mean_d / std_d) * np.sqrt(len(d))\n",
    "    \n",
    "    # Calculate the p-value using a two-tailed test\n",
    "    p_value = 2 * (1 - norm.cdf(abs(test_stat)))\n",
    "    \n",
    "    return test_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "30638f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-1.5409780152539156, 0.12332213672026038),\n",
       " (-0.964647417570333, 0.33472144064198583),\n",
       " (4.536865189273556, 5.709657849406469e-06))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diebold_mariano_test(testPredict, testPredict2, teY, 0), diebold_mariano_test(testPredict, testPredict2, teY, 1), diebold_mariano_test(testPredict, testPredict2, teY, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754528be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
