{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25a6f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# noinspection PyUnresolvedReferences\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Subtract, Add, Reshape\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e871ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import recall_score, precision_score # New\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.preprocessing\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Add, Multiply, Subtract\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "# regularizers\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dropout, SpatialDropout1D\n",
    "from scipy import signal\n",
    "import openpyxl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import svd\n",
    "import gc\n",
    "from keijzer import *\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.layers import Dense, Input, Flatten, Add, concatenate, Dropout, Activation, Multiply, Embedding, Layer, Reshape\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D  \n",
    "from keras.layers import Conv1D, AveragePooling1D, MaxPooling1D\n",
    "#from keras.ops import convert_to_tensor, convert_to_numpy\n",
    "#from keras.utils import plot_model\n",
    "from keras import activations\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "class LossHistory(tensorflow.keras.callbacks.Callback):  # history = LossHistory()\n",
    "    def init(self):  # history.init()\n",
    "        self.losses = []\n",
    "        # self.accs = []\n",
    "        self.val_losses = []\n",
    "        # self.val_accs = []\n",
    "        self.rmses = []\n",
    "        self.mses = []\n",
    "        self.maes = []\n",
    "        self.mapes = []\n",
    "        self.val_rmses = []\n",
    "        self.val_mses = []\n",
    "        self.val_maes = []\n",
    "        self.val_mapes = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        # self.accs.append(logs.get('acc'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        # self.val_accs.append(logs.get('val_accuracy\n",
    "        self.rmses.append(logs.get('root_mean_squared_error'))\n",
    "        self.mses.append(logs.get('mean_squared_error'))\n",
    "        self.maes.append(logs.get('mean_absolute_error'))\n",
    "        self.mapes.append(logs.get('mean_absolute_percentage_error'))\n",
    "        self.val_rmses.append(logs.get('val_root_mean_squared_error'))\n",
    "        self.val_mses.append(logs.get('val_mean_squared_error'))\n",
    "        self.val_maes.append(logs.get('val_mean_absolute_error'))\n",
    "        self.val_mapes.append(logs.get('val_mean_absolute_percentage_error'))\n",
    "\n",
    "\n",
    "\n",
    "def root_squared_mean_error(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true-K.mean(y_true)))*100\n",
    "\n",
    "def mse_mae(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))+K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dbbaec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import scipy as sc\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # 여러개 사용시 '0,1,2' 식으로 하나의 문자열에 입력\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') # 호스트 러나임에 표시되는 GPU 장치 목록 반환\n",
    "\n",
    "if gpus: # 반환된 GPU 장치 목록이 있다면\n",
    "    try: # 해당 장치에 대한 메모리 증가 활성화 여부 설정\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e: # try문 실패시에 에러문구 출력\n",
    "        print(e)\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7741100c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Wind Change</th>\n",
       "      <th>Wind Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.620197</td>\n",
       "      <td>0.644724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.544580</td>\n",
       "      <td>0.658617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.570751</td>\n",
       "      <td>0.683924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.599594</td>\n",
       "      <td>0.721813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.495246</td>\n",
       "      <td>0.714187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35058</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.774643</td>\n",
       "      <td>0.484984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.735512</td>\n",
       "      <td>0.582157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.601593</td>\n",
       "      <td>0.620917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.536583</td>\n",
       "      <td>0.631322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.454969</td>\n",
       "      <td>0.606127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35063 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  Wind Change  Wind Output\n",
       "0      2020     0.620197     0.644724\n",
       "1      2020     0.544580     0.658617\n",
       "2      2020     0.570751     0.683924\n",
       "3      2020     0.599594     0.721813\n",
       "4      2020     0.495246     0.714187\n",
       "...     ...          ...          ...\n",
       "35058  2023     0.774643     0.484984\n",
       "35059  2023     0.735512     0.582157\n",
       "35060  2023     0.601593     0.620917\n",
       "35061  2023     0.536583     0.631322\n",
       "35062  2023     0.454969     0.606127\n",
       "\n",
       "[35063 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2020.csv'\n",
    "data_source1 = pd.read_csv(data_path)\n",
    "dat_source1 = pd.DataFrame(data_source1)\n",
    "#dat_source1 = dat_source1.iloc[:-3,:]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2021.csv'\n",
    "data_source2 = pd.read_csv(data_path)\n",
    "dat_source2 = pd.DataFrame(data_source2)\n",
    "dat_source2 = dat_source2.iloc[:8761,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2022.csv'\n",
    "data_source3 = pd.read_csv(data_path)\n",
    "dat_source3 = pd.DataFrame(data_source3)\n",
    "dat_source3 = dat_source3.iloc[:8760,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2023.csv'\n",
    "data_source4 = pd.read_csv(data_path)\n",
    "dat_source4 = pd.DataFrame(data_source4)\n",
    "dat_source4 = dat_source4.iloc[:8760,:9]\n",
    "\n",
    "dat_arr1 = np.array(dat_source1)\n",
    "dat_arr2 = np.array(dat_source2)\n",
    "dat_arr3 = np.array(dat_source3)\n",
    "dat_arr4 = np.array(dat_source4)\n",
    "\n",
    "dat_arr = np.concatenate([dat_arr1, dat_arr2, dat_arr3, dat_arr4],axis=0)\n",
    "dat_arr.shape # 8784, 8761, 8760, 8760\n",
    "\n",
    "dat_source = pd.DataFrame(dat_arr, columns=['Time-Date stamp','Date','ERCOT Load','Total Wind Output','Total Wind Installed','Wind Output, % of Load','Wind Output, % of Installed','1-hr MW change','1-hr % change'])\n",
    "dat = pd.DataFrame()\n",
    "dat['MW % change'] = np.array(dat_source.iloc[1:,3], dtype=float)-np.array(dat_source.iloc[0:-1,3], dtype=float)\n",
    "dat['MW % change'] = np.divide(np.array(dat['MW % change']),np.array(dat_source.iloc[1:,4]))*100\n",
    "dat['Total Wind Output'] = np.divide(np.array(dat_source.iloc[1:,3]), np.array(dat_source.iloc[1:,4]))\n",
    "\n",
    "ramp_ratio = 20\n",
    "load_ratio = 10\n",
    "year_arr = np.concatenate([np.ones(8784)*2020, np.ones(8760)*2021, np.ones(8760)*2022, np.ones(8760)*2023])\n",
    "year_df = pd.DataFrame(year_arr, columns=['Year'], dtype=int)\n",
    "wind_df = pd.concat([year_df, dat], axis=1)\n",
    "wind_df = wind_df.reset_index(drop=True)\n",
    "\n",
    "std_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_load = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_ramp = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "wind_df[['Total Wind Output']] = std_scaler.fit_transform(wind_df[['Total Wind Output']])\n",
    "wind_df[['MW % change']] = std_scaler_ramp.fit_transform(wind_df[['MW % change']])\n",
    "wind_df = wind_df.iloc[:-1,:]\n",
    "ramp_df = pd.DataFrame(wind_df)\n",
    "ramp_df.columns=['Year', 'Wind Change', 'Wind Output']\n",
    "ramp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696d53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, timesteps, output_timesteps, leadtime):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - timesteps -output_timesteps - leadtime - 1):\n",
    "        a = dataset[i:(i + timesteps), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[(i + timesteps + leadtime):(i+timesteps+output_timesteps+leadtime), :])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def svdadd(X, start, end, b_size):\n",
    "    dec_x = X[start:end, :, :]\n",
    "    U, S, Vh = svd(dec_x.reshape(b_size, -1), full_matrices=True)\n",
    "\n",
    "    high_sig = np.matmul(np.matmul(U[:,:high_ind], np.diag(S[0:high_ind])), Vh[:high_ind,:])\n",
    "    low_sig = X[:dec_num, :, :].reshape(b_size,-1)-high_sig\n",
    "\n",
    "    rec_x = np.zeros((b_size, b_size, high_ind))\n",
    "    \n",
    "    for i in range(high_ind):\n",
    "        rec_x[:,:,i] = np.matmul((U[:,i]*S[i]).reshape(-1,1), Vh[i,:].reshape(1,-1))    \n",
    "    return rec_x, low_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49f6aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = np.array(wind_df.iloc[:,2]).reshape(-1,1)\n",
    "timesteps = 24*7\n",
    "output_timesteps = 24\n",
    "leadtime = 6\n",
    "num_features = 1\n",
    "X, Y = create_dataset(norm_df, timesteps, output_timesteps, leadtime)\n",
    "b_size = timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a3ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_num = timesteps\n",
    "high_ind = 35\n",
    "\n",
    "addX = np.zeros((X.shape[0], dec_num, high_ind+1))\n",
    "\n",
    "for k in range(X.shape[0]//dec_num):\n",
    "    rec_x, low_sig = svdadd(X, k*dec_num, (k+1)*dec_num, b_size)\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, :-1] = rec_x\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, -1] = low_sig\n",
    "    \n",
    "rec_x, low_sig = svdadd(X, X.shape[0]-dec_num, X.shape[0], b_size)\n",
    "addX[X.shape[0]-dec_num:X.shape[0], :, :-1] = rec_x\n",
    "addX[X.shape[0]-dec_num:X.shape[0],:, -1] = low_sig\n",
    "\n",
    "addX_det = addX[:, :, :-1]\n",
    "addX_det = np.sum(addX_det, axis=2)\n",
    "X = np.concatenate([X, addX], axis=2)\n",
    "num_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c76594d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31377, 168, 37), (31377, 24), (3487, 168, 37), (3487, 24))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trvaX, teX, trvaY, teY = train_test_split(X, Y, test_size=0.1, shuffle=False)\n",
    "\n",
    "idx = np.arange(trvaX.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "strvaX = trvaX[idx]\n",
    "strvaY = trvaY[idx]\n",
    "strvaY = strvaY.reshape(-1, output_timesteps)\n",
    "\n",
    "trvaY = trvaY.reshape(-1, output_timesteps)\n",
    "teY = teY.reshape(-1, output_timesteps)\n",
    "\n",
    "strX, svaX, strY, svaY = train_test_split(strvaX, strvaY, test_size=0.3, shuffle=False)\n",
    "trX, vaX, trY, vaY = train_test_split(trvaX, trvaY, test_size=0.3, shuffle=False)\n",
    "\n",
    "trvaX.shape, trvaY.shape, teX.shape, teY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb7570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAEMS(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efadd72e",
   "metadata": {},
   "source": [
    "## N-Beats Model - With SVD With Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2898414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBeatsNet:\n",
    "    GENERIC_BLOCK = 'generic'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "\n",
    "    _BACKCAST = 'backcast'\n",
    "    _FORECAST = 'forecast'\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim=1,\n",
    "                 output_dim=1,\n",
    "                 exo_dim=0,\n",
    "                 backcast_length=10,\n",
    "                 forecast_length=1,\n",
    "                 stack_types=(TREND_BLOCK, SEASONALITY_BLOCK),\n",
    "                 nb_blocks_per_stack=3,\n",
    "                 thetas_dim=(4, 8),\n",
    "                 share_weights_in_stack=False,\n",
    "                 hidden_layer_units=256,\n",
    "                 nb_harmonics=None):\n",
    "\n",
    "        self.stack_types = stack_types\n",
    "        self.nb_blocks_per_stack = nb_blocks_per_stack\n",
    "        self.thetas_dim = thetas_dim\n",
    "        self.units = hidden_layer_units\n",
    "        self.share_weights_in_stack = share_weights_in_stack\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.exo_dim = exo_dim\n",
    "        self.input_shape = (self.backcast_length, self.input_dim)\n",
    "        self.exo_shape = (self.backcast_length, self.exo_dim)\n",
    "        self.output_shape = (self.forecast_length, self.output_dim)\n",
    "        self.weights = {}\n",
    "        self.nb_harmonics = nb_harmonics\n",
    "        self._gen_intermediate_outputs = False\n",
    "        self._intermediary_outputs = []\n",
    "        assert len(self.stack_types) == len(self.thetas_dim)\n",
    "\n",
    "        x = Input(shape=self.input_shape, name='input_variable')\n",
    "        x_ = {}\n",
    "        for k in range(self.input_dim):\n",
    "            x_[k] = Lambda(lambda z: z[..., k])(x)\n",
    "        e_ = {}\n",
    "        if self.has_exog():\n",
    "            e = Input(shape=self.exo_shape, name='exos_variables')\n",
    "            for k in range(self.exo_dim):\n",
    "                e_[k] = Lambda(lambda z: z[..., k])(e)\n",
    "        else:\n",
    "            e = None\n",
    "        y_ = {}\n",
    "\n",
    "        for stack_id in range(len(self.stack_types)):\n",
    "            stack_type = self.stack_types[stack_id]\n",
    "            nb_poly = self.thetas_dim[stack_id]\n",
    "            for block_id in range(self.nb_blocks_per_stack):\n",
    "                backcast, forecast = self.create_block(x_, e_, stack_id, block_id, stack_type, nb_poly)\n",
    "                for k in range(self.input_dim):\n",
    "                    x_[k] = Subtract()([x_[k], backcast[k]])\n",
    "                    layer_name = f'stack_{stack_id}-{stack_type.title()}Block_{block_id}'\n",
    "                    if self.input_dim >= 1:\n",
    "                        layer_name += f'_Dim_{k}'\n",
    "                    # rename.\n",
    "                    forecast[k] = Lambda(function=lambda _x: _x, name=layer_name)(forecast[k])\n",
    "                    if stack_id == 0 and block_id == 0:\n",
    "                        y_[k] = forecast[k]\n",
    "                    else:\n",
    "                        y_[k] = Add()([y_[k], forecast[k]])\n",
    "\n",
    "        for k in range(self.input_dim):\n",
    "            y_[k] = Reshape(target_shape=(self.forecast_length, 1))(y_[k])\n",
    "            x_[k] = Reshape(target_shape=(self.backcast_length, 1))(x_[k])\n",
    "        if self.input_dim > 1:\n",
    "            y_ = Concatenate()([y_[ll] for ll in range(self.input_dim)])\n",
    "            x_ = Concatenate()([x_[ll] for ll in range(self.input_dim)])\n",
    "        else:\n",
    "            y_ = y_[0]\n",
    "            x_ = x_[0]\n",
    "\n",
    "        if self.input_dim != self.output_dim:\n",
    "            y_ = Dense(self.output_dim, activation='linear', name='reg_y')(y_)\n",
    "            x_ = Dense(self.output_dim, activation='linear', name='reg_x')(x_)\n",
    "\n",
    "        inputs_x = [x, e] if self.has_exog() else x\n",
    "        n_beats_forecast = Model(inputs_x, y_, name=self._FORECAST)\n",
    "        n_beats_backcast = Model(inputs_x, x_, name=self._BACKCAST)\n",
    "\n",
    "        self.models = {model.name: model for model in [n_beats_backcast, n_beats_forecast]}\n",
    "        self.cast_type = self._FORECAST\n",
    "\n",
    "    def get_generic_and_interpretable_outputs(self):\n",
    "        g_pred = sum([a['value'][0] for a in self._intermediary_outputs if 'generic' in a['layer'].lower()])\n",
    "        i_pred = sum([a['value'][0] for a in self._intermediary_outputs if 'generic' not in a['layer'].lower()])\n",
    "        outputs = {o['layer']: o['value'][0] for o in self._intermediary_outputs}\n",
    "        return g_pred, i_pred, outputs\n",
    "\n",
    "    def has_exog(self):\n",
    "        # exo/exog is short for 'exogenous variable', i.e. any input\n",
    "        # features other than the target time-series itself.\n",
    "        return self.exo_dim > 0\n",
    "\n",
    "    @staticmethod\n",
    "    def name():\n",
    "        return 'NBeatsKeras'\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath, custom_objects=None, compile=True):\n",
    "        from tensorflow.keras.models import load_model\n",
    "        return load_model(filepath, custom_objects, compile)\n",
    "\n",
    "    def _r(self, layer_with_weights, stack_id):\n",
    "        # mechanism to restore weights when block share the same weights.\n",
    "        # only useful when share_weights_in_stack=True.\n",
    "        if self.share_weights_in_stack:\n",
    "            layer_name = layer_with_weights.name.split('/')[-1]\n",
    "            try:\n",
    "                reused_weights = self.weights[stack_id][layer_name]\n",
    "                return reused_weights\n",
    "            except KeyError:\n",
    "                pass\n",
    "            if stack_id not in self.weights:\n",
    "                self.weights[stack_id] = {}\n",
    "            self.weights[stack_id][layer_name] = layer_with_weights\n",
    "        return layer_with_weights\n",
    "\n",
    "    def disable_intermediate_outputs(self):\n",
    "        self._gen_intermediate_outputs = False\n",
    "\n",
    "    def enable_intermediate_outputs(self):\n",
    "        self._gen_intermediate_outputs = True\n",
    "\n",
    "    def create_block(self, x, e, stack_id, block_id, stack_type, nb_poly):\n",
    "        # register weights (useful when share_weights_in_stack=True)\n",
    "        def reg(layer):\n",
    "            return self._r(layer, stack_id)\n",
    "\n",
    "        # update name (useful when share_weights_in_stack=True)\n",
    "        def n(layer_name):\n",
    "            return '/'.join([str(stack_id), str(block_id), stack_type, layer_name])\n",
    "\n",
    "        backcast_ = {}\n",
    "        forecast_ = {}\n",
    "        d1 = reg(Dense(self.units, activation='relu', name=n('d1')))\n",
    "        d2 = reg(Dense(self.units, activation='relu', name=n('d2')))\n",
    "        d3 = reg(Dense(self.units, activation='relu', name=n('d3')))\n",
    "        d4 = reg(Dense(self.units, activation='relu', name=n('d4')))\n",
    "        if stack_type == 'generic':\n",
    "            theta_b = reg(Dense(nb_poly, activation='linear', use_bias=False, name=n('theta_b')))\n",
    "            theta_f = reg(Dense(nb_poly, activation='linear', use_bias=False, name=n('theta_f')))\n",
    "            backcast = reg(Dense(self.backcast_length, activation='linear', name=n('backcast')))\n",
    "            forecast = reg(Dense(self.forecast_length, activation='linear', name=n('forecast')))\n",
    "        elif stack_type == 'trend':\n",
    "            theta_f = theta_b = reg(Dense(nb_poly, activation='linear', use_bias=False, name=n('theta_f_b')))\n",
    "            backcast = Lambda(trend_model, arguments={'is_forecast': False, 'backcast_length': self.backcast_length,\n",
    "                                                      'forecast_length': self.forecast_length})\n",
    "            forecast = Lambda(trend_model, arguments={'is_forecast': True, 'backcast_length': self.backcast_length,\n",
    "                                                      'forecast_length': self.forecast_length})\n",
    "        else:  # 'seasonality'\n",
    "            if self.nb_harmonics:\n",
    "                theta_b = reg(Dense(self.nb_harmonics, activation='linear', use_bias=False, name=n('theta_b')))\n",
    "            else:\n",
    "                theta_b = reg(Dense(self.forecast_length, activation='linear', use_bias=False, name=n('theta_b')))\n",
    "            theta_f = reg(Dense(self.forecast_length, activation='linear', use_bias=False, name=n('theta_f')))\n",
    "            backcast = Lambda(seasonality_model,\n",
    "                              arguments={'is_forecast': False, 'backcast_length': self.backcast_length,\n",
    "                                         'forecast_length': self.forecast_length})\n",
    "            forecast = Lambda(seasonality_model,\n",
    "                              arguments={'is_forecast': True, 'backcast_length': self.backcast_length,\n",
    "                                         'forecast_length': self.forecast_length})\n",
    "        for k in range(self.input_dim):\n",
    "            if self.has_exog():\n",
    "                d0 = Concatenate()([x[k]] + [e[ll] for ll in range(self.exo_dim)])\n",
    "            else:\n",
    "                d0 = x[k]\n",
    "            d1_ = d1(d0)\n",
    "            d2_ = d2(d1_)\n",
    "            d3_ = d3(d2_)\n",
    "            d4_ = d4(d3_)\n",
    "            theta_f_ = theta_f(d4_)\n",
    "            theta_b_ = theta_b(d4_)\n",
    "            backcast_[k] = backcast(theta_b_)\n",
    "            forecast_[k] = forecast(theta_f_)\n",
    "\n",
    "        return backcast_, forecast_\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # https://github.com/faif/python-patterns\n",
    "        # model.predict() instead of model.n_beats.predict()\n",
    "        # same for fit(), train_on_batch()...\n",
    "        attr = getattr(self.models[self._FORECAST], name)\n",
    "\n",
    "        if not callable(attr):\n",
    "            return attr\n",
    "\n",
    "        def wrapper(*args, **kwargs):\n",
    "            cast_type = self._FORECAST\n",
    "            if attr.__name__ == 'predict' and 'return_backcast' in kwargs and kwargs['return_backcast']:\n",
    "                del kwargs['return_backcast']\n",
    "                cast_type = self._BACKCAST\n",
    "\n",
    "            if attr.__name__ == 'predict' and self._gen_intermediate_outputs:\n",
    "                import keract\n",
    "                outputs = keract.get_activations(model=self, x=args)\n",
    "                self._intermediary_outputs = [\n",
    "                    {'layer': a, 'value': b} for a, b in outputs.items() if str(a).startswith('stack_')\n",
    "                ]\n",
    "            return getattr(self.models[cast_type], attr.__name__)(*args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "\n",
    "def linear_space(backcast_length, forecast_length, is_forecast=True):\n",
    "    # ls = K.arange(-float(backcast_length), float(forecast_length), 1) / forecast_length\n",
    "    # return ls[backcast_length:] if is_forecast else K.abs(K.reverse(ls[:backcast_length], axes=0))\n",
    "    horizon = forecast_length if is_forecast else backcast_length\n",
    "    return K.arange(0, horizon) / horizon\n",
    "\n",
    "\n",
    "def seasonality_model(thetas, backcast_length, forecast_length, is_forecast):\n",
    "    p = thetas.get_shape().as_list()[-1]\n",
    "    p1, p2 = (p // 2, p // 2) if p % 2 == 0 else (p // 2, p // 2 + 1)\n",
    "    t = linear_space(backcast_length, forecast_length, is_forecast=is_forecast)\n",
    "    s1 = K.stack([K.cos(2 * np.pi * i * t) for i in range(p1)])\n",
    "    s2 = K.stack([K.sin(2 * np.pi * i * t) for i in range(p2)])\n",
    "    if p == 1:\n",
    "        s = s2\n",
    "    else:\n",
    "        s = K.concatenate([s1, s2], axis=0)\n",
    "    s = K.cast(s, np.float32)\n",
    "    return K.dot(thetas, s)\n",
    "\n",
    "\n",
    "def trend_model(thetas, backcast_length, forecast_length, is_forecast):\n",
    "    p = thetas.shape[-1]\n",
    "    t = linear_space(backcast_length, forecast_length, is_forecast=is_forecast)\n",
    "    t = K.transpose(K.stack([t ** i for i in range(p)]))\n",
    "    t = K.cast(t, np.float32)\n",
    "    return K.dot(thetas, K.transpose(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a19386",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model = NBeatsNet(input_dim=num_features, backcast_length=timesteps, forecast_length=output_timesteps,\n",
    "                stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n",
    "                nb_blocks_per_stack=4, thetas_dim=(24, 24), share_weights_in_stack=True,\n",
    "                hidden_layer_units=168)\n",
    "    #print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a49b1c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8199"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b92c8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.compile(loss=MAEMS, optimizer='adam', metrics=['mse','mae', MAEMS])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "    batch_size = 100\n",
    "    epochs = 1000\n",
    "    history = LossHistory()\n",
    "    history.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7fa0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "131/131 [==============================] - 149s 861ms/step - loss: 3.8704 - mse: 0.0913 - mae: 0.2450 - MAEMS: 3.8687 - val_loss: 3.2609 - val_mse: 0.0683 - val_mae: 0.2118 - val_MAEMS: 3.2594\n",
      "Epoch 2/1000\n",
      "131/131 [==============================] - 102s 782ms/step - loss: 3.1110 - mse: 0.0747 - mae: 0.2197 - MAEMS: 3.1103 - val_loss: 3.0622 - val_mse: 0.0590 - val_mae: 0.1946 - val_MAEMS: 3.0576\n",
      "Epoch 3/1000\n",
      "131/131 [==============================] - 104s 794ms/step - loss: 2.8554 - mse: 0.0659 - mae: 0.2028 - MAEMS: 2.8547 - val_loss: 2.8212 - val_mse: 0.0648 - val_mae: 0.2003 - val_MAEMS: 2.8172\n",
      "Epoch 4/1000\n",
      "131/131 [==============================] - 103s 786ms/step - loss: 2.6328 - mse: 0.0588 - mae: 0.1883 - MAEMS: 2.6324 - val_loss: 2.6671 - val_mse: 0.0580 - val_mae: 0.1876 - val_MAEMS: 2.6627\n",
      "Epoch 5/1000\n",
      "131/131 [==============================] - 98s 752ms/step - loss: 2.5115 - mse: 0.0541 - mae: 0.1784 - MAEMS: 2.5114 - val_loss: 2.9613 - val_mse: 0.0402 - val_mae: 0.1583 - val_MAEMS: 2.9576\n",
      "Epoch 6/1000\n",
      "131/131 [==============================] - 98s 751ms/step - loss: 2.4277 - mse: 0.0508 - mae: 0.1717 - MAEMS: 2.4273 - val_loss: 2.6505 - val_mse: 0.0412 - val_mae: 0.1567 - val_MAEMS: 2.6553\n",
      "Epoch 7/1000\n",
      "131/131 [==============================] - 99s 756ms/step - loss: 2.2926 - mse: 0.0471 - mae: 0.1632 - MAEMS: 2.2921 - val_loss: 2.5038 - val_mse: 0.0396 - val_mae: 0.1522 - val_MAEMS: 2.5037\n",
      "Epoch 8/1000\n",
      "131/131 [==============================] - 98s 752ms/step - loss: 2.2248 - mse: 0.0443 - mae: 0.1571 - MAEMS: 2.2243 - val_loss: 2.2850 - val_mse: 0.0465 - val_mae: 0.1617 - val_MAEMS: 2.2789\n",
      "Epoch 9/1000\n",
      "131/131 [==============================] - 99s 757ms/step - loss: 2.0614 - mse: 0.0410 - mae: 0.1490 - MAEMS: 2.0607 - val_loss: 2.2195 - val_mse: 0.0455 - val_mae: 0.1583 - val_MAEMS: 2.2179\n",
      "Epoch 10/1000\n",
      "131/131 [==============================] - 100s 763ms/step - loss: 1.8821 - mse: 0.0368 - mae: 0.1388 - MAEMS: 1.8818 - val_loss: 2.1488 - val_mse: 0.0435 - val_mae: 0.1542 - val_MAEMS: 2.1511\n",
      "Epoch 11/1000\n",
      "131/131 [==============================] - 98s 752ms/step - loss: 1.8130 - mse: 0.0346 - mae: 0.1336 - MAEMS: 1.8125 - val_loss: 2.1437 - val_mse: 0.0426 - val_mae: 0.1522 - val_MAEMS: 2.1407\n",
      "Epoch 12/1000\n",
      "131/131 [==============================] - 98s 749ms/step - loss: 1.7530 - mse: 0.0323 - mae: 0.1283 - MAEMS: 1.7529 - val_loss: 2.0401 - val_mse: 0.0369 - val_mae: 0.1397 - val_MAEMS: 2.0368\n",
      "Epoch 13/1000\n",
      "131/131 [==============================] - 99s 756ms/step - loss: 1.8313 - mse: 0.0325 - mae: 0.1294 - MAEMS: 1.8315 - val_loss: 2.2764 - val_mse: 0.0281 - val_mae: 0.1250 - val_MAEMS: 2.2769\n",
      "Epoch 14/1000\n",
      "131/131 [==============================] - 99s 758ms/step - loss: 1.8697 - mse: 0.0320 - mae: 0.1288 - MAEMS: 1.8691 - val_loss: 2.2611 - val_mse: 0.0294 - val_mae: 0.1278 - val_MAEMS: 2.2632\n",
      "Epoch 15/1000\n",
      "131/131 [==============================] - 99s 758ms/step - loss: 1.7462 - mse: 0.0295 - mae: 0.1222 - MAEMS: 1.7462 - val_loss: 1.9001 - val_mse: 0.0359 - val_mae: 0.1381 - val_MAEMS: 1.8973\n",
      "Epoch 16/1000\n",
      "131/131 [==============================] - 99s 754ms/step - loss: 1.7005 - mse: 0.0281 - mae: 0.1186 - MAEMS: 1.6999 - val_loss: 1.9233 - val_mse: 0.0359 - val_mae: 0.1375 - val_MAEMS: 1.9164\n",
      "Epoch 17/1000\n",
      "131/131 [==============================] - 99s 757ms/step - loss: 1.6812 - mse: 0.0275 - mae: 0.1169 - MAEMS: 1.6808 - val_loss: 1.9956 - val_mse: 0.0251 - val_mae: 0.1152 - val_MAEMS: 1.9906\n",
      "Epoch 18/1000\n",
      "131/131 [==============================] - 99s 759ms/step - loss: 1.5131 - mse: 0.0250 - mae: 0.1093 - MAEMS: 1.5123 - val_loss: 1.7815 - val_mse: 0.0243 - val_mae: 0.1106 - val_MAEMS: 1.7855\n",
      "Epoch 19/1000\n",
      "131/131 [==============================] - 101s 774ms/step - loss: 1.3931 - mse: 0.0227 - mae: 0.1025 - MAEMS: 1.3926 - val_loss: 1.7078 - val_mse: 0.0230 - val_mae: 0.1067 - val_MAEMS: 1.7145\n",
      "Epoch 20/1000\n",
      "131/131 [==============================] - 99s 753ms/step - loss: 1.3315 - mse: 0.0215 - mae: 0.0987 - MAEMS: 1.3311 - val_loss: 1.6217 - val_mse: 0.0236 - val_mae: 0.1069 - val_MAEMS: 1.6266\n",
      "Epoch 21/1000\n",
      "131/131 [==============================] - 99s 754ms/step - loss: 1.3615 - mse: 0.0209 - mae: 0.0980 - MAEMS: 1.3611 - val_loss: 1.6967 - val_mse: 0.0232 - val_mae: 0.1072 - val_MAEMS: 1.7070\n",
      "Epoch 22/1000\n",
      "131/131 [==============================] - 100s 762ms/step - loss: 1.4476 - mse: 0.0211 - mae: 0.0997 - MAEMS: 1.4479 - val_loss: 1.9846 - val_mse: 0.0365 - val_mae: 0.1404 - val_MAEMS: 1.9838\n",
      "Epoch 23/1000\n",
      "131/131 [==============================] - 99s 757ms/step - loss: 1.4607 - mse: 0.0211 - mae: 0.0998 - MAEMS: 1.4602 - val_loss: 1.6948 - val_mse: 0.0293 - val_mae: 0.1212 - val_MAEMS: 1.6932\n",
      "Epoch 24/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 1.4349 - mse: 0.0204 - mae: 0.0975 - MAEMS: 1.4344 - val_loss: 1.6459 - val_mse: 0.0247 - val_mae: 0.1101 - val_MAEMS: 1.6447\n",
      "Epoch 25/1000\n",
      "131/131 [==============================] - 101s 776ms/step - loss: 1.4636 - mse: 0.0204 - mae: 0.0980 - MAEMS: 1.4632 - val_loss: 1.5973 - val_mse: 0.0244 - val_mae: 0.1095 - val_MAEMS: 1.5988\n",
      "Epoch 26/1000\n",
      "131/131 [==============================] - 102s 778ms/step - loss: 1.3805 - mse: 0.0194 - mae: 0.0948 - MAEMS: 1.3802 - val_loss: 1.5784 - val_mse: 0.0208 - val_mae: 0.1006 - val_MAEMS: 1.5799\n",
      "Epoch 27/1000\n",
      "131/131 [==============================] - 99s 756ms/step - loss: 1.2539 - mse: 0.0179 - mae: 0.0891 - MAEMS: 1.2536 - val_loss: 1.5436 - val_mse: 0.0191 - val_mae: 0.0958 - val_MAEMS: 1.5435\n",
      "Epoch 28/1000\n",
      "131/131 [==============================] - 99s 757ms/step - loss: 1.1558 - mse: 0.0165 - mae: 0.0841 - MAEMS: 1.1554 - val_loss: 1.4977 - val_mse: 0.0186 - val_mae: 0.0940 - val_MAEMS: 1.4967\n",
      "Epoch 29/1000\n",
      "131/131 [==============================] - 101s 771ms/step - loss: 1.1181 - mse: 0.0157 - mae: 0.0816 - MAEMS: 1.1178 - val_loss: 1.4830 - val_mse: 0.0180 - val_mae: 0.0921 - val_MAEMS: 1.4826\n",
      "Epoch 30/1000\n",
      "131/131 [==============================] - 102s 776ms/step - loss: 1.1047 - mse: 0.0153 - mae: 0.0804 - MAEMS: 1.1046 - val_loss: 1.5936 - val_mse: 0.0166 - val_mae: 0.0901 - val_MAEMS: 1.5946\n",
      "Epoch 31/1000\n",
      "131/131 [==============================] - 102s 776ms/step - loss: 1.1082 - mse: 0.0151 - mae: 0.0799 - MAEMS: 1.1082 - val_loss: 1.6677 - val_mse: 0.0167 - val_mae: 0.0915 - val_MAEMS: 1.6657\n",
      "Epoch 32/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 1.1053 - mse: 0.0148 - mae: 0.0791 - MAEMS: 1.1052 - val_loss: 1.8225 - val_mse: 0.0162 - val_mae: 0.0920 - val_MAEMS: 1.8202\n",
      "Epoch 33/1000\n",
      "131/131 [==============================] - 101s 775ms/step - loss: 1.1687 - mse: 0.0149 - mae: 0.0805 - MAEMS: 1.1682 - val_loss: 1.6997 - val_mse: 0.0167 - val_mae: 0.0920 - val_MAEMS: 1.7012\n",
      "Epoch 34/1000\n",
      "131/131 [==============================] - 102s 778ms/step - loss: 1.3244 - mse: 0.0162 - mae: 0.0863 - MAEMS: 1.3240 - val_loss: 1.4451 - val_mse: 0.0187 - val_mae: 0.0937 - val_MAEMS: 1.4459\n",
      "Epoch 35/1000\n",
      "131/131 [==============================] - 102s 779ms/step - loss: 1.3306 - mse: 0.0161 - mae: 0.0860 - MAEMS: 1.3305 - val_loss: 1.5910 - val_mse: 0.0249 - val_mae: 0.1106 - val_MAEMS: 1.5881\n",
      "Epoch 36/1000\n",
      "131/131 [==============================] - 102s 778ms/step - loss: 1.2441 - mse: 0.0152 - mae: 0.0825 - MAEMS: 1.2437 - val_loss: 1.4223 - val_mse: 0.0193 - val_mae: 0.0955 - val_MAEMS: 1.4192\n",
      "Epoch 37/1000\n",
      "131/131 [==============================] - 102s 779ms/step - loss: 1.0812 - mse: 0.0137 - mae: 0.0760 - MAEMS: 1.0808 - val_loss: 1.3337 - val_mse: 0.0174 - val_mae: 0.0892 - val_MAEMS: 1.3354\n",
      "Epoch 38/1000\n",
      "131/131 [==============================] - 102s 778ms/step - loss: 0.9824 - mse: 0.0126 - mae: 0.0716 - MAEMS: 0.9821 - val_loss: 1.3007 - val_mse: 0.0160 - val_mae: 0.0848 - val_MAEMS: 1.3015\n",
      "Epoch 39/1000\n",
      "131/131 [==============================] - 102s 778ms/step - loss: 0.9406 - mse: 0.0120 - mae: 0.0691 - MAEMS: 0.9403 - val_loss: 1.2676 - val_mse: 0.0161 - val_mae: 0.0849 - val_MAEMS: 1.2678\n",
      "Epoch 40/1000\n",
      "131/131 [==============================] - 102s 780ms/step - loss: 0.9285 - mse: 0.0117 - mae: 0.0681 - MAEMS: 0.9282 - val_loss: 1.2945 - val_mse: 0.0147 - val_mae: 0.0812 - val_MAEMS: 1.2945\n",
      "Epoch 41/1000\n",
      "131/131 [==============================] - 102s 778ms/step - loss: 0.9260 - mse: 0.0113 - mae: 0.0672 - MAEMS: 0.9259 - val_loss: 1.2969 - val_mse: 0.0150 - val_mae: 0.0822 - val_MAEMS: 1.2950\n",
      "Epoch 42/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 0.9560 - mse: 0.0113 - mae: 0.0677 - MAEMS: 0.9557 - val_loss: 1.2770 - val_mse: 0.0164 - val_mae: 0.0855 - val_MAEMS: 1.2741\n",
      "Epoch 43/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 1.0076 - mse: 0.0116 - mae: 0.0693 - MAEMS: 1.0074 - val_loss: 1.3049 - val_mse: 0.0158 - val_mae: 0.0844 - val_MAEMS: 1.3045\n",
      "Epoch 44/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 1.0191 - mse: 0.0120 - mae: 0.0705 - MAEMS: 1.0190 - val_loss: 1.4294 - val_mse: 0.0143 - val_mae: 0.0822 - val_MAEMS: 1.4263\n",
      "Epoch 45/1000\n",
      "131/131 [==============================] - 102s 778ms/step - loss: 0.9830 - mse: 0.0115 - mae: 0.0688 - MAEMS: 0.9827 - val_loss: 1.3317 - val_mse: 0.0137 - val_mae: 0.0792 - val_MAEMS: 1.3289\n",
      "Epoch 46/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 0.9197 - mse: 0.0107 - mae: 0.0653 - MAEMS: 0.9193 - val_loss: 1.4194 - val_mse: 0.0128 - val_mae: 0.0778 - val_MAEMS: 1.4182\n",
      "Epoch 47/1000\n",
      "131/131 [==============================] - 102s 776ms/step - loss: 0.9609 - mse: 0.0107 - mae: 0.0659 - MAEMS: 0.9609 - val_loss: 1.2697 - val_mse: 0.0142 - val_mae: 0.0797 - val_MAEMS: 1.2698\n",
      "Epoch 48/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 1.0052 - mse: 0.0109 - mae: 0.0675 - MAEMS: 1.0049 - val_loss: 1.3189 - val_mse: 0.0157 - val_mae: 0.0843 - val_MAEMS: 1.3167\n",
      "Epoch 49/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 1.0043 - mse: 0.0109 - mae: 0.0676 - MAEMS: 1.0041 - val_loss: 1.3150 - val_mse: 0.0142 - val_mae: 0.0801 - val_MAEMS: 1.3146\n",
      "Wall time: 1h 22min 59s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    b_size = 168\n",
    "    hist = model.fit(strX, strY, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(svaX, svaY), callbacks=[history, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d3ab7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4590"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c193d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = model.predict(teX, batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fbed145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npMAEMS(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true))*100\n",
    "def npMAEMD(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true-np.mean(y_true)))*100\n",
    "def npMSE(y_true, y_pred):\n",
    "    return np.mean(np.square(-y_true+y_pred))\n",
    "def npMAE(y_true, y_pred):\n",
    "    return np.mean(abs(-y_true+y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "682abbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.04717002175844705  MAE ==  0.1746890849998046  MAEMS ==  2.8815779851166354\n"
     ]
    }
   ],
   "source": [
    "tePredict = testPredict.reshape(-1)\n",
    "testY = teY.reshape(-1)\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict), ' MAE == ', npMAE(testY, tePredict), ' MAEMS == ', npMAEMS(testY, tePredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e143e",
   "metadata": {},
   "source": [
    "## Without Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5ec666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model2 = NBeatsNet(input_dim=num_features, backcast_length=timesteps, forecast_length=output_timesteps,\n",
    "                stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n",
    "                nb_blocks_per_stack=4, thetas_dim=(24, 24), share_weights_in_stack=True,\n",
    "                hidden_layer_units=168)\n",
    "    #print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16aa0b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12188"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f68b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model2.compile(loss=MAEMS, optimizer='adam', metrics=['mse','mae', MAEMS])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "    batch_size = 100\n",
    "    epochs = 1000\n",
    "    history2 = LossHistory()\n",
    "    history2.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2200f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "131/131 [==============================] - 146s 837ms/step - loss: 5.9845 - mse: 0.1767 - mae: 0.3058 - MAEMS: 5.9776 - val_loss: 3.4359 - val_mse: 0.1112 - val_mae: 0.2774 - val_MAEMS: 3.4359\n",
      "Epoch 2/1000\n",
      "131/131 [==============================] - 95s 723ms/step - loss: 3.7634 - mse: 0.0932 - mae: 0.2492 - MAEMS: 3.7609 - val_loss: 3.2983 - val_mse: 0.1075 - val_mae: 0.2736 - val_MAEMS: 3.3036\n",
      "Epoch 3/1000\n",
      "131/131 [==============================] - 102s 779ms/step - loss: 3.5737 - mse: 0.0914 - mae: 0.2454 - MAEMS: 3.5716 - val_loss: 3.2439 - val_mse: 0.1029 - val_mae: 0.2676 - val_MAEMS: 3.2478\n",
      "Epoch 4/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 3.4711 - mse: 0.0893 - mae: 0.2415 - MAEMS: 3.4692 - val_loss: 3.2510 - val_mse: 0.1035 - val_mae: 0.2684 - val_MAEMS: 3.2532\n",
      "Epoch 5/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 3.3659 - mse: 0.0870 - mae: 0.2368 - MAEMS: 3.3639 - val_loss: 3.2236 - val_mse: 0.0954 - val_mae: 0.2575 - val_MAEMS: 3.2197\n",
      "Epoch 6/1000\n",
      "131/131 [==============================] - 101s 775ms/step - loss: 3.2939 - mse: 0.0838 - mae: 0.2315 - MAEMS: 3.2921 - val_loss: 3.2124 - val_mse: 0.0955 - val_mae: 0.2573 - val_MAEMS: 3.2126\n",
      "Epoch 7/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 3.1982 - mse: 0.0808 - mae: 0.2257 - MAEMS: 3.1966 - val_loss: 3.2008 - val_mse: 0.0947 - val_mae: 0.2560 - val_MAEMS: 3.1981\n",
      "Epoch 8/1000\n",
      "131/131 [==============================] - 102s 778ms/step - loss: 3.1583 - mse: 0.0792 - mae: 0.2229 - MAEMS: 3.1563 - val_loss: 3.1976 - val_mse: 0.0848 - val_mae: 0.2420 - val_MAEMS: 3.1990\n",
      "Epoch 9/1000\n",
      "131/131 [==============================] - 102s 779ms/step - loss: 3.1167 - mse: 0.0775 - mae: 0.2199 - MAEMS: 3.1145 - val_loss: 3.1947 - val_mse: 0.0803 - val_mae: 0.2359 - val_MAEMS: 3.1837\n",
      "Epoch 10/1000\n",
      "131/131 [==============================] - 102s 780ms/step - loss: 3.1989 - mse: 0.0783 - mae: 0.2217 - MAEMS: 3.1971 - val_loss: 3.3623 - val_mse: 0.1048 - val_mae: 0.2704 - val_MAEMS: 3.3750\n",
      "Epoch 11/1000\n",
      "131/131 [==============================] - 102s 777ms/step - loss: 3.0705 - mse: 0.0755 - mae: 0.2165 - MAEMS: 3.0690 - val_loss: 3.4073 - val_mse: 0.1038 - val_mae: 0.2681 - val_MAEMS: 3.4172\n",
      "Epoch 12/1000\n",
      "131/131 [==============================] - 101s 775ms/step - loss: 2.9942 - mse: 0.0732 - mae: 0.2116 - MAEMS: 2.9916 - val_loss: 3.2535 - val_mse: 0.0723 - val_mae: 0.2225 - val_MAEMS: 3.2440\n",
      "Epoch 13/1000\n",
      "131/131 [==============================] - 102s 776ms/step - loss: 2.9834 - mse: 0.0723 - mae: 0.2103 - MAEMS: 2.9806 - val_loss: 3.3804 - val_mse: 0.0647 - val_mae: 0.2114 - val_MAEMS: 3.3639\n",
      "Epoch 14/1000\n",
      "131/131 [==============================] - 102s 776ms/step - loss: 2.9940 - mse: 0.0727 - mae: 0.2104 - MAEMS: 2.9913 - val_loss: 3.3243 - val_mse: 0.0691 - val_mae: 0.2179 - val_MAEMS: 3.3087\n",
      "Epoch 15/1000\n",
      "131/131 [==============================] - 101s 775ms/step - loss: 3.0284 - mse: 0.0725 - mae: 0.2108 - MAEMS: 3.0275 - val_loss: 3.3685 - val_mse: 0.0991 - val_mae: 0.2617 - val_MAEMS: 3.3865\n",
      "Epoch 16/1000\n",
      "131/131 [==============================] - 102s 776ms/step - loss: 2.9759 - mse: 0.0717 - mae: 0.2081 - MAEMS: 2.9745 - val_loss: 3.4858 - val_mse: 0.1071 - val_mae: 0.2731 - val_MAEMS: 3.4982\n",
      "Epoch 17/1000\n",
      "131/131 [==============================] - 102s 779ms/step - loss: 2.7957 - mse: 0.0668 - mae: 0.1988 - MAEMS: 2.7936 - val_loss: 3.1051 - val_mse: 0.0746 - val_mae: 0.2244 - val_MAEMS: 3.1184\n",
      "Epoch 18/1000\n",
      "131/131 [==============================] - 81s 615ms/step - loss: 2.8092 - mse: 0.0672 - mae: 0.1992 - MAEMS: 2.8085 - val_loss: 3.3766 - val_mse: 0.1020 - val_mae: 0.2664 - val_MAEMS: 3.3979\n",
      "Epoch 19/1000\n",
      "131/131 [==============================] - 65s 499ms/step - loss: 2.7147 - mse: 0.0651 - mae: 0.1954 - MAEMS: 2.7144 - val_loss: 4.1873 - val_mse: 0.1340 - val_mae: 0.3091 - val_MAEMS: 4.2177\n",
      "Epoch 20/1000\n",
      "131/131 [==============================] - 66s 502ms/step - loss: 2.7445 - mse: 0.0642 - mae: 0.1939 - MAEMS: 2.7434 - val_loss: 3.8539 - val_mse: 0.1194 - val_mae: 0.2898 - val_MAEMS: 3.8988\n",
      "Epoch 21/1000\n",
      "131/131 [==============================] - 65s 499ms/step - loss: 2.6908 - mse: 0.0615 - mae: 0.1893 - MAEMS: 2.6920 - val_loss: 5.3832 - val_mse: 0.1806 - val_mae: 0.3744 - val_MAEMS: 5.4210\n",
      "Epoch 22/1000\n",
      "131/131 [==============================] - 66s 501ms/step - loss: 2.6990 - mse: 0.0618 - mae: 0.1889 - MAEMS: 2.6974 - val_loss: 3.9306 - val_mse: 0.1247 - val_mae: 0.2981 - val_MAEMS: 3.9554\n",
      "Epoch 23/1000\n",
      "131/131 [==============================] - 66s 501ms/step - loss: 2.6650 - mse: 0.0605 - mae: 0.1863 - MAEMS: 2.6630 - val_loss: 3.6451 - val_mse: 0.1110 - val_mae: 0.2766 - val_MAEMS: 3.6629\n",
      "Epoch 24/1000\n",
      "131/131 [==============================] - 66s 502ms/step - loss: 2.7011 - mse: 0.0617 - mae: 0.1888 - MAEMS: 2.6996 - val_loss: 3.9324 - val_mse: 0.1135 - val_mae: 0.2813 - val_MAEMS: 3.9605\n",
      "Epoch 25/1000\n",
      "131/131 [==============================] - 66s 501ms/step - loss: 2.7123 - mse: 0.0629 - mae: 0.1911 - MAEMS: 2.7103 - val_loss: 3.8848 - val_mse: 0.1113 - val_mae: 0.2778 - val_MAEMS: 3.9012\n",
      "Epoch 26/1000\n",
      "131/131 [==============================] - 66s 500ms/step - loss: 2.6193 - mse: 0.0588 - mae: 0.1833 - MAEMS: 2.6192 - val_loss: 5.6774 - val_mse: 0.1792 - val_mae: 0.3692 - val_MAEMS: 5.6849\n",
      "Epoch 27/1000\n",
      "131/131 [==============================] - 65s 500ms/step - loss: 2.6250 - mse: 0.0599 - mae: 0.1839 - MAEMS: 2.6241 - val_loss: 4.8801 - val_mse: 0.1502 - val_mae: 0.3332 - val_MAEMS: 4.8932\n",
      "Wall time: 40min 36s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    b_size = 168\n",
    "    hist2 = model2.fit(trX, trY, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(vaX, vaY), callbacks=[history2, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e94cd2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4594"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f86a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict2 = model2.predict(teX, batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f23e8bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.04717002175844705  MAE ==  0.1746890849998046  MAEMS ==  2.8815779851166354\n",
      "Error Test Score > MSE ==  0.1633834042512429  MAE ==  0.35183713934951394  MAEMS ==  4.170949514910341\n"
     ]
    }
   ],
   "source": [
    "tePredict2 = testPredict2.reshape(-1)\n",
    "testY = teY.reshape(-1)\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict), ' MAE == ', npMAE(testY, tePredict), ' MAEMS == ', npMAEMS(testY, tePredict))\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict2), ' MAE == ', npMAE(testY, tePredict2), ' MAEMS == ', npMAEMS(testY, tePredict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2dff2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = testPredict.reshape(-1,24)\n",
    "testPredict2 = testPredict2.reshape(-1,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77ccc284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy.stats import norm\n",
    "\n",
    "def diebold_mariano_test(forecast1, forecast2, actual, opt): \n",
    "    \n",
    "    if opt==0: # MSE\n",
    "        e1 = actual-forecast1\n",
    "        e2 = actual-forecast2\n",
    "        d = e1**2 - e2**2\n",
    "    elif opt==1: # MAE\n",
    "        e1 = abs(actual-forecast1)\n",
    "        e2 = abs(actual-forecast2)\n",
    "        d = e1 - e2\n",
    "    else:\n",
    "        e1 = np.multiply(abs(actual - forecast1), actual**2)\n",
    "        e2 = np.multiply(abs(actual - forecast2), actual**2)\n",
    "        d = e1-e2\n",
    "    \n",
    "    # Mean of the loss differentials\n",
    "    mean_d = np.mean(d)\n",
    "    \n",
    "    # Standard deviation of the loss differentials\n",
    "    std_d = np.std(d, ddof=1)\n",
    "    \n",
    "    # Calculate the test statistic\n",
    "    test_stat = (mean_d / std_d) * np.sqrt(len(d))\n",
    "    \n",
    "    # Calculate the p-value using a two-tailed test\n",
    "    p_value = 2 * (1 - norm.cdf(abs(test_stat)))\n",
    "    \n",
    "    return test_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b4beec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-50.254885390326294, 0.0),\n",
       " (-55.51667870536376, 0.0),\n",
       " (-17.118222487058638, 0.0))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diebold_mariano_test(testPredict, testPredict2, teY, 0), diebold_mariano_test(testPredict, testPredict2, teY, 1), diebold_mariano_test(testPredict, testPredict2, teY, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
