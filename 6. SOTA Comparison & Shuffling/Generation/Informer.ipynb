{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90239ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import time\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26cc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import recall_score, precision_score # New\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.preprocessing\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Add, Multiply, Subtract\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "# regularizers\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dropout, SpatialDropout1D\n",
    "from scipy import signal\n",
    "import openpyxl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import svd\n",
    "import gc\n",
    "from keijzer import *\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.layers import Dense, Input, Flatten, Add, concatenate, Dropout, Activation, Multiply, Embedding, Layer, Reshape\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D  \n",
    "from keras.layers import Conv1D, AveragePooling1D, MaxPooling1D\n",
    "#from keras.ops import convert_to_tensor, convert_to_numpy\n",
    "#from keras.utils import plot_model\n",
    "from keras import activations\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "class LossHistory(tensorflow.keras.callbacks.Callback):  # history = LossHistory()\n",
    "    def init(self):  # history.init()\n",
    "        self.losses = []\n",
    "        # self.accs = []\n",
    "        self.val_losses = []\n",
    "        # self.val_accs = []\n",
    "        self.rmses = []\n",
    "        self.mses = []\n",
    "        self.maes = []\n",
    "        self.mapes = []\n",
    "        self.val_rmses = []\n",
    "        self.val_mses = []\n",
    "        self.val_maes = []\n",
    "        self.val_mapes = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        # self.accs.append(logs.get('acc'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        # self.val_accs.append(logs.get('val_accuracy\n",
    "        self.rmses.append(logs.get('root_mean_squared_error'))\n",
    "        self.mses.append(logs.get('mean_squared_error'))\n",
    "        self.maes.append(logs.get('mean_absolute_error'))\n",
    "        self.mapes.append(logs.get('mean_absolute_percentage_error'))\n",
    "        self.val_rmses.append(logs.get('val_root_mean_squared_error'))\n",
    "        self.val_mses.append(logs.get('val_mean_squared_error'))\n",
    "        self.val_maes.append(logs.get('val_mean_absolute_error'))\n",
    "        self.val_mapes.append(logs.get('val_mean_absolute_percentage_error'))\n",
    "\n",
    "\n",
    "\n",
    "def root_squared_mean_error(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true-K.mean(y_true)))*100\n",
    "\n",
    "def mse_mae(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))+K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e78cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import scipy as sc\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # 여러개 사용시 '0,1,2' 식으로 하나의 문자열에 입력\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') # 호스트 러나임에 표시되는 GPU 장치 목록 반환\n",
    "\n",
    "if gpus: # 반환된 GPU 장치 목록이 있다면\n",
    "    try: # 해당 장치에 대한 메모리 증가 활성화 여부 설정\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e: # try문 실패시에 에러문구 출력\n",
    "        print(e)\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abbc9da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Wind Change</th>\n",
       "      <th>Wind Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.620197</td>\n",
       "      <td>0.644724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.544580</td>\n",
       "      <td>0.658617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.570751</td>\n",
       "      <td>0.683924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.599594</td>\n",
       "      <td>0.721813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.495246</td>\n",
       "      <td>0.714187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35058</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.774643</td>\n",
       "      <td>0.484984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.735512</td>\n",
       "      <td>0.582157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.601593</td>\n",
       "      <td>0.620917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.536583</td>\n",
       "      <td>0.631322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.454969</td>\n",
       "      <td>0.606127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35063 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  Wind Change  Wind Output\n",
       "0      2020     0.620197     0.644724\n",
       "1      2020     0.544580     0.658617\n",
       "2      2020     0.570751     0.683924\n",
       "3      2020     0.599594     0.721813\n",
       "4      2020     0.495246     0.714187\n",
       "...     ...          ...          ...\n",
       "35058  2023     0.774643     0.484984\n",
       "35059  2023     0.735512     0.582157\n",
       "35060  2023     0.601593     0.620917\n",
       "35061  2023     0.536583     0.631322\n",
       "35062  2023     0.454969     0.606127\n",
       "\n",
       "[35063 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2020.csv'\n",
    "data_source1 = pd.read_csv(data_path)\n",
    "dat_source1 = pd.DataFrame(data_source1)\n",
    "#dat_source1 = dat_source1.iloc[:-3,:]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2021.csv'\n",
    "data_source2 = pd.read_csv(data_path)\n",
    "dat_source2 = pd.DataFrame(data_source2)\n",
    "dat_source2 = dat_source2.iloc[:8761,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2022.csv'\n",
    "data_source3 = pd.read_csv(data_path)\n",
    "dat_source3 = pd.DataFrame(data_source3)\n",
    "dat_source3 = dat_source3.iloc[:8760,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2023.csv'\n",
    "data_source4 = pd.read_csv(data_path)\n",
    "dat_source4 = pd.DataFrame(data_source4)\n",
    "dat_source4 = dat_source4.iloc[:8760,:9]\n",
    "\n",
    "dat_arr1 = np.array(dat_source1)\n",
    "dat_arr2 = np.array(dat_source2)\n",
    "dat_arr3 = np.array(dat_source3)\n",
    "dat_arr4 = np.array(dat_source4)\n",
    "\n",
    "dat_arr = np.concatenate([dat_arr1, dat_arr2, dat_arr3, dat_arr4],axis=0)\n",
    "dat_arr.shape # 8784, 8761, 8760, 8760\n",
    "\n",
    "dat_source = pd.DataFrame(dat_arr, columns=['Time-Date stamp','Date','ERCOT Load','Total Wind Output','Total Wind Installed','Wind Output, % of Load','Wind Output, % of Installed','1-hr MW change','1-hr % change'])\n",
    "dat = pd.DataFrame()\n",
    "dat['MW % change'] = np.array(dat_source.iloc[1:,3], dtype=float)-np.array(dat_source.iloc[0:-1,3], dtype=float)\n",
    "dat['MW % change'] = np.divide(np.array(dat['MW % change']),np.array(dat_source.iloc[1:,4]))*100\n",
    "dat['Total Wind Output'] = np.divide(np.array(dat_source.iloc[1:,3]), np.array(dat_source.iloc[1:,4]))\n",
    "\n",
    "ramp_ratio = 20\n",
    "load_ratio = 10\n",
    "year_arr = np.concatenate([np.ones(8784)*2020, np.ones(8760)*2021, np.ones(8760)*2022, np.ones(8760)*2023])\n",
    "year_df = pd.DataFrame(year_arr, columns=['Year'], dtype=int)\n",
    "wind_df = pd.concat([year_df, dat], axis=1)\n",
    "wind_df = wind_df.reset_index(drop=True)\n",
    "\n",
    "std_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_load = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_ramp = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "wind_df[['Total Wind Output']] = std_scaler.fit_transform(wind_df[['Total Wind Output']])\n",
    "wind_df[['MW % change']] = std_scaler_ramp.fit_transform(wind_df[['MW % change']])\n",
    "wind_df = wind_df.iloc[:-1,:]\n",
    "ramp_df = pd.DataFrame(wind_df)\n",
    "ramp_df.columns=['Year', 'Wind Change', 'Wind Output']\n",
    "ramp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15492174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, timesteps, output_timesteps, leadtime):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - timesteps -output_timesteps - leadtime - 1):\n",
    "        a = dataset[i:(i + timesteps), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[(i + timesteps + leadtime):(i+timesteps+output_timesteps+leadtime), :])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def svdadd(X, start, end, b_size):\n",
    "    dec_x = X[start:end, :, :]\n",
    "    U, S, Vh = svd(dec_x.reshape(b_size, -1), full_matrices=True)\n",
    "\n",
    "    high_sig = np.matmul(np.matmul(U[:,:high_ind], np.diag(S[0:high_ind])), Vh[:high_ind,:])\n",
    "    low_sig = X[:dec_num, :, :].reshape(b_size,-1)-high_sig\n",
    "\n",
    "    rec_x = np.zeros((b_size, b_size, high_ind))\n",
    "    \n",
    "    for i in range(high_ind):\n",
    "        rec_x[:,:,i] = np.matmul((U[:,i]*S[i]).reshape(-1,1), Vh[i,:].reshape(1,-1))    \n",
    "    return rec_x, low_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e480d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = np.array(wind_df.iloc[:,2]).reshape(-1,1)\n",
    "timesteps = 24*7\n",
    "output_timesteps = 24\n",
    "leadtime = 6\n",
    "num_features = 1\n",
    "X, Y = create_dataset(norm_df, timesteps, output_timesteps, leadtime)\n",
    "b_size = timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5886f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_num = timesteps\n",
    "high_ind = 35\n",
    "\n",
    "addX = np.zeros((X.shape[0], dec_num, high_ind+1))\n",
    "\n",
    "for k in range(X.shape[0]//dec_num):\n",
    "    rec_x, low_sig = svdadd(X, k*dec_num, (k+1)*dec_num, b_size)\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, :-1] = rec_x\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, -1] = low_sig\n",
    "    \n",
    "rec_x, low_sig = svdadd(X, X.shape[0]-dec_num, X.shape[0], b_size)\n",
    "addX[X.shape[0]-dec_num:X.shape[0], :, :-1] = rec_x\n",
    "addX[X.shape[0]-dec_num:X.shape[0],:, -1] = low_sig\n",
    "\n",
    "addX_det = addX[:, :, :-1]\n",
    "addX_det = np.sum(addX_det, axis=2)\n",
    "X = np.concatenate([X, addX], axis=2)\n",
    "num_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc5b0078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31377, 168, 37), (31377, 24), (3487, 168, 37), (3487, 24))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trvaX, teX, trvaY, teY = train_test_split(X, Y, test_size=0.1, shuffle=False)\n",
    "\n",
    "idx = np.arange(trvaX.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "strvaX = trvaX[idx]\n",
    "strvaY = trvaY[idx]\n",
    "strvaY = strvaY.reshape(-1, output_timesteps)\n",
    "\n",
    "trvaY = trvaY.reshape(-1, output_timesteps)\n",
    "teY = teY.reshape(-1, output_timesteps)\n",
    "\n",
    "strX, svaX, strY, svaY = train_test_split(strvaX, strvaY, test_size=0.3, shuffle=False)\n",
    "trX, vaX, trY, vaY = train_test_split(trvaX, trvaY, test_size=0.3, shuffle=False)\n",
    "\n",
    "trvaX.shape, trvaY.shape, teX.shape, teY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa32058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAEMS(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true))*100\n",
    "\n",
    "def MAE(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true)))\n",
    "\n",
    "def MSE(y_true, y_pred):\n",
    "    return K.mean((K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc4414",
   "metadata": {},
   "source": [
    "## Informer Model - With SVD With Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6780a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.depth = d_model // n_heads\n",
    "\n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        q = self.wq(inputs)\n",
    "        k = self.wk(inputs)\n",
    "        v = self.wv(inputs)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention_logits = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_attention_logits /= tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reduce_mean(output, axis=2)\n",
    "        #print(output.shape)\n",
    "        output = tf.reshape(output, (batch_size, -1, int(self.d_model / n_heads)))\n",
    "        #print(output.shape)\n",
    "        return output\n",
    "\n",
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = SelfAttention(d_model, n_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model // n_heads) \n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.attention(inputs)\n",
    "        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        #print(ffn_output.shape)\n",
    "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n",
    "\n",
    "class Informer(Model):\n",
    "    def __init__(self, input_shape, num_layers, d_model, n_heads, d_ff, output_length):\n",
    "        super(Informer, self).__init__()\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)]\n",
    "        self.flatten = layers.Flatten()  # Flatten before the final Dense layer\n",
    "        self.final_layer = layers.Dense(output_length)  # Ensure output_length matches 24\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.flatten(x)  # Flatten the output\n",
    "        return self.final_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba3541f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"informer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_layer (EncoderLayer) multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_1 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_2 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_3 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_4 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_5 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_6 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_7 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_8 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_9 (EncoderLaye multiple                  17657     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             multiple                  149208    \n",
      "=================================================================\n",
      "Total params: 325,778\n",
      "Trainable params: 325,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (168, num_features)  # None for num_feature\n",
    "num_layers = 10\n",
    "d_model = 37*4\n",
    "n_heads = 4\n",
    "d_ff = n_heads*2\n",
    "output_shape = 24\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model = Informer(input_shape=input_shape, num_layers=num_layers, d_model=d_model, \n",
    "                      n_heads=n_heads, d_ff=d_ff, output_length=output_shape)\n",
    "\n",
    "    batch_size = 168\n",
    "    example_input = tf.random.normal((batch_size, *input_shape))\n",
    "\n",
    "    output = model(example_input)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "838c3478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0be46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.compile(loss=MAEMS, optimizer='adam', metrics=['mse','mae', MAEMS])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    batch_size = 100\n",
    "    epochs = 1000\n",
    "    history = LossHistory()\n",
    "    history.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77f239aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "131/131 [==============================] - 30s 162ms/step - loss: 10.4713 - mse: 0.4411 - mae: 0.4572 - MAEMS: 10.4635 - val_loss: 5.1712 - val_mse: 0.1071 - val_mae: 0.2642 - val_MAEMS: 5.1980\n",
      "Epoch 2/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 5.7412 - mse: 0.1368 - mae: 0.2994 - MAEMS: 5.7394 - val_loss: 3.9825 - val_mse: 0.1093 - val_mae: 0.2717 - val_MAEMS: 3.9909\n",
      "Epoch 3/1000\n",
      "131/131 [==============================] - 20s 149ms/step - loss: 4.7068 - mse: 0.1093 - mae: 0.2694 - MAEMS: 4.7057 - val_loss: 3.8794 - val_mse: 0.0936 - val_mae: 0.2500 - val_MAEMS: 3.8915\n",
      "Epoch 4/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 4.1330 - mse: 0.0937 - mae: 0.2495 - MAEMS: 4.1322 - val_loss: 3.6330 - val_mse: 0.0913 - val_mae: 0.2475 - val_MAEMS: 3.6414\n",
      "Epoch 5/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 3.7568 - mse: 0.0810 - mae: 0.2311 - MAEMS: 3.7563 - val_loss: 3.3129 - val_mse: 0.0770 - val_mae: 0.2240 - val_MAEMS: 3.3133\n",
      "Epoch 6/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 3.5518 - mse: 0.0735 - mae: 0.2187 - MAEMS: 3.5513 - val_loss: 3.2975 - val_mse: 0.0759 - val_mae: 0.2215 - val_MAEMS: 3.2984\n",
      "Epoch 7/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 3.4209 - mse: 0.0689 - mae: 0.2113 - MAEMS: 3.4205 - val_loss: 3.1035 - val_mse: 0.0630 - val_mae: 0.1998 - val_MAEMS: 3.1037\n",
      "Epoch 8/1000\n",
      "131/131 [==============================] - 20s 149ms/step - loss: 3.2990 - mse: 0.0659 - mae: 0.2056 - MAEMS: 3.2984 - val_loss: 3.0156 - val_mse: 0.0561 - val_mae: 0.1873 - val_MAEMS: 3.0172\n",
      "Epoch 9/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 3.2181 - mse: 0.0633 - mae: 0.2008 - MAEMS: 3.2178 - val_loss: 3.1243 - val_mse: 0.0620 - val_mae: 0.1973 - val_MAEMS: 3.1321\n",
      "Epoch 10/1000\n",
      "131/131 [==============================] - 19s 149ms/step - loss: 3.1260 - mse: 0.0614 - mae: 0.1970 - MAEMS: 3.1258 - val_loss: 2.9406 - val_mse: 0.0547 - val_mae: 0.1834 - val_MAEMS: 2.9491\n",
      "Epoch 11/1000\n",
      "131/131 [==============================] - 20s 149ms/step - loss: 3.0628 - mse: 0.0596 - mae: 0.1933 - MAEMS: 3.0629 - val_loss: 3.0667 - val_mse: 0.0463 - val_mae: 0.1691 - val_MAEMS: 3.0750\n",
      "Epoch 12/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 2.9796 - mse: 0.0575 - mae: 0.1891 - MAEMS: 2.9793 - val_loss: 2.8406 - val_mse: 0.0499 - val_mae: 0.1738 - val_MAEMS: 2.8481\n",
      "Epoch 13/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 2.8873 - mse: 0.0555 - mae: 0.1847 - MAEMS: 2.8874 - val_loss: 2.9559 - val_mse: 0.0429 - val_mae: 0.1617 - val_MAEMS: 2.9660\n",
      "Epoch 14/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 2.8404 - mse: 0.0541 - mae: 0.1818 - MAEMS: 2.8401 - val_loss: 2.8555 - val_mse: 0.0436 - val_mae: 0.1619 - val_MAEMS: 2.8604\n",
      "Epoch 15/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 2.7703 - mse: 0.0524 - mae: 0.1780 - MAEMS: 2.7700 - val_loss: 2.7901 - val_mse: 0.0410 - val_mae: 0.1567 - val_MAEMS: 2.7988\n",
      "Epoch 16/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 2.6969 - mse: 0.0508 - mae: 0.1743 - MAEMS: 2.6967 - val_loss: 2.7713 - val_mse: 0.0394 - val_mae: 0.1532 - val_MAEMS: 2.7809\n",
      "Epoch 17/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 2.6378 - mse: 0.0490 - mae: 0.1703 - MAEMS: 2.6374 - val_loss: 2.6546 - val_mse: 0.0396 - val_mae: 0.1520 - val_MAEMS: 2.6635\n",
      "Epoch 18/1000\n",
      "131/131 [==============================] - 20s 151ms/step - loss: 2.5572 - mse: 0.0473 - mae: 0.1666 - MAEMS: 2.5572 - val_loss: 2.4710 - val_mse: 0.0423 - val_mae: 0.1557 - val_MAEMS: 2.4799\n",
      "Epoch 19/1000\n",
      "131/131 [==============================] - 20s 154ms/step - loss: 2.4776 - mse: 0.0454 - mae: 0.1625 - MAEMS: 2.4772 - val_loss: 2.4306 - val_mse: 0.0402 - val_mae: 0.1507 - val_MAEMS: 2.4392\n",
      "Epoch 20/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 2.4038 - mse: 0.0439 - mae: 0.1588 - MAEMS: 2.4036 - val_loss: 2.3349 - val_mse: 0.0391 - val_mae: 0.1480 - val_MAEMS: 2.3435\n",
      "Epoch 21/1000\n",
      "131/131 [==============================] - 26s 203ms/step - loss: 2.3426 - mse: 0.0423 - mae: 0.1552 - MAEMS: 2.3424 - val_loss: 2.3222 - val_mse: 0.0380 - val_mae: 0.1461 - val_MAEMS: 2.3271\n",
      "Epoch 22/1000\n",
      "131/131 [==============================] - 43s 328ms/step - loss: 2.3141 - mse: 0.0413 - mae: 0.1532 - MAEMS: 2.3139 - val_loss: 2.3969 - val_mse: 0.0335 - val_mae: 0.1377 - val_MAEMS: 2.4029\n",
      "Epoch 23/1000\n",
      "131/131 [==============================] - 37s 279ms/step - loss: 2.2764 - mse: 0.0400 - mae: 0.1502 - MAEMS: 2.2761 - val_loss: 2.2544 - val_mse: 0.0440 - val_mae: 0.1570 - val_MAEMS: 2.2626\n",
      "Epoch 24/1000\n",
      "131/131 [==============================] - 37s 280ms/step - loss: 2.1853 - mse: 0.0384 - mae: 0.1464 - MAEMS: 2.1852 - val_loss: 2.1836 - val_mse: 0.0393 - val_mae: 0.1465 - val_MAEMS: 2.1938\n",
      "Epoch 25/1000\n",
      "131/131 [==============================] - 43s 327ms/step - loss: 2.1016 - mse: 0.0367 - mae: 0.1421 - MAEMS: 2.1016 - val_loss: 2.1107 - val_mse: 0.0377 - val_mae: 0.1426 - val_MAEMS: 2.1199\n",
      "Epoch 26/1000\n",
      "131/131 [==============================] - 43s 325ms/step - loss: 2.0703 - mse: 0.0358 - mae: 0.1398 - MAEMS: 2.0701 - val_loss: 2.0917 - val_mse: 0.0360 - val_mae: 0.1398 - val_MAEMS: 2.0960\n",
      "Epoch 27/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 2.0313 - mse: 0.0346 - mae: 0.1371 - MAEMS: 2.0310 - val_loss: 2.0411 - val_mse: 0.0331 - val_mae: 0.1326 - val_MAEMS: 2.0462\n",
      "Epoch 28/1000\n",
      "131/131 [==============================] - 43s 330ms/step - loss: 2.0696 - mse: 0.0346 - mae: 0.1376 - MAEMS: 2.0693 - val_loss: 2.0144 - val_mse: 0.0336 - val_mae: 0.1336 - val_MAEMS: 2.0235\n",
      "Epoch 29/1000\n",
      "131/131 [==============================] - 43s 331ms/step - loss: 1.9546 - mse: 0.0327 - mae: 0.1324 - MAEMS: 1.9544 - val_loss: 1.9641 - val_mse: 0.0307 - val_mae: 0.1269 - val_MAEMS: 1.9712\n",
      "Epoch 30/1000\n",
      "131/131 [==============================] - 42s 325ms/step - loss: 1.9029 - mse: 0.0316 - mae: 0.1297 - MAEMS: 1.9028 - val_loss: 1.9529 - val_mse: 0.0298 - val_mae: 0.1249 - val_MAEMS: 1.9627\n",
      "Epoch 31/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 1.8703 - mse: 0.0307 - mae: 0.1275 - MAEMS: 1.8702 - val_loss: 1.9215 - val_mse: 0.0311 - val_mae: 0.1275 - val_MAEMS: 1.9278\n",
      "Epoch 32/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.8305 - mse: 0.0299 - mae: 0.1254 - MAEMS: 1.8304 - val_loss: 1.8390 - val_mse: 0.0287 - val_mae: 0.1221 - val_MAEMS: 1.8456\n",
      "Epoch 33/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.8202 - mse: 0.0292 - mae: 0.1237 - MAEMS: 1.8203 - val_loss: 1.8890 - val_mse: 0.0265 - val_mae: 0.1181 - val_MAEMS: 1.8951\n",
      "Epoch 34/1000\n",
      "131/131 [==============================] - 42s 317ms/step - loss: 1.8511 - mse: 0.0292 - mae: 0.1243 - MAEMS: 1.8510 - val_loss: 1.8166 - val_mse: 0.0293 - val_mae: 0.1230 - val_MAEMS: 1.8176\n",
      "Epoch 35/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.8062 - mse: 0.0284 - mae: 0.1221 - MAEMS: 1.8058 - val_loss: 1.8489 - val_mse: 0.0294 - val_mae: 0.1237 - val_MAEMS: 1.8562\n",
      "Epoch 36/1000\n",
      "131/131 [==============================] - 41s 317ms/step - loss: 1.7169 - mse: 0.0272 - mae: 0.1183 - MAEMS: 1.7168 - val_loss: 1.7780 - val_mse: 0.0276 - val_mae: 0.1192 - val_MAEMS: 1.7820\n",
      "Epoch 37/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.6899 - mse: 0.0266 - mae: 0.1167 - MAEMS: 1.6897 - val_loss: 1.7549 - val_mse: 0.0290 - val_mae: 0.1223 - val_MAEMS: 1.7627\n",
      "Epoch 38/1000\n",
      "131/131 [==============================] - 42s 317ms/step - loss: 1.6575 - mse: 0.0258 - mae: 0.1146 - MAEMS: 1.6573 - val_loss: 1.7316 - val_mse: 0.0263 - val_mae: 0.1160 - val_MAEMS: 1.7357\n",
      "Epoch 39/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.6412 - mse: 0.0253 - mae: 0.1133 - MAEMS: 1.6411 - val_loss: 1.7152 - val_mse: 0.0263 - val_mae: 0.1159 - val_MAEMS: 1.7206\n",
      "Epoch 40/1000\n",
      "131/131 [==============================] - 41s 317ms/step - loss: 1.6193 - mse: 0.0248 - mae: 0.1119 - MAEMS: 1.6193 - val_loss: 1.6977 - val_mse: 0.0250 - val_mae: 0.1119 - val_MAEMS: 1.7040\n",
      "Epoch 41/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.6117 - mse: 0.0244 - mae: 0.1111 - MAEMS: 1.6118 - val_loss: 1.6533 - val_mse: 0.0256 - val_mae: 0.1135 - val_MAEMS: 1.6564\n",
      "Epoch 42/1000\n",
      "131/131 [==============================] - 42s 321ms/step - loss: 1.6160 - mse: 0.0242 - mae: 0.1107 - MAEMS: 1.6159 - val_loss: 1.6743 - val_mse: 0.0252 - val_mae: 0.1125 - val_MAEMS: 1.6785\n",
      "Epoch 43/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.5801 - mse: 0.0237 - mae: 0.1090 - MAEMS: 1.5799 - val_loss: 1.6339 - val_mse: 0.0254 - val_mae: 0.1127 - val_MAEMS: 1.6385\n",
      "Epoch 44/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.5323 - mse: 0.0229 - mae: 0.1066 - MAEMS: 1.5321 - val_loss: 1.6094 - val_mse: 0.0232 - val_mae: 0.1072 - val_MAEMS: 1.6159\n",
      "Epoch 45/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.5384 - mse: 0.0225 - mae: 0.1059 - MAEMS: 1.5383 - val_loss: 1.6473 - val_mse: 0.0224 - val_mae: 0.1062 - val_MAEMS: 1.6528\n",
      "Epoch 46/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.5203 - mse: 0.0222 - mae: 0.1051 - MAEMS: 1.5201 - val_loss: 1.7243 - val_mse: 0.0187 - val_mae: 0.0981 - val_MAEMS: 1.7317\n",
      "Epoch 47/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.5002 - mse: 0.0218 - mae: 0.1039 - MAEMS: 1.4999 - val_loss: 1.5429 - val_mse: 0.0203 - val_mae: 0.0995 - val_MAEMS: 1.5500\n",
      "Epoch 48/1000\n",
      "131/131 [==============================] - 41s 316ms/step - loss: 1.4671 - mse: 0.0212 - mae: 0.1020 - MAEMS: 1.4670 - val_loss: 1.5369 - val_mse: 0.0204 - val_mae: 0.0999 - val_MAEMS: 1.5387\n",
      "Epoch 49/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.4504 - mse: 0.0208 - mae: 0.1008 - MAEMS: 1.4503 - val_loss: 1.5776 - val_mse: 0.0195 - val_mae: 0.0982 - val_MAEMS: 1.5823\n",
      "Epoch 50/1000\n",
      "131/131 [==============================] - 41s 317ms/step - loss: 1.4446 - mse: 0.0206 - mae: 0.1001 - MAEMS: 1.4445 - val_loss: 1.5124 - val_mse: 0.0204 - val_mae: 0.0995 - val_MAEMS: 1.5174\n",
      "Epoch 51/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.4186 - mse: 0.0202 - mae: 0.0991 - MAEMS: 1.4186 - val_loss: 1.4800 - val_mse: 0.0206 - val_mae: 0.0997 - val_MAEMS: 1.4818\n",
      "Epoch 52/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.3963 - mse: 0.0197 - mae: 0.0975 - MAEMS: 1.3962 - val_loss: 1.4465 - val_mse: 0.0210 - val_mae: 0.1008 - val_MAEMS: 1.4484\n",
      "Epoch 53/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 1.3849 - mse: 0.0194 - mae: 0.0967 - MAEMS: 1.3849 - val_loss: 1.4573 - val_mse: 0.0186 - val_mae: 0.0946 - val_MAEMS: 1.4631\n",
      "Epoch 54/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.3852 - mse: 0.0194 - mae: 0.0968 - MAEMS: 1.3851 - val_loss: 1.4445 - val_mse: 0.0202 - val_mae: 0.0988 - val_MAEMS: 1.4498\n",
      "Epoch 55/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.3507 - mse: 0.0188 - mae: 0.0948 - MAEMS: 1.3507 - val_loss: 1.4354 - val_mse: 0.0212 - val_mae: 0.1012 - val_MAEMS: 1.4375\n",
      "Epoch 56/1000\n",
      "131/131 [==============================] - 42s 320ms/step - loss: 1.3491 - mse: 0.0186 - mae: 0.0944 - MAEMS: 1.3490 - val_loss: 1.4650 - val_mse: 0.0212 - val_mae: 0.1014 - val_MAEMS: 1.4679\n",
      "Epoch 57/1000\n",
      "131/131 [==============================] - 42s 317ms/step - loss: 1.3651 - mse: 0.0185 - mae: 0.0944 - MAEMS: 1.3653 - val_loss: 1.4855 - val_mse: 0.0219 - val_mae: 0.1032 - val_MAEMS: 1.4910\n",
      "Epoch 58/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.3583 - mse: 0.0182 - mae: 0.0936 - MAEMS: 1.3582 - val_loss: 1.4398 - val_mse: 0.0202 - val_mae: 0.0981 - val_MAEMS: 1.4438\n",
      "Epoch 59/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.3134 - mse: 0.0178 - mae: 0.0919 - MAEMS: 1.3133 - val_loss: 1.4155 - val_mse: 0.0213 - val_mae: 0.1016 - val_MAEMS: 1.4167\n",
      "Epoch 60/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.3091 - mse: 0.0176 - mae: 0.0913 - MAEMS: 1.3089 - val_loss: 1.3627 - val_mse: 0.0192 - val_mae: 0.0954 - val_MAEMS: 1.3664\n",
      "Epoch 61/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.3012 - mse: 0.0174 - mae: 0.0908 - MAEMS: 1.3011 - val_loss: 1.3868 - val_mse: 0.0185 - val_mae: 0.0934 - val_MAEMS: 1.3902\n",
      "Epoch 62/1000\n",
      "131/131 [==============================] - 42s 320ms/step - loss: 1.2776 - mse: 0.0171 - mae: 0.0895 - MAEMS: 1.2774 - val_loss: 1.3186 - val_mse: 0.0174 - val_mae: 0.0899 - val_MAEMS: 1.3255\n",
      "Epoch 63/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.2755 - mse: 0.0169 - mae: 0.0890 - MAEMS: 1.2753 - val_loss: 1.3395 - val_mse: 0.0182 - val_mae: 0.0924 - val_MAEMS: 1.3422\n",
      "Epoch 64/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.2837 - mse: 0.0168 - mae: 0.0892 - MAEMS: 1.2836 - val_loss: 1.3086 - val_mse: 0.0178 - val_mae: 0.0907 - val_MAEMS: 1.3121\n",
      "Epoch 65/1000\n",
      "131/131 [==============================] - 43s 330ms/step - loss: 1.2447 - mse: 0.0165 - mae: 0.0877 - MAEMS: 1.2447 - val_loss: 1.3329 - val_mse: 0.0172 - val_mae: 0.0894 - val_MAEMS: 1.3388\n",
      "Epoch 66/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 1.2547 - mse: 0.0164 - mae: 0.0876 - MAEMS: 1.2547 - val_loss: 1.3602 - val_mse: 0.0168 - val_mae: 0.0886 - val_MAEMS: 1.3648\n",
      "Epoch 67/1000\n",
      "131/131 [==============================] - 41s 317ms/step - loss: 1.2383 - mse: 0.0161 - mae: 0.0866 - MAEMS: 1.2383 - val_loss: 1.3458 - val_mse: 0.0184 - val_mae: 0.0927 - val_MAEMS: 1.3490\n",
      "Epoch 68/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.2254 - mse: 0.0159 - mae: 0.0857 - MAEMS: 1.2253 - val_loss: 1.3137 - val_mse: 0.0176 - val_mae: 0.0905 - val_MAEMS: 1.3163\n",
      "Epoch 69/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.2102 - mse: 0.0157 - mae: 0.0852 - MAEMS: 1.2101 - val_loss: 1.2665 - val_mse: 0.0169 - val_mae: 0.0879 - val_MAEMS: 1.2678\n",
      "Epoch 70/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.2410 - mse: 0.0158 - mae: 0.0860 - MAEMS: 1.2412 - val_loss: 1.4322 - val_mse: 0.0193 - val_mae: 0.0961 - val_MAEMS: 1.4378\n",
      "Epoch 71/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.2195 - mse: 0.0155 - mae: 0.0849 - MAEMS: 1.2195 - val_loss: 1.4479 - val_mse: 0.0184 - val_mae: 0.0940 - val_MAEMS: 1.4544\n",
      "Epoch 72/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.2357 - mse: 0.0156 - mae: 0.0853 - MAEMS: 1.2355 - val_loss: 1.4772 - val_mse: 0.0196 - val_mae: 0.0980 - val_MAEMS: 1.4835\n",
      "Epoch 73/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 1.2198 - mse: 0.0152 - mae: 0.0840 - MAEMS: 1.2197 - val_loss: 1.3902 - val_mse: 0.0192 - val_mae: 0.0965 - val_MAEMS: 1.3917\n",
      "Epoch 74/1000\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 1.2008 - mse: 0.0150 - mae: 0.0834 - MAEMS: 1.2007 - val_loss: 1.3132 - val_mse: 0.0177 - val_mae: 0.0911 - val_MAEMS: 1.3181\n",
      "Epoch 75/1000\n",
      "131/131 [==============================] - 42s 320ms/step - loss: 1.1704 - mse: 0.0147 - mae: 0.0821 - MAEMS: 1.1703 - val_loss: 1.3528 - val_mse: 0.0178 - val_mae: 0.0914 - val_MAEMS: 1.3579\n",
      "Epoch 76/1000\n",
      "131/131 [==============================] - 41s 317ms/step - loss: 1.1542 - mse: 0.0144 - mae: 0.0810 - MAEMS: 1.1542 - val_loss: 1.2607 - val_mse: 0.0171 - val_mae: 0.0887 - val_MAEMS: 1.2643\n",
      "Epoch 77/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 1.1527 - mse: 0.0143 - mae: 0.0808 - MAEMS: 1.1526 - val_loss: 1.2211 - val_mse: 0.0153 - val_mae: 0.0833 - val_MAEMS: 1.2239\n",
      "Epoch 78/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 1.1423 - mse: 0.0142 - mae: 0.0803 - MAEMS: 1.1423 - val_loss: 1.2915 - val_mse: 0.0172 - val_mae: 0.0896 - val_MAEMS: 1.2935\n",
      "Epoch 79/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 1.1306 - mse: 0.0140 - mae: 0.0797 - MAEMS: 1.1305 - val_loss: 1.3216 - val_mse: 0.0171 - val_mae: 0.0897 - val_MAEMS: 1.3230\n",
      "Epoch 80/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.1277 - mse: 0.0139 - mae: 0.0793 - MAEMS: 1.1276 - val_loss: 1.2634 - val_mse: 0.0163 - val_mae: 0.0866 - val_MAEMS: 1.2634\n",
      "Epoch 81/1000\n",
      "131/131 [==============================] - 43s 331ms/step - loss: 1.1224 - mse: 0.0137 - mae: 0.0786 - MAEMS: 1.1223 - val_loss: 1.2160 - val_mse: 0.0158 - val_mae: 0.0848 - val_MAEMS: 1.2175\n",
      "Epoch 82/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 1.1081 - mse: 0.0134 - mae: 0.0778 - MAEMS: 1.1081 - val_loss: 1.2024 - val_mse: 0.0154 - val_mae: 0.0835 - val_MAEMS: 1.2028\n",
      "Epoch 83/1000\n",
      "131/131 [==============================] - 42s 320ms/step - loss: 1.1112 - mse: 0.0134 - mae: 0.0778 - MAEMS: 1.1112 - val_loss: 1.2162 - val_mse: 0.0157 - val_mae: 0.0845 - val_MAEMS: 1.2206\n",
      "Epoch 84/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.1103 - mse: 0.0133 - mae: 0.0776 - MAEMS: 1.1102 - val_loss: 1.1847 - val_mse: 0.0144 - val_mae: 0.0805 - val_MAEMS: 1.1884\n",
      "Epoch 85/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.1097 - mse: 0.0133 - mae: 0.0775 - MAEMS: 1.1097 - val_loss: 1.1820 - val_mse: 0.0143 - val_mae: 0.0803 - val_MAEMS: 1.1862\n",
      "Epoch 86/1000\n",
      "131/131 [==============================] - 42s 317ms/step - loss: 1.1064 - mse: 0.0131 - mae: 0.0770 - MAEMS: 1.1066 - val_loss: 1.1711 - val_mse: 0.0154 - val_mae: 0.0835 - val_MAEMS: 1.1730\n",
      "Epoch 87/1000\n",
      "131/131 [==============================] - 42s 319ms/step - loss: 1.1020 - mse: 0.0130 - mae: 0.0766 - MAEMS: 1.1020 - val_loss: 1.3338 - val_mse: 0.0169 - val_mae: 0.0897 - val_MAEMS: 1.3368\n",
      "Epoch 88/1000\n",
      "131/131 [==============================] - 42s 320ms/step - loss: 1.1016 - mse: 0.0129 - mae: 0.0763 - MAEMS: 1.1014 - val_loss: 1.2495 - val_mse: 0.0158 - val_mae: 0.0855 - val_MAEMS: 1.2514\n",
      "Epoch 89/1000\n",
      "131/131 [==============================] - 42s 321ms/step - loss: 1.0994 - mse: 0.0129 - mae: 0.0762 - MAEMS: 1.0994 - val_loss: 1.2192 - val_mse: 0.0152 - val_mae: 0.0832 - val_MAEMS: 1.2225\n",
      "Epoch 90/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.0703 - mse: 0.0125 - mae: 0.0748 - MAEMS: 1.0702 - val_loss: 1.2135 - val_mse: 0.0153 - val_mae: 0.0836 - val_MAEMS: 1.2163\n",
      "Epoch 91/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 1.0824 - mse: 0.0126 - mae: 0.0751 - MAEMS: 1.0823 - val_loss: 1.1596 - val_mse: 0.0153 - val_mae: 0.0834 - val_MAEMS: 1.1633\n",
      "Epoch 92/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 1.0787 - mse: 0.0125 - mae: 0.0751 - MAEMS: 1.0787 - val_loss: 1.1613 - val_mse: 0.0132 - val_mae: 0.0771 - val_MAEMS: 1.1635\n",
      "Epoch 93/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.0617 - mse: 0.0123 - mae: 0.0740 - MAEMS: 1.0617 - val_loss: 1.1347 - val_mse: 0.0131 - val_mae: 0.0763 - val_MAEMS: 1.1360\n",
      "Epoch 94/1000\n",
      "131/131 [==============================] - 43s 325ms/step - loss: 1.0562 - mse: 0.0121 - mae: 0.0736 - MAEMS: 1.0561 - val_loss: 1.1815 - val_mse: 0.0151 - val_mae: 0.0832 - val_MAEMS: 1.1823\n",
      "Epoch 95/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.0717 - mse: 0.0122 - mae: 0.0739 - MAEMS: 1.0716 - val_loss: 1.1728 - val_mse: 0.0146 - val_mae: 0.0810 - val_MAEMS: 1.1715\n",
      "Epoch 96/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 1.0606 - mse: 0.0121 - mae: 0.0735 - MAEMS: 1.0606 - val_loss: 1.1631 - val_mse: 0.0141 - val_mae: 0.0797 - val_MAEMS: 1.1643\n",
      "Epoch 97/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 1.0419 - mse: 0.0120 - mae: 0.0727 - MAEMS: 1.0417 - val_loss: 1.1613 - val_mse: 0.0144 - val_mae: 0.0804 - val_MAEMS: 1.1629\n",
      "Epoch 98/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 1.0404 - mse: 0.0119 - mae: 0.0724 - MAEMS: 1.0404 - val_loss: 1.1289 - val_mse: 0.0141 - val_mae: 0.0794 - val_MAEMS: 1.1300\n",
      "Epoch 99/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 1.0486 - mse: 0.0118 - mae: 0.0724 - MAEMS: 1.0485 - val_loss: 1.1175 - val_mse: 0.0137 - val_mae: 0.0781 - val_MAEMS: 1.1202\n",
      "Epoch 100/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 1.0685 - mse: 0.0118 - mae: 0.0729 - MAEMS: 1.0684 - val_loss: 1.1549 - val_mse: 0.0155 - val_mae: 0.0842 - val_MAEMS: 1.1565\n",
      "Epoch 101/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 1.0741 - mse: 0.0118 - mae: 0.0731 - MAEMS: 1.0741 - val_loss: 1.1971 - val_mse: 0.0157 - val_mae: 0.0854 - val_MAEMS: 1.2039\n",
      "Epoch 102/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.0711 - mse: 0.0117 - mae: 0.0727 - MAEMS: 1.0711 - val_loss: 1.3030 - val_mse: 0.0164 - val_mae: 0.0892 - val_MAEMS: 1.3080\n",
      "Epoch 103/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.0333 - mse: 0.0114 - mae: 0.0712 - MAEMS: 1.0332 - val_loss: 1.2079 - val_mse: 0.0148 - val_mae: 0.0828 - val_MAEMS: 1.2110\n",
      "Epoch 104/1000\n",
      "131/131 [==============================] - 42s 325ms/step - loss: 1.0363 - mse: 0.0114 - mae: 0.0712 - MAEMS: 1.0363 - val_loss: 1.1187 - val_mse: 0.0131 - val_mae: 0.0761 - val_MAEMS: 1.1201\n",
      "Epoch 105/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.0310 - mse: 0.0113 - mae: 0.0708 - MAEMS: 1.0309 - val_loss: 1.0877 - val_mse: 0.0131 - val_mae: 0.0759 - val_MAEMS: 1.0900\n",
      "Epoch 106/1000\n",
      "131/131 [==============================] - 42s 325ms/step - loss: 1.0443 - mse: 0.0113 - mae: 0.0711 - MAEMS: 1.0442 - val_loss: 1.0969 - val_mse: 0.0126 - val_mae: 0.0743 - val_MAEMS: 1.0983\n",
      "Epoch 107/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.0325 - mse: 0.0113 - mae: 0.0708 - MAEMS: 1.0324 - val_loss: 1.0993 - val_mse: 0.0132 - val_mae: 0.0761 - val_MAEMS: 1.1023\n",
      "Epoch 108/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 1.0290 - mse: 0.0112 - mae: 0.0704 - MAEMS: 1.0290 - val_loss: 1.0848 - val_mse: 0.0126 - val_mae: 0.0745 - val_MAEMS: 1.0883\n",
      "Epoch 109/1000\n",
      "131/131 [==============================] - 43s 325ms/step - loss: 1.0145 - mse: 0.0110 - mae: 0.0697 - MAEMS: 1.0144 - val_loss: 1.1197 - val_mse: 0.0127 - val_mae: 0.0751 - val_MAEMS: 1.1212\n",
      "Epoch 110/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.0204 - mse: 0.0110 - mae: 0.0698 - MAEMS: 1.0202 - val_loss: 1.1171 - val_mse: 0.0119 - val_mae: 0.0727 - val_MAEMS: 1.1194\n",
      "Epoch 111/1000\n",
      "131/131 [==============================] - 42s 325ms/step - loss: 1.0067 - mse: 0.0109 - mae: 0.0692 - MAEMS: 1.0069 - val_loss: 1.0931 - val_mse: 0.0122 - val_mae: 0.0736 - val_MAEMS: 1.0958\n",
      "Epoch 112/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.0229 - mse: 0.0109 - mae: 0.0695 - MAEMS: 1.0229 - val_loss: 1.0949 - val_mse: 0.0119 - val_mae: 0.0726 - val_MAEMS: 1.0961\n",
      "Epoch 113/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 1.0199 - mse: 0.0108 - mae: 0.0693 - MAEMS: 1.0200 - val_loss: 1.0889 - val_mse: 0.0106 - val_mae: 0.0686 - val_MAEMS: 1.0931\n",
      "Epoch 114/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.0289 - mse: 0.0109 - mae: 0.0696 - MAEMS: 1.0288 - val_loss: 1.1253 - val_mse: 0.0107 - val_mae: 0.0697 - val_MAEMS: 1.1304\n",
      "Epoch 115/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 1.0570 - mse: 0.0111 - mae: 0.0706 - MAEMS: 1.0569 - val_loss: 1.1413 - val_mse: 0.0108 - val_mae: 0.0701 - val_MAEMS: 1.1430\n",
      "Epoch 116/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 1.0358 - mse: 0.0109 - mae: 0.0698 - MAEMS: 1.0357 - val_loss: 1.0395 - val_mse: 0.0115 - val_mae: 0.0706 - val_MAEMS: 1.0433\n",
      "Epoch 117/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 1.0254 - mse: 0.0108 - mae: 0.0693 - MAEMS: 1.0255 - val_loss: 1.0809 - val_mse: 0.0120 - val_mae: 0.0728 - val_MAEMS: 1.0862\n",
      "Epoch 118/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 1.0126 - mse: 0.0108 - mae: 0.0690 - MAEMS: 1.0124 - val_loss: 1.1031 - val_mse: 0.0125 - val_mae: 0.0742 - val_MAEMS: 1.1062\n",
      "Epoch 119/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 0.9749 - mse: 0.0104 - mae: 0.0673 - MAEMS: 0.9748 - val_loss: 1.1305 - val_mse: 0.0127 - val_mae: 0.0752 - val_MAEMS: 1.1340\n",
      "Epoch 120/1000\n",
      "131/131 [==============================] - 42s 325ms/step - loss: 0.9506 - mse: 0.0102 - mae: 0.0663 - MAEMS: 0.9504 - val_loss: 1.0445 - val_mse: 0.0118 - val_mae: 0.0712 - val_MAEMS: 1.0480\n",
      "Epoch 121/1000\n",
      "131/131 [==============================] - 43s 325ms/step - loss: 0.9448 - mse: 0.0101 - mae: 0.0660 - MAEMS: 0.9447 - val_loss: 1.0595 - val_mse: 0.0125 - val_mae: 0.0741 - val_MAEMS: 1.0610\n",
      "Epoch 122/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 0.9445 - mse: 0.0100 - mae: 0.0657 - MAEMS: 0.9445 - val_loss: 1.0093 - val_mse: 0.0114 - val_mae: 0.0698 - val_MAEMS: 1.0127\n",
      "Epoch 123/1000\n",
      "131/131 [==============================] - 43s 326ms/step - loss: 0.9402 - mse: 0.0100 - mae: 0.0654 - MAEMS: 0.9401 - val_loss: 1.0163 - val_mse: 0.0113 - val_mae: 0.0697 - val_MAEMS: 1.0192\n",
      "Epoch 124/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 0.9370 - mse: 0.0099 - mae: 0.0652 - MAEMS: 0.9369 - val_loss: 1.0767 - val_mse: 0.0118 - val_mae: 0.0721 - val_MAEMS: 1.0786\n",
      "Epoch 125/1000\n",
      "131/131 [==============================] - 43s 325ms/step - loss: 0.9390 - mse: 0.0099 - mae: 0.0652 - MAEMS: 0.9390 - val_loss: 1.0153 - val_mse: 0.0108 - val_mae: 0.0679 - val_MAEMS: 1.0167\n",
      "Epoch 126/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 0.9425 - mse: 0.0098 - mae: 0.0651 - MAEMS: 0.9424 - val_loss: 1.0441 - val_mse: 0.0122 - val_mae: 0.0731 - val_MAEMS: 1.0443\n",
      "Epoch 127/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 0.9318 - mse: 0.0098 - mae: 0.0649 - MAEMS: 0.9317 - val_loss: 1.0768 - val_mse: 0.0115 - val_mae: 0.0708 - val_MAEMS: 1.0777\n",
      "Epoch 128/1000\n",
      "131/131 [==============================] - 43s 325ms/step - loss: 0.9319 - mse: 0.0098 - mae: 0.0647 - MAEMS: 0.9318 - val_loss: 1.0733 - val_mse: 0.0119 - val_mae: 0.0723 - val_MAEMS: 1.0773\n",
      "Epoch 129/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 0.9410 - mse: 0.0097 - mae: 0.0648 - MAEMS: 0.9411 - val_loss: 1.0331 - val_mse: 0.0122 - val_mae: 0.0729 - val_MAEMS: 1.0344\n",
      "Epoch 130/1000\n",
      "131/131 [==============================] - 43s 325ms/step - loss: 0.9364 - mse: 0.0097 - mae: 0.0647 - MAEMS: 0.9364 - val_loss: 0.9912 - val_mse: 0.0106 - val_mae: 0.0669 - val_MAEMS: 0.9948\n",
      "Epoch 131/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 0.9288 - mse: 0.0096 - mae: 0.0641 - MAEMS: 0.9288 - val_loss: 0.9846 - val_mse: 0.0100 - val_mae: 0.0654 - val_MAEMS: 0.9880\n",
      "Epoch 132/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 0.9352 - mse: 0.0097 - mae: 0.0645 - MAEMS: 0.9351 - val_loss: 1.0151 - val_mse: 0.0096 - val_mae: 0.0646 - val_MAEMS: 1.0232\n",
      "Epoch 133/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 0.9606 - mse: 0.0097 - mae: 0.0652 - MAEMS: 0.9606 - val_loss: 1.0088 - val_mse: 0.0098 - val_mae: 0.0653 - val_MAEMS: 1.0137\n",
      "Epoch 134/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 0.9506 - mse: 0.0096 - mae: 0.0646 - MAEMS: 0.9505 - val_loss: 0.9620 - val_mse: 0.0105 - val_mae: 0.0666 - val_MAEMS: 0.9640\n",
      "Epoch 135/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 0.9298 - mse: 0.0094 - mae: 0.0637 - MAEMS: 0.9298 - val_loss: 0.9747 - val_mse: 0.0104 - val_mae: 0.0662 - val_MAEMS: 0.9762\n",
      "Epoch 136/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 0.9202 - mse: 0.0094 - mae: 0.0633 - MAEMS: 0.9202 - val_loss: 0.9693 - val_mse: 0.0109 - val_mae: 0.0679 - val_MAEMS: 0.9718\n",
      "Epoch 137/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 0.9106 - mse: 0.0093 - mae: 0.0631 - MAEMS: 0.9106 - val_loss: 0.9717 - val_mse: 0.0097 - val_mae: 0.0644 - val_MAEMS: 0.9744\n",
      "Epoch 138/1000\n",
      "131/131 [==============================] - 42s 325ms/step - loss: 0.9070 - mse: 0.0093 - mae: 0.0629 - MAEMS: 0.9070 - val_loss: 0.9698 - val_mse: 0.0100 - val_mae: 0.0652 - val_MAEMS: 0.9741\n",
      "Epoch 139/1000\n",
      "131/131 [==============================] - 30s 225ms/step - loss: 0.9295 - mse: 0.0093 - mae: 0.0633 - MAEMS: 0.9294 - val_loss: 0.9690 - val_mse: 0.0105 - val_mae: 0.0665 - val_MAEMS: 0.9722\n",
      "Epoch 140/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 0.9391 - mse: 0.0094 - mae: 0.0637 - MAEMS: 0.9390 - val_loss: 1.0199 - val_mse: 0.0107 - val_mae: 0.0682 - val_MAEMS: 1.0240\n",
      "Epoch 141/1000\n",
      "131/131 [==============================] - 20s 150ms/step - loss: 0.9455 - mse: 0.0093 - mae: 0.0637 - MAEMS: 0.9453 - val_loss: 1.0181 - val_mse: 0.0108 - val_mae: 0.0681 - val_MAEMS: 1.0190\n",
      "Epoch 142/1000\n",
      "131/131 [==============================] - 37s 284ms/step - loss: 0.9351 - mse: 0.0093 - mae: 0.0634 - MAEMS: 0.9352 - val_loss: 1.0507 - val_mse: 0.0112 - val_mae: 0.0697 - val_MAEMS: 1.0533\n",
      "Epoch 143/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 0.9578 - mse: 0.0094 - mae: 0.0640 - MAEMS: 0.9578 - val_loss: 1.1888 - val_mse: 0.0128 - val_mae: 0.0772 - val_MAEMS: 1.1984\n",
      "Epoch 144/1000\n",
      "131/131 [==============================] - 32s 244ms/step - loss: 0.9194 - mse: 0.0092 - mae: 0.0626 - MAEMS: 0.9193 - val_loss: 1.0493 - val_mse: 0.0124 - val_mae: 0.0744 - val_MAEMS: 1.0515\n",
      "Wall time: 1h 32min 8s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    b_size = 168\n",
    "    hist = model.fit(strX, strY, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(svaX, svaY), callbacks=[history, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e50b98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4594"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d155c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = model.predict(teX, batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "702cb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npMAEMS(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true))*100\n",
    "def npMAEMD(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true-np.mean(y_true)))*100\n",
    "def npMSE(y_true, y_pred):\n",
    "    return np.mean(np.square(-y_true+y_pred))\n",
    "def npMAE(y_true, y_pred):\n",
    "    return np.mean(abs(-y_true+y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37126743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.03959064570529233  MAE ==  0.15955883383473385  MAEMS ==  2.523037164188736\n"
     ]
    }
   ],
   "source": [
    "tePredict = testPredict.reshape(-1)\n",
    "testY = teY.reshape(-1)\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict), ' MAE == ', npMAE(testY, tePredict), ' MAEMS == ', npMAEMS(testY, tePredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc3bc0",
   "metadata": {},
   "source": [
    "## Without Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db8061b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"informer_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_layer_10 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_11 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_12 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_13 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_14 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_15 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_16 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_17 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_18 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "encoder_layer_19 (EncoderLay multiple                  17657     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            multiple                  149208    \n",
      "=================================================================\n",
      "Total params: 325,778\n",
      "Trainable params: 325,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (168, num_features)  # None for num_feature\n",
    "num_layers = 10\n",
    "d_model = 37*4\n",
    "n_heads = 4\n",
    "d_ff = n_heads*2\n",
    "output_shape = 24\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model2 = Informer(input_shape=input_shape, num_layers=num_layers, d_model=d_model, \n",
    "                      n_heads=n_heads, d_ff=d_ff, output_length=output_shape)\n",
    "\n",
    "    batch_size = 168\n",
    "    example_input = tf.random.normal((batch_size, *input_shape))\n",
    "\n",
    "    output = model2(example_input)\n",
    "    model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "658dcfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2541"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df3a2198",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model2.compile(loss=MAEMS, optimizer='adam', metrics=['mse','mae', MAEMS])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    batch_size = 168\n",
    "    epochs = 1000\n",
    "    history2 = LossHistory()\n",
    "    history2.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "538a07c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "  6/131 [>.............................] - ETA: 34s - loss: 42.0455 - mse: 3.7418 - mae: 1.5714 - MAEMS: 42.0455WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1375s vs `on_train_batch_end` time: 0.1484s). Check your callbacks.\n",
      "131/131 [==============================] - 53s 337ms/step - loss: 14.6823 - mse: 0.6238 - mae: 0.5822 - MAEMS: 14.6619 - val_loss: 9.6217 - val_mse: 0.2273 - val_mae: 0.3839 - val_MAEMS: 9.5858\n",
      "Epoch 2/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 9.6300 - mse: 0.2585 - mae: 0.4063 - MAEMS: 9.6202 - val_loss: 10.4821 - val_mse: 0.1828 - val_mae: 0.3466 - val_MAEMS: 10.4239\n",
      "Epoch 3/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 7.6704 - mse: 0.1943 - mae: 0.3523 - MAEMS: 7.6615 - val_loss: 8.5606 - val_mse: 0.1018 - val_mae: 0.2561 - val_MAEMS: 8.4917\n",
      "Epoch 4/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 6.5658 - mse: 0.1587 - mae: 0.3186 - MAEMS: 6.5571 - val_loss: 6.6327 - val_mse: 0.0588 - val_mae: 0.1977 - val_MAEMS: 6.5666\n",
      "Epoch 5/1000\n",
      "131/131 [==============================] - 42s 321ms/step - loss: 5.5164 - mse: 0.1225 - mae: 0.2839 - MAEMS: 5.5090 - val_loss: 5.5502 - val_mse: 0.0515 - val_mae: 0.1884 - val_MAEMS: 5.4891\n",
      "Epoch 6/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 5.0390 - mse: 0.1055 - mae: 0.2641 - MAEMS: 5.0323 - val_loss: 4.1222 - val_mse: 0.0542 - val_mae: 0.1950 - val_MAEMS: 4.0861\n",
      "Epoch 7/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 4.7890 - mse: 0.0962 - mae: 0.2514 - MAEMS: 4.7824 - val_loss: 4.1100 - val_mse: 0.0479 - val_mae: 0.1860 - val_MAEMS: 4.0667\n",
      "Epoch 8/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 4.6381 - mse: 0.0931 - mae: 0.2482 - MAEMS: 4.6322 - val_loss: 5.4763 - val_mse: 0.0511 - val_mae: 0.1877 - val_MAEMS: 5.4255\n",
      "Epoch 9/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 4.6365 - mse: 0.0943 - mae: 0.2506 - MAEMS: 4.6309 - val_loss: 4.9779 - val_mse: 0.0573 - val_mae: 0.1967 - val_MAEMS: 4.9292\n",
      "Epoch 10/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 4.3691 - mse: 0.0908 - mae: 0.2459 - MAEMS: 4.3637 - val_loss: 4.0336 - val_mse: 0.0531 - val_mae: 0.1929 - val_MAEMS: 3.9988\n",
      "Epoch 11/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 4.1872 - mse: 0.0881 - mae: 0.2413 - MAEMS: 4.1827 - val_loss: 3.8156 - val_mse: 0.0656 - val_mae: 0.2118 - val_MAEMS: 3.7934\n",
      "Epoch 12/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 4.0029 - mse: 0.0851 - mae: 0.2367 - MAEMS: 3.9984 - val_loss: 3.8439 - val_mse: 0.0571 - val_mae: 0.1989 - val_MAEMS: 3.8161\n",
      "Epoch 13/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 3.9757 - mse: 0.0837 - mae: 0.2346 - MAEMS: 3.9721 - val_loss: 3.5652 - val_mse: 0.0790 - val_mae: 0.2317 - val_MAEMS: 3.5617\n",
      "Epoch 14/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 3.8310 - mse: 0.0807 - mae: 0.2299 - MAEMS: 3.8278 - val_loss: 3.3458 - val_mse: 0.0793 - val_mae: 0.2333 - val_MAEMS: 3.3486\n",
      "Epoch 15/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 3.7303 - mse: 0.0794 - mae: 0.2272 - MAEMS: 3.7267 - val_loss: 3.5872 - val_mse: 0.0679 - val_mae: 0.2152 - val_MAEMS: 3.5743\n",
      "Epoch 16/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 3.6811 - mse: 0.0782 - mae: 0.2253 - MAEMS: 3.6774 - val_loss: 3.2529 - val_mse: 0.0660 - val_mae: 0.2138 - val_MAEMS: 3.2463\n",
      "Epoch 17/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 3.5628 - mse: 0.0762 - mae: 0.2215 - MAEMS: 3.5591 - val_loss: 3.2097 - val_mse: 0.0654 - val_mae: 0.2128 - val_MAEMS: 3.2044\n",
      "Epoch 18/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 3.5336 - mse: 0.0762 - mae: 0.2211 - MAEMS: 3.5301 - val_loss: 3.3111 - val_mse: 0.0687 - val_mae: 0.2173 - val_MAEMS: 3.3086\n",
      "Epoch 19/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 3.4772 - mse: 0.0745 - mae: 0.2184 - MAEMS: 3.4735 - val_loss: 3.1969 - val_mse: 0.0615 - val_mae: 0.2064 - val_MAEMS: 3.1949\n",
      "Epoch 20/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 3.5106 - mse: 0.0753 - mae: 0.2192 - MAEMS: 3.5069 - val_loss: 3.3446 - val_mse: 0.0671 - val_mae: 0.2139 - val_MAEMS: 3.3412\n",
      "Epoch 21/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 3.4160 - mse: 0.0717 - mae: 0.2134 - MAEMS: 3.4124 - val_loss: 3.2668 - val_mse: 0.0687 - val_mae: 0.2176 - val_MAEMS: 3.2771\n",
      "Epoch 22/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 3.4007 - mse: 0.0700 - mae: 0.2111 - MAEMS: 3.3980 - val_loss: 3.5077 - val_mse: 0.0854 - val_mae: 0.2416 - val_MAEMS: 3.5254\n",
      "Epoch 23/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 3.3496 - mse: 0.0692 - mae: 0.2090 - MAEMS: 3.3471 - val_loss: 3.2787 - val_mse: 0.0905 - val_mae: 0.2522 - val_MAEMS: 3.3153\n",
      "Epoch 24/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 3.2951 - mse: 0.0673 - mae: 0.2055 - MAEMS: 3.2926 - val_loss: 3.3817 - val_mse: 0.0927 - val_mae: 0.2549 - val_MAEMS: 3.4155\n",
      "Epoch 25/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 3.2251 - mse: 0.0656 - mae: 0.2027 - MAEMS: 3.2216 - val_loss: 3.2355 - val_mse: 0.0679 - val_mae: 0.2165 - val_MAEMS: 3.2437\n",
      "Epoch 26/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 3.2105 - mse: 0.0656 - mae: 0.2026 - MAEMS: 3.2069 - val_loss: 3.2720 - val_mse: 0.0591 - val_mae: 0.2032 - val_MAEMS: 3.2630\n",
      "Epoch 27/1000\n",
      "131/131 [==============================] - 42s 325ms/step - loss: 3.1843 - mse: 0.0648 - mae: 0.2009 - MAEMS: 3.1804 - val_loss: 3.3642 - val_mse: 0.0524 - val_mae: 0.1923 - val_MAEMS: 3.3409\n",
      "Epoch 28/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 3.1894 - mse: 0.0637 - mae: 0.1983 - MAEMS: 3.1855 - val_loss: 3.3029 - val_mse: 0.0544 - val_mae: 0.1956 - val_MAEMS: 3.2837\n",
      "Epoch 29/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 3.1969 - mse: 0.0644 - mae: 0.1997 - MAEMS: 3.1939 - val_loss: 3.0920 - val_mse: 0.0801 - val_mae: 0.2357 - val_MAEMS: 3.1025\n",
      "Epoch 30/1000\n",
      "131/131 [==============================] - 43s 325ms/step - loss: 2.9924 - mse: 0.0601 - mae: 0.1916 - MAEMS: 2.9893 - val_loss: 3.4555 - val_mse: 0.0607 - val_mae: 0.2027 - val_MAEMS: 3.4395\n",
      "Epoch 31/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 3.1106 - mse: 0.0602 - mae: 0.1914 - MAEMS: 3.1090 - val_loss: 3.7413 - val_mse: 0.1042 - val_mae: 0.2720 - val_MAEMS: 3.7423\n",
      "Epoch 32/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 3.4046 - mse: 0.0692 - mae: 0.2074 - MAEMS: 3.4022 - val_loss: 3.2071 - val_mse: 0.0809 - val_mae: 0.2367 - val_MAEMS: 3.2111\n",
      "Epoch 33/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 3.1841 - mse: 0.0631 - mae: 0.1989 - MAEMS: 3.1806 - val_loss: 3.1088 - val_mse: 0.0810 - val_mae: 0.2370 - val_MAEMS: 3.1155\n",
      "Epoch 34/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 2.9978 - mse: 0.0591 - mae: 0.1899 - MAEMS: 2.9955 - val_loss: 3.2497 - val_mse: 0.0900 - val_mae: 0.2502 - val_MAEMS: 3.2640\n",
      "Epoch 35/1000\n",
      "131/131 [==============================] - 42s 324ms/step - loss: 2.9260 - mse: 0.0558 - mae: 0.1838 - MAEMS: 2.9234 - val_loss: 3.1472 - val_mse: 0.0762 - val_mae: 0.2306 - val_MAEMS: 3.1561\n",
      "Epoch 36/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 2.9858 - mse: 0.0595 - mae: 0.1903 - MAEMS: 2.9830 - val_loss: 3.1424 - val_mse: 0.0800 - val_mae: 0.2359 - val_MAEMS: 3.1519\n",
      "Epoch 37/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 2.7528 - mse: 0.0541 - mae: 0.1802 - MAEMS: 2.7500 - val_loss: 3.0926 - val_mse: 0.0788 - val_mae: 0.2340 - val_MAEMS: 3.1057\n",
      "Epoch 38/1000\n",
      "131/131 [==============================] - 42s 322ms/step - loss: 2.7510 - mse: 0.0538 - mae: 0.1783 - MAEMS: 2.7479 - val_loss: 3.2289 - val_mse: 0.0627 - val_mae: 0.2079 - val_MAEMS: 3.2146\n",
      "Epoch 39/1000\n",
      "131/131 [==============================] - 42s 323ms/step - loss: 2.8155 - mse: 0.0538 - mae: 0.1795 - MAEMS: 2.8139 - val_loss: 3.4754 - val_mse: 0.1063 - val_mae: 0.2749 - val_MAEMS: 3.4955\n",
      "Wall time: 27min 40s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    hist2 = model2.fit(trX, trY, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(vaX, vaY), callbacks=[history2, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c2a944c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4598"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5054b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict2 = model2.predict(teX, batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af1749e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.03959064570529233  MAE ==  0.15955883383473385  MAEMS ==  2.523037164188736\n",
      "Error Test Score > MSE ==  0.08774530686841601  MAE ==  0.2490792216118326  MAEMS ==  2.52397579527004\n"
     ]
    }
   ],
   "source": [
    "tePredict2 = testPredict2.reshape(-1)\n",
    "testY = teY.reshape(-1)\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict), ' MAE == ', npMAE(testY, tePredict), ' MAEMS == ', npMAEMS(testY, tePredict))\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict2), ' MAE == ', npMAE(testY, tePredict2), ' MAEMS == ', npMAEMS(testY, tePredict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1de7fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy.stats import norm\n",
    "\n",
    "def diebold_mariano_test(forecast1, forecast2, actual, opt): \n",
    "    \n",
    "    if opt==0: # MSE\n",
    "        e1 = actual-forecast1\n",
    "        e2 = actual-forecast2\n",
    "        d = e1**2 - e2**2\n",
    "    elif opt==1: # MAE\n",
    "        e1 = abs(actual-forecast1)\n",
    "        e2 = abs(actual-forecast2)\n",
    "        d = e1 - e2\n",
    "    else:\n",
    "        e1 = np.multiply(abs(actual - forecast1), actual**2)\n",
    "        e2 = np.multiply(abs(actual - forecast2), actual**2)\n",
    "        d = e1-e2\n",
    "    \n",
    "    # Mean of the loss differentials\n",
    "    mean_d = np.mean(d)\n",
    "    \n",
    "    # Standard deviation of the loss differentials\n",
    "    std_d = np.std(d, ddof=1)\n",
    "    \n",
    "    # Calculate the test statistic\n",
    "    test_stat = (mean_d / std_d) * np.sqrt(len(d))\n",
    "    \n",
    "    # Calculate the p-value using a two-tailed test\n",
    "    p_value = 2 * (1 - norm.cdf(abs(test_stat)))\n",
    "    \n",
    "    return test_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58bd4863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-34.98063064771141, 0.0),\n",
       " (-35.32320155539728, 0.0),\n",
       " (-0.017158050287283115, 0.9863105282793492))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diebold_mariano_test(testPredict, testPredict2, teY, 0), diebold_mariano_test(testPredict, testPredict2, teY, 1), diebold_mariano_test(testPredict, testPredict2, teY, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
